{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-22T06:05:12.728203Z",
     "start_time": "2020-02-22T06:05:12.347762Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "MhcOrxNibMjb"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "import copy\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-22T06:05:12.731872Z",
     "start_time": "2020-02-22T06:05:12.729358Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "EGxgHeR6bN0K"
   },
   "outputs": [],
   "source": [
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda:0\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-22T06:05:12.739924Z",
     "start_time": "2020-02-22T06:05:12.732772Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "tBeDjO8dbPzP"
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "plot_num = 50000\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-22T06:05:12.765835Z",
     "start_time": "2020-02-22T06:05:12.740771Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "zef81a0tbRC4"
   },
   "outputs": [],
   "source": [
    "class SessionData(object):\n",
    "    def __init__(self,session_index,session_id,items_indexes):\n",
    "        self.session_index = session_index\n",
    "        self.session_id = session_id\n",
    "        self.item_list = items_indexes\n",
    "    def generate_seq_datas(self,session_length,padding_idx=0,predict_length=1):\n",
    "        sessions = []\n",
    "        if len(self.item_list)<2:\n",
    "            self.item_list.append[self.item_list[0]]\n",
    "        if predict_length==1:\n",
    "            # when session length>=3\n",
    "            for i in range(1,len(self.item_list)-1):\n",
    "#             # when session length >=2\n",
    "#             for i in range(len(self.item_list)-1):\n",
    "                if i <session_length:\n",
    "                    train_data = [0 for _ in range(session_length-i-1)]\n",
    "                    train_data.extend(self.item_list[:i+1])\n",
    "                    train_data.append(self.item_list[i+1])\n",
    "                else:\n",
    "                    train_data = self.item_list[i+1-session_length:i+1]\n",
    "                    train_data.append(self.item_list[i+1])\n",
    "                sessions.append(train_data)\n",
    "        else:\n",
    "            pass\n",
    "        return self.session_index,sessions\n",
    "    def __str__(self):\n",
    "        info = \" session index = {}\\n session id = {} \\n the length of item list= {} \\n the fisrt item index in item list is {}\".format(self.session_index,self.session_id,len(self.item_list),self.item_list[0])\n",
    "        return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-22T06:05:12.793095Z",
     "start_time": "2020-02-22T06:05:12.766802Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "hl6NiyNJbTRg"
   },
   "outputs": [],
   "source": [
    "class SessionDataSet(object):\n",
    "    def __init__(self,train_file,test_file,padding_idx=0):\n",
    "        super(SessionDataSet,self).__init__()\n",
    "        self.index_count = 0\n",
    "        self.session_count = 0\n",
    "        self.train_count = 0\n",
    "        self.test_count = 0\n",
    "        self.max_session_length = 0\n",
    "\n",
    "        self.padding_idx = padding_idx\n",
    "        self.item2index = dict()\n",
    "        self.index2item = dict()\n",
    "        self.session2index = dict()\n",
    "        self.index2session = dict()\n",
    "        self.item_total_num = dict()\n",
    "        self.item2index[\"<pad>\"] = padding_idx\n",
    "        self.index2item[padding_idx] = \"<pad>\"\n",
    "        self.train_data = self.load_data(train_file)\n",
    "        print(\"training set is loaded, # index: \",len(self.item2index.keys()))\n",
    "        self.train_count = self.session_count\n",
    "        print(\"train_session_num\",self.train_count)\n",
    "        self.test_data = self.load_data(test_file)\n",
    "        print(\"testing set is loaded, # index: \",len(self.index2item.keys()))\n",
    "        print(\"# item\",self.index_count)\n",
    "        self.test_count = self.session_count-self.train_count\n",
    "        print(\"# test session:\",self.test_count)\n",
    "        self.all_training_data = []\n",
    "        self.all_testing_data = []\n",
    "        self.all_meta_training_data = []\n",
    "        self.all_meta_testing_data = []\n",
    "        self.train_session_length = 0\n",
    "        self.test_session_length = 0\n",
    "    \n",
    "    def load_data(self,file_path):\n",
    "        data =  pickle.load(open(file_path, 'rb'))\n",
    "        session_ids = data[0]\n",
    "        session_data = data[1]\n",
    "        session_label = data[2]\n",
    "\n",
    "        result_data = []\n",
    "        lenth = len(session_ids)\n",
    "        print(\"# session\",lenth)\n",
    "\n",
    "        last_session_id = session_ids[0]\n",
    "        \n",
    "        session_item_indexes = []\n",
    "\n",
    "        for item_id in session_data[0]:\n",
    "            if item_id not in self.item2index.keys():\n",
    "                self.index_count+=1\n",
    "                self.item2index[item_id] = self.index_count\n",
    "                self.index2item[self.index_count] = item_id\n",
    "                self.item_total_num[self.index_count] = 0\n",
    "            session_item_indexes.append(self.item2index[item_id])\n",
    "            self.item_total_num[self.item2index[item_id]] += 1\n",
    "        target_item = session_label[0]\n",
    "        if target_item not in self.item2index.keys():\n",
    "            self.index_count+=1\n",
    "            self.item2index[target_item] = self.index_count\n",
    "            self.index2item[self.index_count] = target_item\n",
    "            self.item_total_num[self.index_count] = 0\n",
    "        session_item_indexes.append(self.item2index[target_item])\n",
    "        self.item_total_num[self.item2index[target_item]] += 1\n",
    "\n",
    "        for session_id,items,target_item in zip(session_ids,session_data,session_label):\n",
    "            if session_id!=last_session_id:\n",
    "\n",
    "                self.session_count+=1\n",
    "                self.session2index[last_session_id] = self.session_count\n",
    "                self.index2session[self.session_count] = last_session_id\n",
    "                if len(session_item_indexes)>self.max_session_length:\n",
    "                    self.max_session_length = len(session_item_indexes)\n",
    "                new_session = SessionData(self.session_count,last_session_id,session_item_indexes)\n",
    "                result_data.append(new_session)\n",
    "                last_session_id = session_id\n",
    "                session_item_indexes = []\n",
    "                for item_id in items:\n",
    "                    if item_id not in self.item2index.keys():\n",
    "                        self.index_count+=1\n",
    "                        self.item2index[item_id] = self.index_count\n",
    "                        self.index2item[self.index_count] = item_id\n",
    "                        self.item_total_num[self.index_count] = 0\n",
    "                    session_item_indexes.append(self.item2index[item_id])\n",
    "                    self.item_total_num[self.item2index[item_id]] += 1\n",
    "                if target_item not in self.item2index.keys():\n",
    "                    self.index_count+=1\n",
    "                    self.item2index[target_item] = self.index_count\n",
    "                    self.index2item[self.index_count] = target_item\n",
    "                    self.item_total_num[self.index_count] = 0\n",
    "                session_item_indexes.append(self.item2index[target_item])\n",
    "                self.item_total_num[self.item2index[target_item]] += 1\n",
    "            else:\n",
    "                # 每个id只处理一次\n",
    "                continue\n",
    "\n",
    "        self.session_count+=1\n",
    "        self.session2index[last_session_id] = self.session_count\n",
    "        new_session = SessionData(self.session_count,last_session_id,session_item_indexes)\n",
    "        result_data.append(new_session)\n",
    "        print(\"loaded\")\n",
    "        print(new_session)\n",
    "        \n",
    "        return result_data\n",
    "    \n",
    "\n",
    "    def get_batch(self,batch_size,session_length=10,predict_length=1,all_data=None,phase=\"train\",neg_num=1,sampling_mathod=\"random\"):\n",
    "\n",
    "        if phase == \"train\":\n",
    "            if all_data is None:\n",
    "                all_data = self.get_all_training_data(session_length)\n",
    "            indexes = np.random.permutation(all_data.shape[0])\n",
    "            all_data = all_data[indexes]\n",
    "        else:\n",
    "            if all_data is None:\n",
    "                all_data = self.get_all_testing_data(session_length)\n",
    "        \n",
    "        sindex = 0\n",
    "        eindex = batch_size\n",
    "        while eindex < all_data.shape[0]:\n",
    "            batch = all_data[sindex: eindex]\n",
    "\n",
    "            temp = eindex\n",
    "            eindex = eindex + batch_size\n",
    "            sindex = temp\n",
    "            if phase ==\"train\":\n",
    "                batch = self.divid_and_extend_negative_samples(batch,session_length=session_length,predict_length=predict_length,neg_num=neg_num,method=sampling_mathod)\n",
    "            else:\n",
    "                batch = [batch[:,:session_length],batch[:,session_length:]]\n",
    "            yield batch\n",
    "\n",
    "        if eindex >= all_data.shape[0]:\n",
    "            batch = all_data[sindex:]\n",
    "            if phase ==\"train\":\n",
    "                batch = self.divid_and_extend_negative_samples(batch,session_length=session_length,predict_length=predict_length,neg_num=neg_num,method=sampling_mathod)\n",
    "            else:\n",
    "                batch = [batch[:,:session_length],batch[:,session_length:]]\n",
    "            yield batch\n",
    "    \n",
    "    def get_batch_with_neg(self,batch_size,session_length=10,predict_length=1,all_data=None,phase=\"train\",neg_num=1,sampling_mathod=\"random\"):\n",
    "        if phase == \"train\":\n",
    "            all_data = self.get_all_training_data_with_neg(session_length,neg_num)\n",
    "            indexes = np.random.permutation(all_data.shape[0])\n",
    "            all_data = all_data[indexes]\n",
    "        else:\n",
    "            all_data = self.get_all_testing_data_with_neg(session_length,neg_num)\n",
    "        \n",
    "        sindex = 0\n",
    "        eindex = batch_size\n",
    "        while eindex < all_data.shape[0]:\n",
    "            batch = all_data[sindex: eindex]\n",
    "\n",
    "            temp = eindex\n",
    "            eindex = eindex + batch_size\n",
    "            sindex = temp\n",
    "            if phase ==\"train\":\n",
    "                batch = [batch[:,:session_length],batch[:,session_length:session_length+predict_length],batch[:,-neg_num:]]\n",
    "            else:\n",
    "                batch = [batch[:,:session_length],batch[:,session_length:]]\n",
    "            yield batch\n",
    "\n",
    "        if eindex >= all_data.shape[0]:\n",
    "            batch = all_data[sindex:]\n",
    "            if phase ==\"train\":\n",
    "                batch = [batch[:,:session_length],batch[:,session_length:session_length+predict_length],batch[:,-neg_num:]]\n",
    "            else:\n",
    "                batch = [batch[:,:session_length],batch[:,session_length:]]\n",
    "            yield batch\n",
    "    \n",
    "    def get_batch_tasks_with_neg(self,batch_size,session_length=10,predict_length=1,all_data=None,phase=\"train\",neg_num=1,sampling_mathod=\"random\"):\n",
    "        if phase == \"train\":\n",
    "            all_data = self.get_all_meta_training_data_with_neg(session_length,neg_num)\n",
    "            random.shuffle(all_data)\n",
    "        else:\n",
    "            all_data = self.get_all_meta_testing_data_with_neg(session_length,neg_num)\n",
    "        sindex = 0\n",
    "        eindex = batch_size\n",
    "        while eindex < len(all_data):\n",
    "            batch = all_data[sindex: eindex]\n",
    "\n",
    "            temp = eindex\n",
    "            eindex = eindex + batch_size\n",
    "            sindex = temp\n",
    "\n",
    "            session_items = [batch[i][:,:session_length] for i in range(len(batch))]\n",
    "\n",
    "            target_item = [batch[i][:,session_length:session_length+predict_length] for i in range(len(batch))]\n",
    "\n",
    "            neg_item = [batch[i][:,-neg_num:] for i in range(len(batch))]\n",
    "            batch = [session_items,target_item,neg_item]\n",
    "            yield batch\n",
    "\n",
    "        if eindex >= len(all_data):\n",
    "            batch = all_data[sindex:]\n",
    "            session_items = [batch[i][:,:session_length] for i in range(len(batch))]\n",
    "\n",
    "            target_item = [batch[i][:,session_length:session_length+predict_length] for i in range(len(batch))]\n",
    "\n",
    "            neg_item = [batch[i][:,-neg_num:] for i in range(len(batch))]\n",
    "            batch = [session_items,target_item,neg_item]\n",
    "            yield batch\n",
    "    \n",
    "    def divid_and_extend_negative_samples(self,batch_data,session_length,predict_length=1,neg_num=1,method=\"random\"):\n",
    "        \"\"\"\n",
    "        divid and extend negative samples\n",
    "        \"\"\"\n",
    "        neg_items = []\n",
    "        if method == \"random\":\n",
    "            for session_and_target in batch_data:\n",
    "                neg_item = []\n",
    "                for i in range(neg_num):\n",
    "                    rand_item = random.randint(1,self.index_count)\n",
    "                    while rand_item in session_and_target or rand_item in neg_item:\n",
    "                        rand_item = random.randint(1,self.index_count)\n",
    "                    neg_item.append(rand_item)\n",
    "                neg_items.append(neg_item)\n",
    "        else:\n",
    "\n",
    "            total_list = set()\n",
    "            for session in batch_data:\n",
    "                for i in session:\n",
    "                    total_list.add(i) \n",
    "            total_list = list(total_list)\n",
    "            total_list =  sorted(total_list, key=lambda item: self.item_total_num[item],reverse=True)\n",
    "            for i,session in enumerate(batch_data):\n",
    "                np.random.choice(total_list)\n",
    "        session_items = batch_data[:,:session_length]\n",
    "        target_item = batch_data[:,session_length:]\n",
    "        neg_items = np.array(neg_items)\n",
    "        return [session_items,target_item,neg_items]\n",
    "    \n",
    "    def get_all_training_data(self,session_length,predict_length=1):\n",
    "        if len(self.all_training_data)!=0 and self.train_session_length==session_length:\n",
    "#             print(\"The build is complete and there is no need to repeat the build\")\n",
    "            return self.all_training_data\n",
    "        print(\"Start building the all training dataset\")\n",
    "        all_sessions = []\n",
    "        for session_data in self.train_data:\n",
    "            # 前session_length为session，后predict_length为target_item\n",
    "            session_index,sessions = session_data.generate_seq_datas(session_length,padding_idx=self.padding_idx)\n",
    "            if sessions is not None:\n",
    "                all_sessions.extend(sessions)\n",
    "        all_sessions = np.array(all_sessions)\n",
    "        self.all_training_data = all_sessions\n",
    "        self.train_session_length=session_length\n",
    "        print(\"The total number of training samples is：\",all_sessions.shape)\n",
    "        return all_sessions\n",
    "    \n",
    "    def get_all_testing_data(self,session_length,predict_length=1):\n",
    "        if len(self.all_testing_data)!=0 and self.test_session_length==session_length:\n",
    "            return self.all_testing_data\n",
    "        all_sessions = []\n",
    "        for session_data in self.test_data:\n",
    "            session_index,sessions = session_data.generate_seq_datas(session_length,padding_idx=self.padding_idx)\n",
    "            if sessions is not None:\n",
    "                all_sessions.extend(sessions)\n",
    "        all_sessions = np.array(all_sessions)\n",
    "        self.all_testing_data = all_sessions\n",
    "        self.test_session_length=session_length\n",
    "        print(\"The total number of testing samples is：\",all_sessions.shape)\n",
    "        return all_sessions\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-22T06:05:21.129694Z",
     "start_time": "2020-02-22T06:05:12.793937Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312.0
    },
    "colab_type": "code",
    "id": "ctSu0HF8bUqh",
    "outputId": "7287ecc0-f73d-4883-c8f6-20c9b8aeddb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# session 3924324\n",
      "loaded\n",
      " session index = 1071676\n",
      " session id = 11497318 \n",
      " the length of item list= 3 \n",
      " the fisrt item index in item list is 26087\n",
      "training set is loaded, # index:  27035\n",
      "train_session_num 1071676\n",
      "# session 40548\n",
      "loaded\n",
      " session index = 1081019\n",
      " session id = 11560908 \n",
      " the length of item list= 3 \n",
      " the fisrt item index in item list is 1867\n",
      "testing set is loaded, # index:  27053\n",
      "# item 27052\n",
      "# test session: 9343\n"
     ]
    }
   ],
   "source": [
    "# dataset = SessionDataSet(train_file=\"../data/retailrocket/train.txt\",test_file=\"../data/retailrocket/test.txt\")\n",
    "# dataset = SessionDataSet(train_file=\"../data/diginetica/train.txt\",test_file=\"../data/diginetica/test.txt\")\n",
    "# dataset = SessionDataSet(train_file=\"../data/yoochoose1_4/train.txt\",test_file=\"../data/yoochoose1_4/test.txt\")\n",
    "# dataset = SessionDataSet(train_file=\"../data/yoochoose1_64/train.txt\",test_file=\"../data/yoochoose1_64/test.txt\")\n",
    "# dataset = SessionDataSet(train_file=\"../data/retailrocket_gcsan_my/train.txt\",test_file=\"../data/retailrocket_gcsan_my/test.txt\")\n",
    "# dataset = SessionDataSet(train_file=\"../data/diginetica_gcsan_my/train.txt\",test_file=\"../data/diginetica_gcsan_my/test.txt\")\n",
    "dataset = SessionDataSet(train_file=\"../data/yoochoose1_4_gcsan_my/train.txt\",test_file=\"../data/yoochoose1_4_gcsan_my/test.txt\")\n",
    "# dataset = SessionDataSet(train_file=\"../data/yoochoose1_64_gcsan_my/train.txt\",test_file=\"../data/yoochoose1_64_gcsan_my/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-22T06:05:21.138917Z",
     "start_time": "2020-02-22T06:05:21.131050Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "cm7qU4B6bq2i"
   },
   "outputs": [],
   "source": [
    "def bpr_loss(r):\n",
    "    return torch.sum(-torch.log(torch.sigmoid(r)))\n",
    "def get_hit_num(pred,y_truth):\n",
    "    \"\"\"\n",
    "        pred: numpy type(batch_size,k) \n",
    "        y_truth: list type (batch_size,groudtruth_num)\n",
    "    \"\"\"\n",
    "\n",
    "    hit_num = 0\n",
    "    for i in range(len(y_truth)):\n",
    "        for value in y_truth[i]:\n",
    "            hit_num += np.sum(pred[i]==value)\n",
    "    return hit_num\n",
    "\n",
    "def get_rr(pred,y_truth):\n",
    "    rr=0.\n",
    "    for i in range(len(y_truth)):\n",
    "        for value in y_truth[i]:\n",
    "            hit_indexes = np.where(pred[i]==value)[0]\n",
    "            for hit_index in hit_indexes:\n",
    "                rr += 1/(hit_index+1)\n",
    "    return rr\n",
    "\n",
    "def get_dcg(pred,y_truth):\n",
    "    y_pred_score = np.zeros_like(pred)\n",
    "\n",
    "    for i in range(len(y_truth)):\n",
    "\n",
    "        for j,y_pred in enumerate(pred[i]):\n",
    "            if y_pred == y_truth[i][0]:\n",
    "                y_pred_score[i][j]=1\n",
    "    gain = 2 ** y_pred_score - 1\n",
    "    discounts = np.tile(np.log2(np.arange(pred.shape[1]) + 2),(len(y_truth),1))\n",
    "    dcg = np.sum(gain / discounts,axis=1)\n",
    "    return dcg\n",
    "\n",
    "def get_ndcg(pred,y_truth):\n",
    "    dcg = get_dcg(pred, y_truth)\n",
    "    idcg = get_dcg(np.concatenate((y_truth,np.zeros_like(pred)[:,:-1]-1),axis=1), y_truth)\n",
    "    ndcg = np.sum(dcg / idcg)\n",
    "\n",
    "    return ndcg\n",
    "\n",
    "def dcg_score(y_pre, y_true, k):\n",
    "    y_pre_score = np.zeros(k)\n",
    "    if len(y_pre) > k:\n",
    "        y_pre = y_pre[:k]\n",
    "    for i in range(len(y_pre)):\n",
    "        pre_tag = y_pre[i]\n",
    "        if pre_tag in y_true:\n",
    "            y_pre_score[i] = 1\n",
    "    gain = 2 ** y_pre_score - 1\n",
    "    discounts = np.log2(np.arange(k) + 2)\n",
    "    return np.sum(gain / discounts)\n",
    "\n",
    "\n",
    "def ndcg_score(y_pre, y_true, k=5):\n",
    "    dcg = dcg_score(y_pre, y_true, k)\n",
    "    idcg = dcg_score(y_true, y_true, k)\n",
    "    return dcg / idcg\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-22T06:05:21.152241Z",
     "start_time": "2020-02-22T06:05:21.140038Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "QdNvTTApbr_g"
   },
   "outputs": [],
   "source": [
    "# SelfAttention Layer\n",
    "class SelfAttention(torch.nn.Module):\n",
    "    def __init__(self, method, hidden_size,activate=\"selu\",dropout=0):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.config = list()\n",
    "        # 使用的Attention方法\n",
    "        self.method = method\n",
    "        # 隐藏层大小\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method not in ['dot', 'general']:\n",
    "            raise ValueError(self.method, \"Attention method do not exists.\")\n",
    "\n",
    "        if self.method == \"dot\":\n",
    "            self.query = torch.nn.Linear(self.hidden_size *2, self.hidden_size*2)\n",
    "            self.key = torch.nn.Linear(self.hidden_size*2, self.hidden_size*2)\n",
    "            torch.nn.init.constant_(self.query.bias,0)\n",
    "            torch.nn.init.constant_(self.key.bias,0)\n",
    "\n",
    "        if self.method == \"general\":\n",
    "            self.attention = torch.nn.Linear(self.hidden_size*2, self.hidden_size*2)\n",
    "            torch.nn.init.constant_(self.attention.bias,0)\n",
    "        \n",
    "        if activate == \"relu\":\n",
    "            self.activate = torch.relu\n",
    "        elif activate == \"tanh\":\n",
    "            self.activate = torch.tanh\n",
    "        elif activate == \"elu\":\n",
    "            self.activate = torch.nn.ELU()\n",
    "        elif activate == \"selu\":\n",
    "            self.activate = torch.selu\n",
    "        elif activate == \"gelu\":\n",
    "            self.activate = F.gelu\n",
    "        elif activate == \"dice\":\n",
    "            self.activate = DICE(embedding_dims=self.hidden_size*2)\n",
    "        else:\n",
    "            self.activate = torch.sigmoid\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        torch.nn.utils.clip_grad_norm_(self.parameters(),max_norm=110)\n",
    "\n",
    "    def dot_score(self, encoder_output,is_train=True,weights=None):\n",
    "\n",
    "        if weights is None:\n",
    "            if is_train:\n",
    "                query = self.dropout(self.activate(self.query(encoder_output)))\n",
    "                key = self.dropout(self.activate(self.key(encoder_output)))\n",
    "            else:\n",
    "                query = self.activate(self.query(encoder_output))\n",
    "                key = self.activate(self.key(encoder_output))\n",
    "        else:\n",
    "            query = self.activate(torch.matmul(encoder_output,weights[0].t())+weights[1])\n",
    "            key = self.activate(torch.matmul(encoder_output,weights[2].t())+weights[3])\n",
    "        dot = query.bmm(key.permute(0, 2, 1))\n",
    "        return dot\n",
    "\n",
    "    def general_score(self, encoder_output,is_train=True,weights=None):\n",
    "        if weights is None:\n",
    "            if is_train:\n",
    "                energy = self.dropout(self.activate(self.attention(encoder_output)))\n",
    "            else:\n",
    "                energy = self.activate(self.attention(encoder_output))\n",
    "        else:\n",
    "            energy = self.activate(torch.matmul(encoder_output,weights[0].t())+weights[1])\n",
    "        return encoder_output.bmm(energy.permute(0, 2, 1))\n",
    "\n",
    "    def forward(self, encoder_outputs, mask=None,is_train=True):\n",
    "        # (batch_size,length,dim)\n",
    "        if self.method == \"general\":\n",
    "            attention_energies = self.general_score(encoder_outputs,is_train=is_train)\n",
    "        elif self.method == \"dot\":\n",
    "            attention_energies = self.dot_score(encoder_outputs,is_train=is_train)\n",
    "\n",
    "        #  (batch_size,length,length)\n",
    "        attention_energies.div_(torch.sqrt(torch.tensor(self.hidden_size*2, dtype=torch.float)))\n",
    "        if mask is not None:\n",
    "            new_mask = (1 - (1 - mask.float()).unsqueeze(1).permute(0, 2, 1).bmm(\n",
    "                (1 - mask.float()).unsqueeze(1)))\n",
    "\n",
    "            attention_energies = attention_energies - new_mask*1e12\n",
    "            weights = F.softmax(attention_energies, dim=2)\n",
    "            weights = weights*(1-new_mask)\n",
    "            # batch_size,length,length)*(batch_size,length,dim)->(batch_size,length,dim)->(batch_size,1,dim)->(batch_size,dim)\n",
    "            outputs = weights.bmm(encoder_outputs)\n",
    "            outputs.div_(mask.shape[1]-torch.sum(mask,dim=1).unsqueeze(1).unsqueeze(2).repeat((1,mask.shape[1],outputs.shape[2])).float())\n",
    "            outputs = outputs.sum(dim=1).squeeze(1)\n",
    "        else:\n",
    "            weights = F.softmax(attention_energies, dim=2)\n",
    "            # (batch_size,length,length)*(batch_size,length,dim)->(batch_size,length,dim)->(batch_size,1,dim)->(batch_size,dim)\n",
    "            outputs = (weights.bmm(encoder_outputs).sum(dim=1) / encoder_outputs.shape[1]).squeeze(1)\n",
    "        sa_weights = weights.sum(dim=1).squeeze(1)\n",
    "        return outputs, sa_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-22T06:05:21.167267Z",
     "start_time": "2020-02-22T06:05:21.153087Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "aVIrdlmKbtUq"
   },
   "outputs": [],
   "source": [
    "class POEM(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=64, itemNum=0, posNum=0, padding_idx=0, dropout=0.5,attention_method=\"dot\",head_num=4,\n",
    "                 activate=\"selu\",session_length=20,delta=16.0):\n",
    "        super(POEM, self).__init__()\n",
    "        self.padding_idx = padding_idx\n",
    "        self.hidden_size = hidden_size\n",
    "        self.head_num = head_num\n",
    "        self.delta = delta\n",
    "        self.session_length = session_length\n",
    "        if activate == \"sigmoid\":\n",
    "            self.activate = torch.sigmoid\n",
    "        elif activate == \"tanh\":\n",
    "            self.activate = torch.tanh\n",
    "        elif activate == \"relu\":\n",
    "            self.activate = torch.relu\n",
    "        elif activate == \"gelu\":\n",
    "            self.activate = F.gelu\n",
    "        elif activate == \"elu\":\n",
    "            self.activate = torch.nn.ELU()\n",
    "        else:\n",
    "            self.activate = torch.selu\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.item_embedding = torch.nn.Embedding(itemNum, hidden_size, padding_idx=self.padding_idx,max_norm=1.5)\n",
    "        self.position_embedding = torch.nn.Embedding(posNum,hidden_size,padding_idx=self.padding_idx,max_norm=1.5)\n",
    "        self.position_weights = torch.nn.Embedding(posNum,1,padding_idx=self.padding_idx)\n",
    "        \n",
    "        self.self_attention = SelfAttention(attention_method, hidden_size,activate=activate,dropout=dropout).to(device)\n",
    "        torch.nn.init.constant_(self.item_embedding.weight[0],0)\n",
    "        torch.nn.init.constant_(self.position_embedding.weight[0],0)\n",
    "        torch.nn.init.constant_(self.position_weights.weight,1)\n",
    "        torch.nn.init.constant_(self.position_weights.weight[0],0)\n",
    "        \n",
    "        self.gen_mlp = torch.nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.cur_mlp = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.deep_mlp = torch.nn.Linear(hidden_size*3, hidden_size,bias=False)\n",
    "        \n",
    "    def forward(self, session,item=None,bpr_loss=False,neg_num=50):\n",
    "\n",
    "        mask = (session!=0).float()\n",
    "        length = torch.sum(mask,1).unsqueeze(1).expand((session.shape[0],self.hidden_size))\n",
    "        mask = mask.unsqueeze(2).expand((session.shape[0],session.shape[1],self.hidden_size))\n",
    "        session_item_embeddings = self.item_embedding(session) * mask\n",
    "        positions = session.shape[1] - torch.arange(0,session.shape[1]).unsqueeze(0).expand_as(session).to(device)\n",
    "        session_position_embeddings = self.dropout(self.position_embedding(positions))*mask\n",
    "        session_item_vecs = torch.cat((session_item_embeddings,session_position_embeddings), dim=2)\n",
    "        attention_mask = (session == self.padding_idx)\n",
    "        sa_output, sa_weights = self.self_attention(session_item_vecs, attention_mask)\n",
    "        session_position_weights = self.dropout(self.position_weights(positions))*mask\n",
    "        sa_weights = sa_weights.unsqueeze(2).expand_as(session_item_embeddings)\n",
    "        session_item_vecs2 = session_item_embeddings * session_position_weights * sa_weights\n",
    "        psa_output = torch.sum(session_item_vecs2, dim=1)/length\n",
    "        gen_output = self.dropout(self.activate(self.gen_mlp(sa_output)))\n",
    "        cur_output = self.dropout(self.activate(self.cur_mlp(session_item_embeddings[:,-1])))\n",
    "        deep_output = self.dropout(self.activate(self.deep_mlp(torch.cat((sa_output,session_item_embeddings[:,-1]),1))))\n",
    "        session_output =  F.normalize(gen_output * cur_output + deep_output + psa_output,dim=-1)\n",
    "        \n",
    "    \n",
    "    \n",
    "        session_output = session_output*self.delta\n",
    "        item_embedding_weight = F.normalize(self.item_embedding.weight[1:],dim=-1)\n",
    "        result = torch.matmul(session_output,item_embedding_weight.t())\n",
    "        return result\n",
    "    \n",
    "    def predict_top_k(self, session, k=20):\n",
    "        mask = (session!=0).float()\n",
    "        length = torch.sum(mask,1).unsqueeze(1).expand((session.shape[0],self.hidden_size))\n",
    "        mask = mask.unsqueeze(2).expand((session.shape[0],session.shape[1],self.hidden_size))\n",
    "        session_item_embeddings = self.item_embedding(session) * mask\n",
    "        positions = session.shape[1] - torch.arange(0,session.shape[1]).unsqueeze(0).expand_as(session).to(device)\n",
    "        session_position_embeddings = self.position_embedding(positions)*mask\n",
    "        session_item_vecs = torch.cat((session_item_embeddings,session_position_embeddings), dim=2)\n",
    "        attention_mask = (session == self.padding_idx)\n",
    "        sa_output, sa_weights = self.self_attention(session_item_vecs, attention_mask,is_train=False)\n",
    "        session_position_weights = self.position_weights(positions)*mask\n",
    "        sa_weights = sa_weights.unsqueeze(2).expand_as(session_item_embeddings)\n",
    "        session_item_vecs2 = session_item_embeddings * session_position_weights * sa_weights\n",
    "        psa_output = torch.sum(session_item_vecs2, dim=1)/length\n",
    "        gen_output =self.activate(self.gen_mlp(sa_output))\n",
    "\n",
    "        cur_output = self.activate(self.cur_mlp(session_item_embeddings[:,-1]))\n",
    "        deep_output = self.activate(self.deep_mlp(torch.cat((sa_output,session_item_embeddings[:,-1]),1)))\n",
    "        session_output =  F.normalize(gen_output * cur_output + deep_output + psa_output,dim=-1)\n",
    "        \n",
    "    \n",
    "    \n",
    "        session_output = session_output*self.delta\n",
    "        item_embedding_weight = F.normalize(self.item_embedding.weight[1:],dim=-1)\n",
    "        result = torch.matmul(session_output,item_embedding_weight.t())\n",
    "        result = torch.topk(result,k,dim=1)[1]\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-22T06:05:21.179888Z",
     "start_time": "2020-02-22T06:05:21.168258Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Jhrg56xebung"
   },
   "outputs": [],
   "source": [
    "epochs=50\n",
    "def train(args):\n",
    "    hidden_size = args[\"hidden_size\"] if \"hidden_size\" in args.keys() else 100\n",
    "    dropout = args[\"dropout\"] if \"dropout\" in args.keys()  else 0.5\n",
    "    attention_method = args[\"method\"] if \"method\" in args.keys()  else \"general\"\n",
    "    lr = args[\"lr\"] if \"lr\" in args.keys()  else 5e-4\n",
    "    weight_decay = args[\"weight_decay\"] if \"weight_decay\" in args.keys()  else 1e-5\n",
    "    amsgrad = args[\"amsgrad\"] if \"amsgrad\" in args.keys() else True\n",
    "    session_length = args[\"session_length\"] if \"session_length\" in args.keys() else 20\n",
    "    delta = args[\"delta\"] if \"delta\" in args.keys() else 20\n",
    "    model = POEM(hidden_size=hidden_size, itemNum=dataset.index_count+1, posNum=session_length+1, padding_idx=0, dropout=dropout,\n",
    "                 activate=\"selu\",attention_method=attention_method,delta=delta).to(device)\n",
    "    opti = torch.optim.Adam(model.parameters(),lr=lr,weight_decay=weight_decay,amsgrad=amsgrad)\n",
    "    patience = args[\"patience\"] if \"patience\" in args.keys() else 5\n",
    "    best_model_hr = 0.0\n",
    "    best_model_mrr = 0.0\n",
    "    best_r1m = 0.0\n",
    "    best_model = None\n",
    "    predict_nums = [1,5,10,20]\n",
    "    no_improvement_epoch = 0\n",
    "    start_train_time = datetime.datetime.now()\n",
    "    for epoch in range(epochs):\n",
    "        batch_losses = []\n",
    "        epoch_losses = []\n",
    "        for i,batch_data in enumerate(dataset.get_batch(batch_size,session_length,phase=\"train\")):\n",
    "            sessions = torch.tensor(batch_data[0]).to(device)\n",
    "            target_items = torch.tensor(batch_data[1]).squeeze().to(device)-1\n",
    "            result_pos = model(sessions)\n",
    "            loss = loss_function(result_pos,target_items)\n",
    "            opti.zero_grad()\n",
    "            loss.backward()\n",
    "            opti.step()\n",
    "            batch_losses.append(loss.cpu().detach().numpy())\n",
    "            epoch_losses.append(loss.cpu().detach().numpy())\n",
    "            if i % plot_num == 0:\n",
    "                time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                print(\"[%s] [%d/%d] %d mean_batch_loss : %0.6f\" % (time, epoch+1, epochs, i, np.mean(batch_losses)))\n",
    "                batch_losses = []\n",
    "        with torch.no_grad():\n",
    "            start_test_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(\"Start predicting\",start_test_time)\n",
    "            rrs = [0 for _ in range(len(predict_nums))]\n",
    "            hit_nums = [0 for _ in range(len(predict_nums))]\n",
    "            ndcgs = [0 for _ in range(len(predict_nums))]\n",
    "            for i,batch_data in enumerate(dataset.get_batch(batch_size,session_length,phase=\"test\")):\n",
    "                \n",
    "                sessions = torch.tensor(batch_data[0]).to(device)\n",
    "                target_items = np.array(batch_data[1])-1\n",
    "                y_pred = model.predict_top_k(sessions,20).cpu().numpy()\n",
    "                \n",
    "                for j,predict_num in enumerate(predict_nums):\n",
    "                    hit_nums[j]+=get_hit_num(y_pred[:,:predict_num],target_items)\n",
    "                    rrs[j]+=get_rr(y_pred[:,:predict_num],target_items)\n",
    "                    ndcgs[j]+=get_ndcg(y_pred[:,:predict_num],target_items)\n",
    "                    \n",
    "            end_test_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            hrs = [hit_num/len(dataset.all_testing_data) for hit_num in hit_nums]\n",
    "            mrrs = [rr/len(dataset.all_testing_data) for rr in rrs]\n",
    "            mndcgs = [ndcg/len(dataset.all_testing_data) for ndcg in ndcgs]\n",
    "            if hrs[-1] + mrrs[-1] > best_r1m:\n",
    "                print(\"change best\")\n",
    "                best_model = deepcopy(model)\n",
    "                best_model_hr = hrs[-1]\n",
    "                best_model_mrr = mrrs[-1]\n",
    "                best_r1m = hrs[-1] + mrrs[-1]\n",
    "                no_improvement_epoch = 0\n",
    "            else:\n",
    "                no_improvement_epoch +=1\n",
    "            print(\"testing finish [%s] \"%end_test_time)\n",
    "            for k,predict_num in enumerate(predict_nums):\n",
    "                print(\"\\tHR@%d=%.5f  MRR@%d=%.5f  NDCG@%d=%.5f\"%(predict_num,hrs[k],predict_num,mrrs[k],predict_num,mndcgs[k]))\n",
    "        if no_improvement_epoch>=patience:\n",
    "            print(\"early stopping\")\n",
    "            break\n",
    "    end_train_time = datetime.datetime.now()\n",
    "    print(\"training and testting over, Total time\",end_train_time-start_train_time)\n",
    "    return best_model,best_model_hr,best_model_mrr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2H2SFJ7F77Z0"
   },
   "source": [
    "# CIKM-Session >= 3 Total 417.4s Avg 13.46s\n",
    "    HR@20=0.67856  MRR@20=0.33175, hyper-parameters: session_length-20, hidden_size-100, lr-0.0010,delta=16.0, amsgrad-True, method-general, dropout-0.5, weight_decay-0.000000. \n",
    "        HR@1=0.21627  MRR@1=0.21627  NDCG@1=0.21627\n",
    "        HR@5=0.46828  MRR@5=0.31020  NDCG@5=0.34961\n",
    "        HR@10=0.57668  MRR@10=0.32467  NDCG@10=0.38467\n",
    "        HR@20=0.67856  MRR@20=0.33175  NDCG@20=0.41044\n",
    "# RR-Session >= 3 Total 183.0s Avg 9.15‬s\n",
    "    HR@20=0.57534  MRR@20=0.31288， hyper-parameters: session_length-20, hidden_size-100, lr-0.0010,delta=16.0, amsgrad-True, method-general, dropout-0.5, weight_decay-0.000000. \n",
    "        HR@1=0.22588  MRR@1=0.22588  NDCG@1=0.22588\n",
    "        HR@5=0.42083  MRR@5=0.29699  NDCG@5=0.32779\n",
    "        HR@10=0.50007  MRR@10=0.30766  NDCG@10=0.35350\n",
    "        HR@20=0.57534  MRR@20=0.31288  NDCG@20=0.37252\n",
    "\n",
    "# RSC64-Session >= 3 Total 103.9‬s Avg 6.11s\n",
    "    HR@20=0.71121  MRR@20=0.29529, hyper-parameters: session_length-20, hidden_size-100, lr-0.0010,delta=16.0, amsgrad-True, method-general, dropout-0.5, weight_decay-0.000000. \n",
    "        HR@1=0.16351  MRR@1=0.16351  NDCG@1=0.16351\n",
    "        HR@5=0.45822  MRR@5=0.26828  NDCG@5=0.31538\n",
    "        HR@10=0.60072  MRR@10=0.28747  NDCG@10=0.36163\n",
    "        HR@20=0.71121  MRR@20=0.29529  NDCG@20=0.38976\n",
    "        \n",
    "# RSC4-Session >= 3 Total 2,431.5s Avg 67.54s\n",
    "    HR@20=0.72339  MRR@20=0.30720, hyper-parameters: session_length-20, hidden_size-100, lr-0.0003,delta=16.0, amsgrad-True, method-general, dropout-0.0, weight_decay-0.000000. \n",
    "        HR@1=0.17476  MRR@1=0.17476  NDCG@1=0.17476\n",
    "        HR@5=0.47018  MRR@5=0.28032  NDCG@5=0.32742\n",
    "        HR@10=0.61135  MRR@10=0.29927  NDCG@10=0.37318\n",
    "        HR@20=0.72339  MRR@20=0.30720  NDCG@20=0.40169"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-22T06:40:32.309405Z",
     "start_time": "2020-02-22T06:05:21.180964Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000.0
    },
    "colab_type": "code",
    "id": "S8pn0Z7bbv4t",
    "outputId": "c1aa9692-56a0-40d2-d99b-e75b4f1e537e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current model hyper-parameters: session_length=20, hidden_size=100, lr=0.0003,delta=16.0, amsgrad=True, method=general, dropout=0.0, weight_decay=0.000000. \n",
      "\n",
      "Start building the all training dataset\n",
      "The total number of training samples is： (3924324, 21)\n",
      "[2020-02-22 14:05:34] [1/50] 0 mean_batch_loss : 10.095714\n",
      "Start predicting 2020-02-22 14:06:39\n",
      "The total number of testing samples is： (40548, 21)\n",
      "change best\n",
      "testing finish [2020-02-22 14:06:42] \n",
      "\tHR@1=0.14620  MRR@1=0.14620  NDCG@1=0.14620\n",
      "\tHR@5=0.42032  MRR@5=0.24342  NDCG@5=0.28727\n",
      "\tHR@10=0.55810  MRR@10=0.26200  NDCG@10=0.33202\n",
      "\tHR@20=0.67125  MRR@20=0.26994  NDCG@20=0.36074\n",
      "[2020-02-22 14:06:42] [2/50] 0 mean_batch_loss : 4.543967\n",
      "Start predicting 2020-02-22 14:07:47\n",
      "change best\n",
      "testing finish [2020-02-22 14:07:50] \n",
      "\tHR@1=0.15685  MRR@1=0.15685  NDCG@1=0.15685\n",
      "\tHR@5=0.44426  MRR@5=0.25911  NDCG@5=0.30502\n",
      "\tHR@10=0.58671  MRR@10=0.27824  NDCG@10=0.35121\n",
      "\tHR@20=0.69836  MRR@20=0.28613  NDCG@20=0.37962\n",
      "[2020-02-22 14:07:50] [3/50] 0 mean_batch_loss : 4.107845\n",
      "Start predicting 2020-02-22 14:08:55\n",
      "change best\n",
      "testing finish [2020-02-22 14:08:57] \n",
      "\tHR@1=0.16368  MRR@1=0.16368  NDCG@1=0.16368\n",
      "\tHR@5=0.45349  MRR@5=0.26636  NDCG@5=0.31274\n",
      "\tHR@10=0.59692  MRR@10=0.28553  NDCG@10=0.35915\n",
      "\tHR@20=0.70926  MRR@20=0.29349  NDCG@20=0.38775\n",
      "[2020-02-22 14:08:58] [4/50] 0 mean_batch_loss : 3.989915\n",
      "Start predicting 2020-02-22 14:10:03\n",
      "change best\n",
      "testing finish [2020-02-22 14:10:05] \n",
      "\tHR@1=0.16516  MRR@1=0.16516  NDCG@1=0.16516\n",
      "\tHR@5=0.45879  MRR@5=0.26981  NDCG@5=0.31668\n",
      "\tHR@10=0.59904  MRR@10=0.28869  NDCG@10=0.36221\n",
      "\tHR@20=0.71177  MRR@20=0.29668  NDCG@20=0.39091\n",
      "[2020-02-22 14:10:06] [5/50] 0 mean_batch_loss : 3.986671\n",
      "Start predicting 2020-02-22 14:11:10\n",
      "change best\n",
      "testing finish [2020-02-22 14:11:13] \n",
      "\tHR@1=0.16652  MRR@1=0.16652  NDCG@1=0.16652\n",
      "\tHR@5=0.46153  MRR@5=0.27130  NDCG@5=0.31847\n",
      "\tHR@10=0.60388  MRR@10=0.29041  NDCG@10=0.36462\n",
      "\tHR@20=0.71594  MRR@20=0.29833  NDCG@20=0.39313\n",
      "[2020-02-22 14:11:13] [6/50] 0 mean_batch_loss : 3.841992\n",
      "Start predicting 2020-02-22 14:12:18\n",
      "change best\n",
      "testing finish [2020-02-22 14:12:20] \n",
      "\tHR@1=0.16763  MRR@1=0.16763  NDCG@1=0.16763\n",
      "\tHR@5=0.46446  MRR@5=0.27309  NDCG@5=0.32053\n",
      "\tHR@10=0.60504  MRR@10=0.29200  NDCG@10=0.36615\n",
      "\tHR@20=0.71626  MRR@20=0.29988  NDCG@20=0.39447\n",
      "[2020-02-22 14:12:21] [7/50] 0 mean_batch_loss : 4.084721\n",
      "Start predicting 2020-02-22 14:13:25\n",
      "change best\n",
      "testing finish [2020-02-22 14:13:28] \n",
      "\tHR@1=0.16881  MRR@1=0.16881  NDCG@1=0.16881\n",
      "\tHR@5=0.46380  MRR@5=0.27370  NDCG@5=0.32084\n",
      "\tHR@10=0.60501  MRR@10=0.29271  NDCG@10=0.36668\n",
      "\tHR@20=0.71875  MRR@20=0.30073  NDCG@20=0.39559\n",
      "[2020-02-22 14:13:28] [8/50] 0 mean_batch_loss : 3.875661\n",
      "Start predicting 2020-02-22 14:14:33\n",
      "change best\n",
      "testing finish [2020-02-22 14:14:36] \n",
      "\tHR@1=0.16820  MRR@1=0.16820  NDCG@1=0.16820\n",
      "\tHR@5=0.46491  MRR@5=0.27382  NDCG@5=0.32120\n",
      "\tHR@10=0.60755  MRR@10=0.29308  NDCG@10=0.36756\n",
      "\tHR@20=0.72036  MRR@20=0.30104  NDCG@20=0.39624\n",
      "[2020-02-22 14:14:36] [9/50] 0 mean_batch_loss : 3.943815\n",
      "Start predicting 2020-02-22 14:15:41\n",
      "change best\n",
      "testing finish [2020-02-22 14:15:43] \n",
      "\tHR@1=0.16935  MRR@1=0.16935  NDCG@1=0.16935\n",
      "\tHR@5=0.46636  MRR@5=0.27537  NDCG@5=0.32276\n",
      "\tHR@10=0.60730  MRR@10=0.29436  NDCG@10=0.36852\n",
      "\tHR@20=0.72078  MRR@20=0.30237  NDCG@20=0.39738\n",
      "[2020-02-22 14:15:44] [10/50] 0 mean_batch_loss : 3.879982\n",
      "Start predicting 2020-02-22 14:16:48\n",
      "change best\n",
      "testing finish [2020-02-22 14:16:51] \n",
      "\tHR@1=0.17079  MRR@1=0.17079  NDCG@1=0.17079\n",
      "\tHR@5=0.46668  MRR@5=0.27635  NDCG@5=0.32357\n",
      "\tHR@10=0.60718  MRR@10=0.29528  NDCG@10=0.36918\n",
      "\tHR@20=0.72058  MRR@20=0.30332  NDCG@20=0.39806\n",
      "[2020-02-22 14:16:51] [11/50] 0 mean_batch_loss : 3.856361\n",
      "Start predicting 2020-02-22 14:17:56\n",
      "change best\n",
      "testing finish [2020-02-22 14:17:59] \n",
      "\tHR@1=0.17194  MRR@1=0.17194  NDCG@1=0.17194\n",
      "\tHR@5=0.46880  MRR@5=0.27757  NDCG@5=0.32500\n",
      "\tHR@10=0.60967  MRR@10=0.29654  NDCG@10=0.37072\n",
      "\tHR@20=0.72105  MRR@20=0.30442  NDCG@20=0.39907\n",
      "[2020-02-22 14:17:59] [12/50] 0 mean_batch_loss : 3.846474\n",
      "Start predicting 2020-02-22 14:19:04\n",
      "change best\n",
      "testing finish [2020-02-22 14:19:06] \n",
      "\tHR@1=0.17254  MRR@1=0.17254  NDCG@1=0.17254\n",
      "\tHR@5=0.46870  MRR@5=0.27791  NDCG@5=0.32522\n",
      "\tHR@10=0.60846  MRR@10=0.29673  NDCG@10=0.37058\n",
      "\tHR@20=0.72189  MRR@20=0.30477  NDCG@20=0.39947\n",
      "[2020-02-22 14:19:07] [13/50] 0 mean_batch_loss : 3.796911\n",
      "Start predicting 2020-02-22 14:20:11\n",
      "testing finish [2020-02-22 14:20:14] \n",
      "\tHR@1=0.17088  MRR@1=0.17088  NDCG@1=0.17088\n",
      "\tHR@5=0.46838  MRR@5=0.27693  NDCG@5=0.32441\n",
      "\tHR@10=0.61103  MRR@10=0.29613  NDCG@10=0.37070\n",
      "\tHR@20=0.72129  MRR@20=0.30394  NDCG@20=0.39877\n",
      "[2020-02-22 14:20:14] [14/50] 0 mean_batch_loss : 3.774378\n",
      "Start predicting 2020-02-22 14:21:18\n",
      "testing finish [2020-02-22 14:21:21] \n",
      "\tHR@1=0.17214  MRR@1=0.17214  NDCG@1=0.17214\n",
      "\tHR@5=0.46996  MRR@5=0.27814  NDCG@5=0.32572\n",
      "\tHR@10=0.60997  MRR@10=0.29701  NDCG@10=0.37118\n",
      "\tHR@20=0.72161  MRR@20=0.30492  NDCG@20=0.39960\n",
      "[2020-02-22 14:21:21] [15/50] 0 mean_batch_loss : 3.859840\n",
      "Start predicting 2020-02-22 14:22:26\n",
      "change best\n",
      "testing finish [2020-02-22 14:22:29] \n",
      "\tHR@1=0.17170  MRR@1=0.17170  NDCG@1=0.17170\n",
      "\tHR@5=0.46893  MRR@5=0.27770  NDCG@5=0.32513\n",
      "\tHR@10=0.60891  MRR@10=0.29655  NDCG@10=0.37057\n",
      "\tHR@20=0.72272  MRR@20=0.30460  NDCG@20=0.39954\n",
      "[2020-02-22 14:22:29] [16/50] 0 mean_batch_loss : 3.753348\n",
      "Start predicting 2020-02-22 14:23:34\n",
      "testing finish [2020-02-22 14:23:36] \n",
      "\tHR@1=0.17071  MRR@1=0.17071  NDCG@1=0.17071\n",
      "\tHR@5=0.46984  MRR@5=0.27742  NDCG@5=0.32515\n",
      "\tHR@10=0.61071  MRR@10=0.29638  NDCG@10=0.37087\n",
      "\tHR@20=0.72260  MRR@20=0.30431  NDCG@20=0.39937\n",
      "[2020-02-22 14:23:37] [17/50] 0 mean_batch_loss : 3.707756\n",
      "Start predicting 2020-02-22 14:24:41\n",
      "change best\n",
      "testing finish [2020-02-22 14:24:44] \n",
      "\tHR@1=0.17219  MRR@1=0.17219  NDCG@1=0.17219\n",
      "\tHR@5=0.46939  MRR@5=0.27829  NDCG@5=0.32571\n",
      "\tHR@10=0.61029  MRR@10=0.29725  NDCG@10=0.37142\n",
      "\tHR@20=0.72233  MRR@20=0.30519  NDCG@20=0.39995\n",
      "[2020-02-22 14:24:44] [18/50] 0 mean_batch_loss : 3.899013\n",
      "Start predicting 2020-02-22 14:25:49\n",
      "testing finish [2020-02-22 14:25:52] \n",
      "\tHR@1=0.17069  MRR@1=0.17069  NDCG@1=0.17069\n",
      "\tHR@5=0.47085  MRR@5=0.27791  NDCG@5=0.32577\n",
      "\tHR@10=0.61174  MRR@10=0.29682  NDCG@10=0.37144\n",
      "\tHR@20=0.72221  MRR@20=0.30464  NDCG@20=0.39956\n",
      "[2020-02-22 14:25:52] [19/50] 0 mean_batch_loss : 3.652316\n",
      "Start predicting 2020-02-22 14:26:56\n",
      "change best\n",
      "testing finish [2020-02-22 14:26:59] \n",
      "\tHR@1=0.17310  MRR@1=0.17310  NDCG@1=0.17310\n",
      "\tHR@5=0.46952  MRR@5=0.27866  NDCG@5=0.32599\n",
      "\tHR@10=0.60923  MRR@10=0.29749  NDCG@10=0.37136\n",
      "\tHR@20=0.72307  MRR@20=0.30553  NDCG@20=0.40031\n",
      "[2020-02-22 14:26:59] [20/50] 0 mean_batch_loss : 3.774298\n",
      "Start predicting 2020-02-22 14:28:04\n",
      "testing finish [2020-02-22 14:28:07] \n",
      "\tHR@1=0.17234  MRR@1=0.17234  NDCG@1=0.17234\n",
      "\tHR@5=0.47013  MRR@5=0.27868  NDCG@5=0.32618\n",
      "\tHR@10=0.60945  MRR@10=0.29739  NDCG@10=0.37136\n",
      "\tHR@20=0.72243  MRR@20=0.30540  NDCG@20=0.40012\n",
      "[2020-02-22 14:28:07] [21/50] 0 mean_batch_loss : 3.850456\n",
      "Start predicting 2020-02-22 14:29:12\n",
      "testing finish [2020-02-22 14:29:14] \n",
      "\tHR@1=0.17212  MRR@1=0.17212  NDCG@1=0.17212\n",
      "\tHR@5=0.47016  MRR@5=0.27871  NDCG@5=0.32621\n",
      "\tHR@10=0.61034  MRR@10=0.29762  NDCG@10=0.37175\n",
      "\tHR@20=0.72277  MRR@20=0.30559  NDCG@20=0.40038\n",
      "[2020-02-22 14:29:15] [22/50] 0 mean_batch_loss : 3.709146\n",
      "Start predicting 2020-02-22 14:30:20\n",
      "testing finish [2020-02-22 14:30:22] \n",
      "\tHR@1=0.17283  MRR@1=0.17283  NDCG@1=0.17283\n",
      "\tHR@5=0.46920  MRR@5=0.27845  NDCG@5=0.32576\n",
      "\tHR@10=0.61002  MRR@10=0.29746  NDCG@10=0.37151\n",
      "\tHR@20=0.72179  MRR@20=0.30539  NDCG@20=0.39999\n",
      "[2020-02-22 14:30:22] [23/50] 0 mean_batch_loss : 3.845058\n",
      "Start predicting 2020-02-22 14:31:27\n",
      "testing finish [2020-02-22 14:31:30] \n",
      "\tHR@1=0.17069  MRR@1=0.17069  NDCG@1=0.17069\n",
      "\tHR@5=0.46996  MRR@5=0.27766  NDCG@5=0.32538\n",
      "\tHR@10=0.61029  MRR@10=0.29655  NDCG@10=0.37091\n",
      "\tHR@20=0.72248  MRR@20=0.30448  NDCG@20=0.39946\n",
      "[2020-02-22 14:31:30] [24/50] 0 mean_batch_loss : 3.926407\n",
      "Start predicting 2020-02-22 14:32:35\n",
      "change best\n",
      "testing finish [2020-02-22 14:32:37] \n",
      "\tHR@1=0.17448  MRR@1=0.17448  NDCG@1=0.17448\n",
      "\tHR@5=0.47016  MRR@5=0.27999  NDCG@5=0.32717\n",
      "\tHR@10=0.61078  MRR@10=0.29896  NDCG@10=0.37284\n",
      "\tHR@20=0.72203  MRR@20=0.30685  NDCG@20=0.40118\n",
      "[2020-02-22 14:32:38] [25/50] 0 mean_batch_loss : 3.770951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting 2020-02-22 14:33:42\n",
      "testing finish [2020-02-22 14:33:45] \n",
      "\tHR@1=0.17162  MRR@1=0.17162  NDCG@1=0.17162\n",
      "\tHR@5=0.47013  MRR@5=0.27841  NDCG@5=0.32598\n",
      "\tHR@10=0.61142  MRR@10=0.29748  NDCG@10=0.37188\n",
      "\tHR@20=0.72226  MRR@20=0.30531  NDCG@20=0.40008\n",
      "[2020-02-22 14:33:45] [26/50] 0 mean_batch_loss : 3.777514\n",
      "Start predicting 2020-02-22 14:34:51\n",
      "change best\n",
      "testing finish [2020-02-22 14:34:53] \n",
      "\tHR@1=0.17397  MRR@1=0.17397  NDCG@1=0.17397\n",
      "\tHR@5=0.47107  MRR@5=0.28005  NDCG@5=0.32743\n",
      "\tHR@10=0.61073  MRR@10=0.29886  NDCG@10=0.37277\n",
      "\tHR@20=0.72299  MRR@20=0.30679  NDCG@20=0.40132\n",
      "[2020-02-22 14:34:54] [27/50] 0 mean_batch_loss : 3.816503\n",
      "Start predicting 2020-02-22 14:35:58\n",
      "testing finish [2020-02-22 14:36:01] \n",
      "\tHR@1=0.17352  MRR@1=0.17352  NDCG@1=0.17352\n",
      "\tHR@5=0.46967  MRR@5=0.27961  NDCG@5=0.32678\n",
      "\tHR@10=0.61169  MRR@10=0.29871  NDCG@10=0.37285\n",
      "\tHR@20=0.72238  MRR@20=0.30655  NDCG@20=0.40104\n",
      "[2020-02-22 14:36:01] [28/50] 0 mean_batch_loss : 3.790324\n",
      "Start predicting 2020-02-22 14:37:06\n",
      "testing finish [2020-02-22 14:37:09] \n",
      "\tHR@1=0.17320  MRR@1=0.17320  NDCG@1=0.17320\n",
      "\tHR@5=0.47028  MRR@5=0.27940  NDCG@5=0.32675\n",
      "\tHR@10=0.60950  MRR@10=0.29811  NDCG@10=0.37191\n",
      "\tHR@20=0.72184  MRR@20=0.30605  NDCG@20=0.40049\n",
      "[2020-02-22 14:37:09] [29/50] 0 mean_batch_loss : 3.605835\n",
      "Start predicting 2020-02-22 14:38:14\n",
      "testing finish [2020-02-22 14:38:17] \n",
      "\tHR@1=0.17362  MRR@1=0.17362  NDCG@1=0.17362\n",
      "\tHR@5=0.47065  MRR@5=0.27943  NDCG@5=0.32685\n",
      "\tHR@10=0.60992  MRR@10=0.29822  NDCG@10=0.37209\n",
      "\tHR@20=0.72248  MRR@20=0.30619  NDCG@20=0.40075\n",
      "[2020-02-22 14:38:17] [30/50] 0 mean_batch_loss : 3.653809\n",
      "Start predicting 2020-02-22 14:39:22\n",
      "testing finish [2020-02-22 14:39:24] \n",
      "\tHR@1=0.17315  MRR@1=0.17315  NDCG@1=0.17315\n",
      "\tHR@5=0.47028  MRR@5=0.27913  NDCG@5=0.32654\n",
      "\tHR@10=0.61031  MRR@10=0.29799  NDCG@10=0.37200\n",
      "\tHR@20=0.72203  MRR@20=0.30592  NDCG@20=0.40046\n",
      "[2020-02-22 14:39:25] [31/50] 0 mean_batch_loss : 3.690441\n",
      "Start predicting 2020-02-22 14:40:29\n",
      "testing finish [2020-02-22 14:40:32] \n",
      "\tHR@1=0.17185  MRR@1=0.17185  NDCG@1=0.17185\n",
      "\tHR@5=0.47122  MRR@5=0.27877  NDCG@5=0.32650\n",
      "\tHR@10=0.61115  MRR@10=0.29761  NDCG@10=0.37192\n",
      "\tHR@20=0.72208  MRR@20=0.30547  NDCG@20=0.40017\n",
      "early stopping\n",
      "training and testting over, Total time 0:35:07.872827\n",
      "best model change\n",
      "current model hyper-parameters: session_length=20, hidden_size=100, lr=0.0003,delta=16.0, amsgrad=True, method=general, dropout=0.0, weight_decay=0.000000. \n",
      "\n",
      "current model HR@20=0.72299  MRR@20=0.30679.\n",
      "the best result so far. HR@20=0.72299  MRR@20=0.30679， hyper-parameters: session_length-20, hidden_size-100, lr-0.0003,delta=16.0, amsgrad-True, method-general, dropout-0.0, weight_decay-0.000000. \n",
      "\n",
      "The best result HR@20=0.72299  MRR@20=0.30679, hyper-parameters: session_length-20, hidden_size-100, lr-0.0003,delta=16.0, amsgrad-True, method-general, dropout-0.0, weight_decay-0.000000. \n",
      "over.\n"
     ]
    }
   ],
   "source": [
    "hidden_sizes = [100]\n",
    "\n",
    "# # YC64 RR DN\n",
    "# dropouts = [0.5]\n",
    "# lrs = [1e-3]\n",
    "# YC4\n",
    "dropouts = [0]\n",
    "lrs = [3e-4]\n",
    "\n",
    "attention_methods = [\"general\"]\n",
    "\n",
    "session_lengths = [20]\n",
    "weight_decays = [0]\n",
    "patience = 5\n",
    "deltas = [16]\n",
    "amsgrads = [True]\n",
    "best_params = \"\"\n",
    "best_all_model = 0.0\n",
    "best_all_hr = 0.0\n",
    "best_all_mrr = 0.0\n",
    "best_all_r1m = 0.0\n",
    "for session_length in session_lengths:\n",
    "    for hidden_size in hidden_sizes:\n",
    "        for amsgrad in amsgrads:\n",
    "            for attention_method in attention_methods:\n",
    "                for dropout in dropouts:\n",
    "                    for weight_decay in weight_decays:\n",
    "                        for lr in lrs:\n",
    "                            for delta in deltas:\n",
    "                                args = {}\n",
    "                                print(\"current model hyper-parameters: session_length=%d, hidden_size=%d, lr=%.4f,delta=%.1f, amsgrad=%s, method=%s, dropout=%.1f, weight_decay=%.6f. \\n\" % (session_length,hidden_size,lr,delta,str(amsgrad),attention_method,dropout,weight_decay))\n",
    "                                args[\"session_length\"] = session_length\n",
    "                                args[\"hidden_size\"] = hidden_size\n",
    "                                args[\"amsgrad\"] = amsgrad\n",
    "                                args[\"method\"] = attention_method\n",
    "                                args[\"dropout\"] = dropout\n",
    "                                args[\"weight_decay\"] = weight_decay\n",
    "                                args[\"lr\"] = lr\n",
    "                                args[\"delta\"] = delta\n",
    "                                args[\"patience\"] = patience\n",
    "                                best_model,best_model_hr,best_model_mrr = train(args)\n",
    "                                if best_model_hr + best_model_mrr > best_all_r1m:\n",
    "                                    print(\"best model change\")\n",
    "                                    best_all_r1m = best_model_hr + best_model_mrr\n",
    "                                    best_all_hr = best_model_hr\n",
    "                                    best_all_mrr = best_model_mrr\n",
    "                                    best_all_model = best_model\n",
    "                                    best_params = \"session_length-%d, hidden_size-%d, lr-%.4f,delta=%.1f, amsgrad-%s, method-%s, dropout-%.1f, weight_decay-%.6f\"%(session_length,hidden_size,lr,delta,str(amsgrad),attention_method,dropout,weight_decay)\n",
    "                                best_model = None\n",
    "                                print(\"current model hyper-parameters: session_length=%d, hidden_size=%d, lr=%.4f,delta=%.1f, amsgrad=%s, method=%s, dropout=%.1f, weight_decay=%.6f. \\n\" % (session_length,hidden_size,lr,delta,str(amsgrad),attention_method,dropout,weight_decay))\n",
    "                                print(\"current model HR@20=%.5f  MRR@20=%.5f.\"%(best_model_hr,best_model_mrr))\n",
    "                                print(\"the best result so far. HR@20=%.5f  MRR@20=%.5f， hyper-parameters: %s. \\n\"%(best_all_hr,best_all_mrr,best_params))\n",
    "print(\"The best result HR@20=%.5f  MRR@20=%.5f, hyper-parameters: %s. \"%(best_all_hr,best_all_mrr,best_params))\n",
    "print(\"over.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-22T06:40:32.314734Z",
     "start_time": "2020-02-22T06:40:32.310304Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "O8eiYy5UbyFR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.0000],\n",
       "        [1.7269],\n",
       "        [0.7762],\n",
       "        [0.9150],\n",
       "        [1.1490],\n",
       "        [1.1934],\n",
       "        [1.2849],\n",
       "        [1.3035],\n",
       "        [1.4496],\n",
       "        [1.4134],\n",
       "        [1.4859],\n",
       "        [1.4452],\n",
       "        [1.4093],\n",
       "        [1.4043],\n",
       "        [1.1061],\n",
       "        [1.2634],\n",
       "        [1.1614],\n",
       "        [1.0266],\n",
       "        [1.0742],\n",
       "        [0.9835],\n",
       "        [0.8442]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_all_model.position_weights.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-22T06:40:32.338037Z",
     "start_time": "2020-02-22T06:40:32.315514Z"
    }
   },
   "outputs": [],
   "source": [
    "# best_all_model.gate_weights.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "WDP-CE",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "smy",
   "language": "python",
   "name": "smy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
