{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T08:10:49.765664Z",
     "start_time": "2020-01-03T08:10:49.387882Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "MhcOrxNibMjb"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "import copy\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T08:10:49.769220Z",
     "start_time": "2020-01-03T08:10:49.766849Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "EGxgHeR6bN0K"
   },
   "outputs": [],
   "source": [
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda:0\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T08:10:49.777592Z",
     "start_time": "2020-01-03T08:10:49.771153Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "tBeDjO8dbPzP"
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "plot_num = 50000\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T08:10:49.803474Z",
     "start_time": "2020-01-03T08:10:49.778309Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "zef81a0tbRC4"
   },
   "outputs": [],
   "source": [
    "class SessionData(object):\n",
    "    def __init__(self,session_index,session_id,items_indexes):\n",
    "        self.session_index = session_index\n",
    "        self.session_id = session_id\n",
    "        self.item_list = items_indexes\n",
    "    def generate_seq_datas(self,session_length,padding_idx=0,predict_length=1):\n",
    "        sessions = []\n",
    "        if len(self.item_list)<2:\n",
    "            self.item_list.append[self.item_list[0]]\n",
    "        if predict_length==1:\n",
    "#             # when session length>=3\n",
    "#             for i in range(1,len(self.item_list)-1):\n",
    "            # when session length >=2\n",
    "            for i in range(len(self.item_list)-1):\n",
    "                if i <session_length:\n",
    "                    train_data = [0 for _ in range(session_length-i-1)]\n",
    "                    train_data.extend(self.item_list[:i+1])\n",
    "                    train_data.append(self.item_list[i+1])\n",
    "                else:\n",
    "                    train_data = self.item_list[i+1-session_length:i+1]\n",
    "                    train_data.append(self.item_list[i+1])\n",
    "                sessions.append(train_data)\n",
    "        else:\n",
    "            pass\n",
    "        return self.session_index,sessions\n",
    "    def __str__(self):\n",
    "        info = \" session index = {}\\n session id = {} \\n the length of item list= {} \\n the fisrt item index in item list is {}\".format(self.session_index,self.session_id,len(self.item_list),self.item_list[0])\n",
    "        return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T08:10:49.831464Z",
     "start_time": "2020-01-03T08:10:49.804472Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "hl6NiyNJbTRg"
   },
   "outputs": [],
   "source": [
    "class SessionDataSet(object):\n",
    "    def __init__(self,train_file,test_file,padding_idx=0):\n",
    "        super(SessionDataSet,self).__init__()\n",
    "        self.index_count = 0\n",
    "        self.session_count = 0\n",
    "        self.train_count = 0\n",
    "        self.test_count = 0\n",
    "        self.max_session_length = 0\n",
    "\n",
    "        self.padding_idx = padding_idx\n",
    "        self.item2index = dict()\n",
    "        self.index2item = dict()\n",
    "        self.session2index = dict()\n",
    "        self.index2session = dict()\n",
    "        self.item_total_num = dict()\n",
    "        self.item2index[\"<pad>\"] = padding_idx\n",
    "        self.index2item[padding_idx] = \"<pad>\"\n",
    "        self.train_data = self.load_data(train_file)\n",
    "        print(\"training set is loaded, # index: \",len(self.item2index.keys()))\n",
    "        self.train_count = self.session_count\n",
    "        print(\"train_session_num\",self.train_count)\n",
    "        self.test_data = self.load_data(test_file)\n",
    "        print(\"testing set is loaded, # index: \",len(self.index2item.keys()))\n",
    "        print(\"# item\",self.index_count)\n",
    "        self.test_count = self.session_count-self.train_count\n",
    "        print(\"# test session:\",self.test_count)\n",
    "        self.all_training_data = []\n",
    "        self.all_testing_data = []\n",
    "        self.all_meta_training_data = []\n",
    "        self.all_meta_testing_data = []\n",
    "        self.train_session_length = 0\n",
    "        self.test_session_length = 0\n",
    "    \n",
    "    def load_data(self,file_path):\n",
    "        data =  pickle.load(open(file_path, 'rb'))\n",
    "        session_ids = data[0]\n",
    "        session_data = data[1]\n",
    "        session_label = data[2]\n",
    "\n",
    "        result_data = []\n",
    "        lenth = len(session_ids)\n",
    "        print(\"# session\",lenth)\n",
    "\n",
    "        last_session_id = session_ids[0]\n",
    "        \n",
    "        session_item_indexes = []\n",
    "\n",
    "        for item_id in session_data[0]:\n",
    "            if item_id not in self.item2index.keys():\n",
    "                self.index_count+=1\n",
    "                self.item2index[item_id] = self.index_count\n",
    "                self.index2item[self.index_count] = item_id\n",
    "                self.item_total_num[self.index_count] = 0\n",
    "            session_item_indexes.append(self.item2index[item_id])\n",
    "            self.item_total_num[self.item2index[item_id]] += 1\n",
    "        target_item = session_label[0]\n",
    "        if target_item not in self.item2index.keys():\n",
    "            self.index_count+=1\n",
    "            self.item2index[target_item] = self.index_count\n",
    "            self.index2item[self.index_count] = target_item\n",
    "            self.item_total_num[self.index_count] = 0\n",
    "        session_item_indexes.append(self.item2index[target_item])\n",
    "        self.item_total_num[self.item2index[target_item]] += 1\n",
    "\n",
    "        for session_id,items,target_item in zip(session_ids,session_data,session_label):\n",
    "            if session_id!=last_session_id:\n",
    "\n",
    "                self.session_count+=1\n",
    "                self.session2index[last_session_id] = self.session_count\n",
    "                self.index2session[self.session_count] = last_session_id\n",
    "                if len(session_item_indexes)>self.max_session_length:\n",
    "                    self.max_session_length = len(session_item_indexes)\n",
    "                new_session = SessionData(self.session_count,last_session_id,session_item_indexes)\n",
    "                result_data.append(new_session)\n",
    "                last_session_id = session_id\n",
    "                session_item_indexes = []\n",
    "                for item_id in items:\n",
    "                    if item_id not in self.item2index.keys():\n",
    "                        self.index_count+=1\n",
    "                        self.item2index[item_id] = self.index_count\n",
    "                        self.index2item[self.index_count] = item_id\n",
    "                        self.item_total_num[self.index_count] = 0\n",
    "                    session_item_indexes.append(self.item2index[item_id])\n",
    "                    self.item_total_num[self.item2index[item_id]] += 1\n",
    "                if target_item not in self.item2index.keys():\n",
    "                    self.index_count+=1\n",
    "                    self.item2index[target_item] = self.index_count\n",
    "                    self.index2item[self.index_count] = target_item\n",
    "                    self.item_total_num[self.index_count] = 0\n",
    "                session_item_indexes.append(self.item2index[target_item])\n",
    "                self.item_total_num[self.item2index[target_item]] += 1\n",
    "            else:\n",
    "                # 每个id只处理一次\n",
    "                continue\n",
    "\n",
    "        self.session_count+=1\n",
    "        self.session2index[last_session_id] = self.session_count\n",
    "        new_session = SessionData(self.session_count,last_session_id,session_item_indexes)\n",
    "        result_data.append(new_session)\n",
    "        print(\"loaded\")\n",
    "        print(new_session)\n",
    "        \n",
    "        return result_data\n",
    "    \n",
    "\n",
    "    def get_batch(self,batch_size,session_length=10,predict_length=1,all_data=None,phase=\"train\",neg_num=1,sampling_mathod=\"random\"):\n",
    "\n",
    "        if phase == \"train\":\n",
    "            if all_data is None:\n",
    "                all_data = self.get_all_training_data(session_length)\n",
    "            indexes = np.random.permutation(all_data.shape[0])\n",
    "            all_data = all_data[indexes]\n",
    "        else:\n",
    "            if all_data is None:\n",
    "                all_data = self.get_all_testing_data(session_length)\n",
    "        \n",
    "        sindex = 0\n",
    "        eindex = batch_size\n",
    "        while eindex < all_data.shape[0]:\n",
    "            batch = all_data[sindex: eindex]\n",
    "\n",
    "            temp = eindex\n",
    "            eindex = eindex + batch_size\n",
    "            sindex = temp\n",
    "            if phase ==\"train\":\n",
    "                batch = self.divid_and_extend_negative_samples(batch,session_length=session_length,predict_length=predict_length,neg_num=neg_num,method=sampling_mathod)\n",
    "            else:\n",
    "                batch = [batch[:,:session_length],batch[:,session_length:]]\n",
    "            yield batch\n",
    "\n",
    "        if eindex >= all_data.shape[0]:\n",
    "            batch = all_data[sindex:]\n",
    "            if phase ==\"train\":\n",
    "                batch = self.divid_and_extend_negative_samples(batch,session_length=session_length,predict_length=predict_length,neg_num=neg_num,method=sampling_mathod)\n",
    "            else:\n",
    "                batch = [batch[:,:session_length],batch[:,session_length:]]\n",
    "            yield batch\n",
    "    \n",
    "    def get_batch_with_neg(self,batch_size,session_length=10,predict_length=1,all_data=None,phase=\"train\",neg_num=1,sampling_mathod=\"random\"):\n",
    "        if phase == \"train\":\n",
    "            all_data = self.get_all_training_data_with_neg(session_length,neg_num)\n",
    "            indexes = np.random.permutation(all_data.shape[0])\n",
    "            all_data = all_data[indexes]\n",
    "        else:\n",
    "            all_data = self.get_all_testing_data_with_neg(session_length,neg_num)\n",
    "        \n",
    "        sindex = 0\n",
    "        eindex = batch_size\n",
    "        while eindex < all_data.shape[0]:\n",
    "            batch = all_data[sindex: eindex]\n",
    "\n",
    "            temp = eindex\n",
    "            eindex = eindex + batch_size\n",
    "            sindex = temp\n",
    "            if phase ==\"train\":\n",
    "                batch = [batch[:,:session_length],batch[:,session_length:session_length+predict_length],batch[:,-neg_num:]]\n",
    "            else:\n",
    "                batch = [batch[:,:session_length],batch[:,session_length:]]\n",
    "            yield batch\n",
    "\n",
    "        if eindex >= all_data.shape[0]:\n",
    "            batch = all_data[sindex:]\n",
    "            if phase ==\"train\":\n",
    "                batch = [batch[:,:session_length],batch[:,session_length:session_length+predict_length],batch[:,-neg_num:]]\n",
    "            else:\n",
    "                batch = [batch[:,:session_length],batch[:,session_length:]]\n",
    "            yield batch\n",
    "    \n",
    "    def get_batch_tasks_with_neg(self,batch_size,session_length=10,predict_length=1,all_data=None,phase=\"train\",neg_num=1,sampling_mathod=\"random\"):\n",
    "        if phase == \"train\":\n",
    "            all_data = self.get_all_meta_training_data_with_neg(session_length,neg_num)\n",
    "            random.shuffle(all_data)\n",
    "        else:\n",
    "            all_data = self.get_all_meta_testing_data_with_neg(session_length,neg_num)\n",
    "        sindex = 0\n",
    "        eindex = batch_size\n",
    "        while eindex < len(all_data):\n",
    "            batch = all_data[sindex: eindex]\n",
    "\n",
    "            temp = eindex\n",
    "            eindex = eindex + batch_size\n",
    "            sindex = temp\n",
    "\n",
    "            session_items = [batch[i][:,:session_length] for i in range(len(batch))]\n",
    "\n",
    "            target_item = [batch[i][:,session_length:session_length+predict_length] for i in range(len(batch))]\n",
    "\n",
    "            neg_item = [batch[i][:,-neg_num:] for i in range(len(batch))]\n",
    "            batch = [session_items,target_item,neg_item]\n",
    "            yield batch\n",
    "\n",
    "        if eindex >= len(all_data):\n",
    "            batch = all_data[sindex:]\n",
    "            session_items = [batch[i][:,:session_length] for i in range(len(batch))]\n",
    "\n",
    "            target_item = [batch[i][:,session_length:session_length+predict_length] for i in range(len(batch))]\n",
    "\n",
    "            neg_item = [batch[i][:,-neg_num:] for i in range(len(batch))]\n",
    "            batch = [session_items,target_item,neg_item]\n",
    "            yield batch\n",
    "    \n",
    "    def divid_and_extend_negative_samples(self,batch_data,session_length,predict_length=1,neg_num=1,method=\"random\"):\n",
    "        \"\"\"\n",
    "        divid and extend negative samples\n",
    "        \"\"\"\n",
    "        neg_items = []\n",
    "        if method == \"random\":\n",
    "            for session_and_target in batch_data:\n",
    "                neg_item = []\n",
    "                for i in range(neg_num):\n",
    "                    rand_item = random.randint(1,self.index_count)\n",
    "                    while rand_item in session_and_target or rand_item in neg_item:\n",
    "                        rand_item = random.randint(1,self.index_count)\n",
    "                    neg_item.append(rand_item)\n",
    "                neg_items.append(neg_item)\n",
    "        else:\n",
    "\n",
    "            total_list = set()\n",
    "            for session in batch_data:\n",
    "                for i in session:\n",
    "                    total_list.add(i) \n",
    "            total_list = list(total_list)\n",
    "            total_list =  sorted(total_list, key=lambda item: self.item_total_num[item],reverse=True)\n",
    "            for i,session in enumerate(batch_data):\n",
    "                np.random.choice(total_list)\n",
    "        session_items = batch_data[:,:session_length]\n",
    "        target_item = batch_data[:,session_length:]\n",
    "        neg_items = np.array(neg_items)\n",
    "        return [session_items,target_item,neg_items]\n",
    "    \n",
    "    def get_all_training_data(self,session_length,predict_length=1):\n",
    "        if len(self.all_training_data)!=0 and self.train_session_length==session_length:\n",
    "#             print(\"The build is complete and there is no need to repeat the build\")\n",
    "            return self.all_training_data\n",
    "        print(\"Start building the all training dataset\")\n",
    "        all_sessions = []\n",
    "        for session_data in self.train_data:\n",
    "            # 前session_length为session，后predict_length为target_item\n",
    "            session_index,sessions = session_data.generate_seq_datas(session_length,padding_idx=self.padding_idx)\n",
    "            if sessions is not None:\n",
    "                all_sessions.extend(sessions)\n",
    "        all_sessions = np.array(all_sessions)\n",
    "        self.all_training_data = all_sessions\n",
    "        self.train_session_length=session_length\n",
    "        print(\"The total number of training samples is：\",all_sessions.shape)\n",
    "        return all_sessions\n",
    "    \n",
    "    def get_all_testing_data(self,session_length,predict_length=1):\n",
    "        if len(self.all_testing_data)!=0 and self.test_session_length==session_length:\n",
    "            return self.all_testing_data\n",
    "        all_sessions = []\n",
    "        for session_data in self.test_data:\n",
    "            session_index,sessions = session_data.generate_seq_datas(session_length,padding_idx=self.padding_idx)\n",
    "            if sessions is not None:\n",
    "                all_sessions.extend(sessions)\n",
    "        all_sessions = np.array(all_sessions)\n",
    "        self.all_testing_data = all_sessions\n",
    "        self.test_session_length=session_length\n",
    "        print(\"The total number of testing samples is：\",all_sessions.shape)\n",
    "        return all_sessions\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T08:10:51.173230Z",
     "start_time": "2020-01-03T08:10:49.832721Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "ctSu0HF8bUqh",
    "outputId": "7287ecc0-f73d-4883-c8f6-20c9b8aeddb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# session 719470\n",
      "loaded\n",
      " session index = 186670\n",
      " session id = 600377 \n",
      " the length of item list= 2 \n",
      " the fisrt item index in item list is 12270\n",
      "training set is loaded, # index:  43098\n",
      "train_session_num 186670\n",
      "# session 60858\n",
      "loaded\n",
      " session index = 202633\n",
      " session id = 600404 \n",
      " the length of item list= 2 \n",
      " the fisrt item index in item list is 19587\n",
      "testing set is loaded, # index:  43098\n",
      "# item 43097\n",
      "# test session: 15963\n"
     ]
    }
   ],
   "source": [
    "# dataset = SessionDataSet(train_file=\"../data/retailrocket/train.txt\",test_file=\"../data/srgnn/retailrocket/test.txt\")\n",
    "# dataset = SessionDataSet(train_file=\"../data/diginetica/train.txt\",test_file=\"../data/srgnn/diginetica/test.txt\")\n",
    "dataset = SessionDataSet(train_file=\"../data/yoochoose1_4/train.txt\",test_file=\"../data/srgnn/yoochoose1_4/test.txt\")\n",
    "# dataset = SessionDataSet(train_file=\"../data/yoochoose1_64/train.txt\",test_file=\"../data/srgnn/yoochoose1_64/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T08:10:51.181993Z",
     "start_time": "2020-01-03T08:10:51.174394Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "cm7qU4B6bq2i"
   },
   "outputs": [],
   "source": [
    "def bpr_loss(r):\n",
    "    return torch.sum(-torch.log(torch.sigmoid(r)))\n",
    "def get_hit_num(pred,y_truth):\n",
    "    \"\"\"\n",
    "        pred: numpy type(batch_size,k) \n",
    "        y_truth: list type (batch_size,groudtruth_num)\n",
    "    \"\"\"\n",
    "\n",
    "    hit_num = 0\n",
    "    for i in range(len(y_truth)):\n",
    "        for value in y_truth[i]:\n",
    "            hit_num += np.sum(pred[i]==value)\n",
    "    return hit_num\n",
    "\n",
    "def get_rr(pred,y_truth):\n",
    "    rr=0.\n",
    "    for i in range(len(y_truth)):\n",
    "        for value in y_truth[i]:\n",
    "            hit_indexes = np.where(pred[i]==value)[0]\n",
    "            for hit_index in hit_indexes:\n",
    "                rr += 1/(hit_index+1)\n",
    "    return rr\n",
    "\n",
    "def get_dcg(pred,y_truth):\n",
    "    y_pred_score = np.zeros_like(pred)\n",
    "\n",
    "    for i in range(len(y_truth)):\n",
    "\n",
    "        for j,y_pred in enumerate(pred[i]):\n",
    "            if y_pred == y_truth[i][0]:\n",
    "                y_pred_score[i][j]=1\n",
    "    gain = 2 ** y_pred_score - 1\n",
    "    discounts = np.tile(np.log2(np.arange(pred.shape[1]) + 2),(len(y_truth),1))\n",
    "    dcg = np.sum(gain / discounts,axis=1)\n",
    "    return dcg\n",
    "\n",
    "def get_ndcg(pred,y_truth):\n",
    "    dcg = get_dcg(pred, y_truth)\n",
    "    idcg = get_dcg(np.concatenate((y_truth,np.zeros_like(pred)[:,:-1]-1),axis=1), y_truth)\n",
    "    ndcg = np.sum(dcg / idcg)\n",
    "\n",
    "    return ndcg\n",
    "\n",
    "def dcg_score(y_pre, y_true, k):\n",
    "    y_pre_score = np.zeros(k)\n",
    "    if len(y_pre) > k:\n",
    "        y_pre = y_pre[:k]\n",
    "    for i in range(len(y_pre)):\n",
    "        pre_tag = y_pre[i]\n",
    "        if pre_tag in y_true:\n",
    "            y_pre_score[i] = 1\n",
    "    gain = 2 ** y_pre_score - 1\n",
    "    discounts = np.log2(np.arange(k) + 2)\n",
    "    return np.sum(gain / discounts)\n",
    "\n",
    "\n",
    "def ndcg_score(y_pre, y_true, k=5):\n",
    "    dcg = dcg_score(y_pre, y_true, k)\n",
    "    idcg = dcg_score(y_true, y_true, k)\n",
    "    return dcg / idcg\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T08:10:51.209104Z",
     "start_time": "2020-01-03T08:10:51.182883Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "QdNvTTApbr_g"
   },
   "outputs": [],
   "source": [
    "# SelfAttention Layer\n",
    "class SelfAttention(torch.nn.Module):\n",
    "    def __init__(self, method, hidden_size,activate=\"selu\",dropout=0):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.config = list()\n",
    "        # 使用的Attention方法\n",
    "        self.method = method\n",
    "        # 隐藏层大小\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method not in ['dot', 'general']:\n",
    "            raise ValueError(self.method, \"Attention method do not exists.\")\n",
    "\n",
    "        if self.method == \"dot\":\n",
    "            self.query = torch.nn.Linear(self.hidden_size *2, self.hidden_size*2)\n",
    "            self.key = torch.nn.Linear(self.hidden_size*2, self.hidden_size*2)\n",
    "            torch.nn.init.constant_(self.query.bias,0)\n",
    "            torch.nn.init.constant_(self.key.bias,0)\n",
    "\n",
    "        if self.method == \"general\":\n",
    "            self.attention = torch.nn.Linear(self.hidden_size*2, self.hidden_size*2)\n",
    "            torch.nn.init.constant_(self.attention.bias,0)\n",
    "        \n",
    "        if activate == \"relu\":\n",
    "            self.activate = torch.relu\n",
    "        elif activate == \"tanh\":\n",
    "            self.activate = torch.tanh\n",
    "        elif activate == \"elu\":\n",
    "            self.activate = torch.nn.ELU()\n",
    "        elif activate == \"selu\":\n",
    "            self.activate = torch.selu\n",
    "        else:\n",
    "            self.activate = torch.sigmoid\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        torch.nn.utils.clip_grad_norm_(self.parameters(),max_norm=110)\n",
    "\n",
    "    def dot_score(self, encoder_output,is_train=True,weights=None):\n",
    "\n",
    "        if weights is None:\n",
    "            if is_train:\n",
    "                query = self.dropout(self.activate(self.query(encoder_output)))\n",
    "                key = self.dropout(self.activate(self.key(encoder_output)))\n",
    "            else:\n",
    "                query = self.activate(self.query(encoder_output))\n",
    "                key = self.activate(self.key(encoder_output))\n",
    "        else:\n",
    "            query = self.activate(torch.matmul(encoder_output,weights[0].t())+weights[1])\n",
    "            key = self.activate(torch.matmul(encoder_output,weights[2].t())+weights[3])\n",
    "        dot = query.bmm(key.permute(0, 2, 1))\n",
    "        return dot\n",
    "\n",
    "    def general_score(self, encoder_output,is_train=True,weights=None):\n",
    "        if weights is None:\n",
    "            if is_train:\n",
    "                energy = self.dropout(self.activate(self.attention(encoder_output)))\n",
    "            else:\n",
    "                energy = self.activate(self.attention(encoder_output))\n",
    "        else:\n",
    "            energy = self.activate(torch.matmul(encoder_output,weights[0].t())+weights[1])\n",
    "        return encoder_output.bmm(energy.permute(0, 2, 1))\n",
    "\n",
    "    def forward(self, encoder_outputs, mask=None,is_train=True):\n",
    "        # (batch_size,length,dim)\n",
    "        if self.method == \"general\":\n",
    "            attention_energies = self.general_score(encoder_outputs,is_train=is_train)\n",
    "        elif self.method == \"dot\":\n",
    "            attention_energies = self.dot_score(encoder_outputs,is_train=is_train)\n",
    "\n",
    "        #  (batch_size,length,length)\n",
    "        attention_energies.div_(torch.sqrt(torch.tensor(self.hidden_size, dtype=torch.float)))\n",
    "        if mask is not None:\n",
    "            new_mask = (1 - (1 - mask.float()).unsqueeze(1).permute(0, 2, 1).bmm(\n",
    "                (1 - mask.float()).unsqueeze(1)))\n",
    "\n",
    "            attention_energies = attention_energies - new_mask*1e12\n",
    "            weights = F.softmax(attention_energies, dim=2)\n",
    "            weights = weights*(1-new_mask)\n",
    "            # batch_size,length,length)*(batch_size,length,dim)->(batch_size,length,dim)->(batch_size,1,dim)->(batch_size,dim)\n",
    "            outputs = weights.bmm(encoder_outputs)\n",
    "            outputs.div_(mask.shape[1]-torch.sum(mask,dim=1).unsqueeze(1).unsqueeze(2).repeat((1,mask.shape[1],outputs.shape[2])).float())\n",
    "            outputs = outputs.sum(dim=1).squeeze(1)\n",
    "        else:\n",
    "            weights = F.softmax(attention_energies, dim=2)\n",
    "            # (batch_size,length,length)*(batch_size,length,dim)->(batch_size,length,dim)->(batch_size,1,dim)->(batch_size,dim)\n",
    "            outputs = (weights.bmm(encoder_outputs).sum(dim=1) / encoder_outputs.shape[1]).squeeze(1)\n",
    "        sa_weights = weights.sum(dim=1).squeeze(1)\n",
    "        return outputs, sa_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T08:10:51.292960Z",
     "start_time": "2020-01-03T08:10:51.211134Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "aVIrdlmKbtUq"
   },
   "outputs": [],
   "source": [
    "class POEM(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=64, itemNum=0, posNum=0, padding_idx=0, dropout=0.5,attention_method=\"dot\",head_num=4,\n",
    "                 activate=\"selu\",session_length=20,delta=16.0):\n",
    "        super(POEM, self).__init__()\n",
    "        self.padding_idx = padding_idx\n",
    "        self.hidden_size = hidden_size\n",
    "        self.head_num = head_num\n",
    "        self.delta = delta\n",
    "        self.session_length = session_length\n",
    "        if activate == \"sigmoid\":\n",
    "            self.activate = torch.sigmoid\n",
    "        elif activate == \"tanh\":\n",
    "            self.activate = torch.tanh\n",
    "        elif activate == \"relu\":\n",
    "            self.activate = torch.relu\n",
    "        elif activate == \"elu\":\n",
    "            self.activate = torch.nn.ELU()\n",
    "        else:\n",
    "            self.activate = torch.selu\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "        self.item_embedding = torch.nn.Embedding(itemNum, hidden_size, padding_idx=self.padding_idx,max_norm=1.5)\n",
    "        \n",
    "        self.position_embedding = torch.nn.Embedding(posNum,hidden_size,padding_idx=self.padding_idx,max_norm=1.5)\n",
    "    \n",
    "        self.position_weights = torch.nn.Embedding(posNum,1,padding_idx=self.padding_idx)\n",
    "        \n",
    "        self.self_attention = SelfAttention(attention_method, hidden_size,activate=activate,dropout=dropout).to(device)\n",
    "        torch.nn.init.constant_(self.item_embedding.weight[0],0)\n",
    "        torch.nn.init.constant_(self.position_embedding.weight[0],0)\n",
    "        torch.nn.init.constant_(self.position_weights.weight,1)\n",
    "        torch.nn.init.constant_(self.position_weights.weight[0],0)\n",
    "        \n",
    "        self.gen_mlp = torch.nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.cur_mlp = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.deep_mlp = torch.nn.Linear(hidden_size*3, hidden_size,bias=False)\n",
    "        \n",
    "    def forward(self, session,item=None,bpr_loss=False,neg_num=50):\n",
    "\n",
    "        mask = (session!=0).float()\n",
    "        length = torch.sum(mask,1).unsqueeze(1).expand((session.shape[0],self.hidden_size))\n",
    "        mask = mask.unsqueeze(2).expand((session.shape[0],session.shape[1],self.hidden_size))\n",
    "        session_item_embeddings = self.item_embedding(session) * mask\n",
    "        positions = session.shape[1] - torch.arange(0,session.shape[1]).unsqueeze(0).expand_as(session).to(device)\n",
    "        session_position_embeddings = self.dropout(self.position_embedding(positions))*mask\n",
    "        session_item_vecs = torch.cat((session_item_embeddings,session_position_embeddings), dim=2)\n",
    "        attention_mask = (session == self.padding_idx)\n",
    "        sa_output, sa_weights = self.self_attention(session_item_vecs, attention_mask)\n",
    "        session_position_weights = self.dropout(self.position_weights(positions))*mask\n",
    "        sa_weights = sa_weights.unsqueeze(2).expand_as(session_item_embeddings)\n",
    "        session_item_vecs2 = session_item_embeddings * session_position_weights * sa_weights\n",
    "        psa_output = torch.sum(session_item_vecs2, dim=1)/length\n",
    "        gen_output = self.dropout(self.activate(self.gen_mlp(sa_output)))\n",
    "        cur_output = self.dropout(self.activate(self.cur_mlp(session_item_embeddings[:,-1])))\n",
    "        deep_output = self.dropout(self.activate(self.deep_mlp(torch.cat((sa_output,session_item_embeddings[:,-1]),1))))\n",
    "        session_output =  F.normalize(gen_output * cur_output + deep_output + psa_output,dim=-1)\n",
    "        session_output = session_output*self.delta\n",
    "        item_embedding_weight = F.normalize(self.item_embedding.weight[1:],dim=-1)\n",
    "        result = torch.matmul(session_output,item_embedding_weight.t())\n",
    "        return result\n",
    "    \n",
    "    def predict_top_k(self, session, k=20):\n",
    "        mask = (session!=0).float()\n",
    "        length = torch.sum(mask,1).unsqueeze(1).expand((session.shape[0],self.hidden_size))\n",
    "        mask = mask.unsqueeze(2).expand((session.shape[0],session.shape[1],self.hidden_size))\n",
    "        session_item_embeddings = self.item_embedding(session) * mask\n",
    "        positions = session.shape[1] - torch.arange(0,session.shape[1]).unsqueeze(0).expand_as(session).to(device)\n",
    "        session_position_embeddings = self.position_embedding(positions)*mask\n",
    "        session_item_vecs = torch.cat((session_item_embeddings,session_position_embeddings), dim=2)\n",
    "        attention_mask = (session == self.padding_idx)\n",
    "        sa_output, sa_weights = self.self_attention(session_item_vecs, attention_mask,is_train=False)\n",
    "        session_position_weights = self.position_weights(positions)*mask\n",
    "        sa_weights = sa_weights.unsqueeze(2).expand_as(session_item_embeddings)\n",
    "        session_item_vecs2 = session_item_embeddings * session_position_weights * sa_weights\n",
    "        psa_output = torch.sum(session_item_vecs2, dim=1)/length\n",
    "        gen_output =self.activate(self.gen_mlp(sa_output))\n",
    "\n",
    "        cur_output = self.activate(self.cur_mlp(session_item_embeddings[:,-1]))\n",
    "        deep_output = self.activate(self.deep_mlp(torch.cat((sa_output,session_item_embeddings[:,-1]),1)))\n",
    "        session_output =  F.normalize(gen_output * cur_output + deep_output + psa_output,dim=-1)\n",
    "        session_output = session_output * self.delta\n",
    "        item_embedding_weight = F.normalize(self.item_embedding.weight[1:],dim=-1)\n",
    "        result = torch.matmul(session_output,item_embedding_weight.t())\n",
    "        result = torch.topk(result,k,dim=1)[1]\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T08:10:51.305717Z",
     "start_time": "2020-01-03T08:10:51.294065Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Jhrg56xebung"
   },
   "outputs": [],
   "source": [
    "epochs=50\n",
    "def train(args):\n",
    "    hidden_size = args[\"hidden_size\"] if \"hidden_size\" in args.keys() else 100\n",
    "    dropout = args[\"dropout\"] if \"dropout\" in args.keys()  else 0.5\n",
    "    attention_method = args[\"method\"] if \"method\" in args.keys()  else \"general\"\n",
    "    lr = args[\"lr\"] if \"lr\" in args.keys()  else 5e-4\n",
    "    weight_decay = args[\"weight_decay\"] if \"weight_decay\" in args.keys()  else 1e-5\n",
    "    amsgrad = args[\"amsgrad\"] if \"amsgrad\" in args.keys() else True\n",
    "    session_length = args[\"session_length\"] if \"session_length\" in args.keys() else 20\n",
    "    delta = args[\"delta\"] if \"delta\" in args.keys() else 20\n",
    "    model = POEM(hidden_size=hidden_size, itemNum=dataset.index_count+1, posNum=session_length+1, padding_idx=0, dropout=dropout,\n",
    "                 activate=\"selu\",attention_method=attention_method,delta=delta).to(device)\n",
    "    opti = torch.optim.Adam(model.parameters(),lr=lr,weight_decay=weight_decay,amsgrad=amsgrad)\n",
    "    patience = args[\"patience\"] if \"patience\" in args.keys() else 5\n",
    "    best_model_hr = 0.0\n",
    "    best_model_mrr = 0.0\n",
    "    best_r1m = 0.0\n",
    "    best_model = None\n",
    "    predict_nums = [1,5,10,20]\n",
    "    no_improvement_epoch = 0\n",
    "    for epoch in range(epochs):\n",
    "        batch_losses = []\n",
    "        epoch_losses = []\n",
    "        for i,batch_data in enumerate(dataset.get_batch(batch_size,session_length,phase=\"train\")):\n",
    "            sessions = torch.tensor(batch_data[0]).to(device)\n",
    "            target_items = torch.tensor(batch_data[1]).squeeze().to(device)-1\n",
    "            result_pos = model(sessions)\n",
    "            loss = loss_function(result_pos,target_items)\n",
    "            opti.zero_grad()\n",
    "            loss.backward()\n",
    "            opti.step()\n",
    "            batch_losses.append(loss.cpu().detach().numpy())\n",
    "            epoch_losses.append(loss.cpu().detach().numpy())\n",
    "            if i % plot_num == 0:\n",
    "                time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                print(\"[%s] [%d/%d] %d mean_batch_loss : %0.6f\" % (time, epoch+1, epochs, i, np.mean(batch_losses)))\n",
    "                batch_losses = []\n",
    "        with torch.no_grad():\n",
    "            start_test_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(\"Start predicting\",start_test_time)\n",
    "            rrs = [0 for _ in range(len(predict_nums))]\n",
    "            hit_nums = [0 for _ in range(len(predict_nums))]\n",
    "            ndcgs = [0 for _ in range(len(predict_nums))]\n",
    "            for i,batch_data in enumerate(dataset.get_batch(batch_size,session_length,phase=\"test\")):\n",
    "                \n",
    "                sessions = torch.tensor(batch_data[0]).to(device)\n",
    "                target_items = np.array(batch_data[1])-1\n",
    "                y_pred = model.predict_top_k(sessions,20).cpu().numpy()\n",
    "                \n",
    "                for j,predict_num in enumerate(predict_nums):\n",
    "                    hit_nums[j]+=get_hit_num(y_pred[:,:predict_num],target_items)\n",
    "                    rrs[j]+=get_rr(y_pred[:,:predict_num],target_items)\n",
    "                    ndcgs[j]+=get_ndcg(y_pred[:,:predict_num],target_items)\n",
    "                    \n",
    "            end_test_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            hrs = [hit_num/len(dataset.all_testing_data) for hit_num in hit_nums]\n",
    "            mrrs = [rr/len(dataset.all_testing_data) for rr in rrs]\n",
    "            mndcgs = [ndcg/len(dataset.all_testing_data) for ndcg in ndcgs]\n",
    "            if hrs[-1] + mrrs[-1] > best_r1m:\n",
    "                print(\"change best\")\n",
    "                best_model = deepcopy(model)\n",
    "                best_model_hr = hrs[-1]\n",
    "                best_model_mrr = mrrs[-1]\n",
    "                best_r1m = hrs[-1] + mrrs[-1]\n",
    "                no_improvement_epoch = 0\n",
    "            else:\n",
    "                no_improvement_epoch +=1\n",
    "            print(\"testing finish [%s] \"%end_test_time)\n",
    "            for k,predict_num in enumerate(predict_nums):\n",
    "                print(\"\\tHR@%d=%.5f  MRR@%d=%.5f  NDCG@%d=%.5f\"%(predict_num,hrs[k],predict_num,mrrs[k],predict_num,mndcgs[k]))\n",
    "        if no_improvement_epoch>=patience:\n",
    "            print(\"early stopping\")\n",
    "            break\n",
    "    return best_model,best_model_hr,best_model_mrr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2H2SFJ7F77Z0"
   },
   "source": [
    "# CIKM-Session>2\n",
    "    HR@20=0.66568  MRR@20=0.31938, hyper-parameters: session_length-20, hidden_size-100, lr-0.0010,delta=16.0, amsgrad-True, method-general, dropout-0.5, weight_decay-0.000000. \n",
    "        HR@1=0.20533  MRR@1=0.20533  NDCG@1=0.20533\n",
    "        HR@5=0.45578  MRR@5=0.29791  NDCG@5=0.33723\n",
    "        HR@10=0.56246  MRR@10=0.31218  NDCG@10=0.37176\n",
    "        HR@20=0.66568  MRR@20=0.31938  NDCG@20=0.39790\n",
    "# RR-Session>2\n",
    "    HR@20=0.63213  MRR@20=0.36563, hyper-parameters: session_length-20, hidden_size-100, lr-0.0010,delta=16.0, amsgrad-True, method-general, dropout-0.5, weight_decay-0.000000. \n",
    "        HR@1=0.27291  MRR@1=0.27291  NDCG@1=0.27291\n",
    "        HR@5=0.47991  MRR@5=0.34995  NDCG@5=0.38234\n",
    "        HR@10=0.55852  MRR@10=0.36047  NDCG@10=0.40779\n",
    "        HR@20=0.63213  MRR@20=0.36563  NDCG@20=0.42647\n",
    "# RSC64-Session>2\n",
    "    HR@20=0.72001  MRR@20=0.31753， hyper-parameters: session_length-20, hidden_size-100, lr-0.0010,delta=16.0, amsgrad-True, method-general, dropout-0.5, weight_decay-0.000000. \n",
    "        HR@1=0.18478  MRR@1=0.18478  NDCG@1=0.18478\n",
    "        HR@5=0.48122  MRR@5=0.29213  NDCG@5=0.33912\n",
    "        HR@10=0.61353  MRR@10=0.31000  NDCG@10=0.38212\n",
    "        HR@20=0.72001  MRR@20=0.31753  NDCG@20=0.40921\n",
    "# RSC4-Session>2\n",
    "    HR@20=0.72514  MRR@20=0.32343, hyper-parameters: session_length-20, hidden_size-100, lr-0.0003,delta=16.0, amsgrad-True, method-general, dropout-0.0, weight_decay-0.000000. \n",
    "        HR@1=0.19038  MRR@1=0.19038  NDCG@1=0.19038\n",
    "        HR@5=0.48971  MRR@5=0.29858  NDCG@5=0.34607\n",
    "        HR@10=0.61949  MRR@10=0.31601  NDCG@10=0.38814\n",
    "        HR@20=0.72514  MRR@20=0.32343  NDCG@20=0.41497"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T08:22:50.101986Z",
     "start_time": "2020-01-03T08:10:51.306588Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "S8pn0Z7bbv4t",
    "outputId": "c1aa9692-56a0-40d2-d99b-e75b4f1e537e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current model hyper-parameters: session_length=20, hidden_size=100, lr=0.0010,delta=16.0, amsgrad=True, method=general, dropout=0.5, weight_decay=0.000000. \n",
      "\n",
      "Start building the all training dataset\n",
      "The total number of training samples is： (719470, 21)\n",
      "[2020-01-03 16:10:56] [1/50] 0 mean_batch_loss : 11.505555\n",
      "Start predicting 2020-01-03 16:11:10\n",
      "The total number of testing samples is： (60858, 21)\n",
      "change best\n",
      "testing finish [2020-01-03 16:11:15] \n",
      "\tHR@1=0.20080  MRR@1=0.20080  NDCG@1=0.20080\n",
      "\tHR@5=0.29513  MRR@5=0.23547  NDCG@5=0.25032\n",
      "\tHR@10=0.34094  MRR@10=0.24159  NDCG@10=0.26514\n",
      "\tHR@20=0.38577  MRR@20=0.24473  NDCG@20=0.27651\n",
      "[2020-01-03 16:11:15] [2/50] 0 mean_batch_loss : 7.147748\n",
      "Start predicting 2020-01-03 16:11:29\n",
      "change best\n",
      "testing finish [2020-01-03 16:11:33] \n",
      "\tHR@1=0.20025  MRR@1=0.20025  NDCG@1=0.20025\n",
      "\tHR@5=0.36414  MRR@5=0.26043  NDCG@5=0.28623\n",
      "\tHR@10=0.44305  MRR@10=0.27095  NDCG@10=0.31174\n",
      "\tHR@20=0.52264  MRR@20=0.27646  NDCG@20=0.33184\n",
      "[2020-01-03 16:11:34] [3/50] 0 mean_batch_loss : 5.610943\n",
      "Start predicting 2020-01-03 16:11:48\n",
      "change best\n",
      "testing finish [2020-01-03 16:11:52] \n",
      "\tHR@1=0.20428  MRR@1=0.20428  NDCG@1=0.20428\n",
      "\tHR@5=0.39710  MRR@5=0.27508  NDCG@5=0.30544\n",
      "\tHR@10=0.48467  MRR@10=0.28674  NDCG@10=0.33373\n",
      "\tHR@20=0.57692  MRR@20=0.29315  NDCG@20=0.35706\n",
      "[2020-01-03 16:11:52] [4/50] 0 mean_batch_loss : 4.872953\n",
      "Start predicting 2020-01-03 16:12:07\n",
      "change best\n",
      "testing finish [2020-01-03 16:12:11] \n",
      "\tHR@1=0.20678  MRR@1=0.20678  NDCG@1=0.20678\n",
      "\tHR@5=0.41245  MRR@5=0.28286  NDCG@5=0.31514\n",
      "\tHR@10=0.50720  MRR@10=0.29551  NDCG@10=0.34578\n",
      "\tHR@20=0.60367  MRR@20=0.30224  NDCG@20=0.37022\n",
      "[2020-01-03 16:12:11] [5/50] 0 mean_batch_loss : 4.317741\n",
      "Start predicting 2020-01-03 16:12:26\n",
      "change best\n",
      "testing finish [2020-01-03 16:12:30] \n",
      "\tHR@1=0.20727  MRR@1=0.20727  NDCG@1=0.20727\n",
      "\tHR@5=0.42318  MRR@5=0.28743  NDCG@5=0.32125\n",
      "\tHR@10=0.52113  MRR@10=0.30055  NDCG@10=0.35297\n",
      "\tHR@20=0.62013  MRR@20=0.30741  NDCG@20=0.37800\n",
      "[2020-01-03 16:12:30] [6/50] 0 mean_batch_loss : 4.316865\n",
      "Start predicting 2020-01-03 16:12:44\n",
      "change best\n",
      "testing finish [2020-01-03 16:12:48] \n",
      "\tHR@1=0.20870  MRR@1=0.20870  NDCG@1=0.20870\n",
      "\tHR@5=0.43133  MRR@5=0.29098  NDCG@5=0.32592\n",
      "\tHR@10=0.53083  MRR@10=0.30429  NDCG@10=0.35813\n",
      "\tHR@20=0.63052  MRR@20=0.31120  NDCG@20=0.38333\n",
      "[2020-01-03 16:12:48] [7/50] 0 mean_batch_loss : 4.183086\n",
      "Start predicting 2020-01-03 16:13:03\n",
      "change best\n",
      "testing finish [2020-01-03 16:13:07] \n",
      "\tHR@1=0.20789  MRR@1=0.20789  NDCG@1=0.20789\n",
      "\tHR@5=0.43572  MRR@5=0.29249  NDCG@5=0.32817\n",
      "\tHR@10=0.53682  MRR@10=0.30600  NDCG@10=0.36089\n",
      "\tHR@20=0.63707  MRR@20=0.31298  NDCG@20=0.38626\n",
      "[2020-01-03 16:13:07] [8/50] 0 mean_batch_loss : 3.956873\n",
      "Start predicting 2020-01-03 16:13:22\n",
      "change best\n",
      "testing finish [2020-01-03 16:13:26] \n",
      "\tHR@1=0.20827  MRR@1=0.20827  NDCG@1=0.20827\n",
      "\tHR@5=0.43998  MRR@5=0.29418  NDCG@5=0.33051\n",
      "\tHR@10=0.54269  MRR@10=0.30791  NDCG@10=0.36374\n",
      "\tHR@20=0.64286  MRR@20=0.31487  NDCG@20=0.38907\n",
      "[2020-01-03 16:13:26] [9/50] 0 mean_batch_loss : 4.020873\n",
      "Start predicting 2020-01-03 16:13:41\n",
      "change best\n",
      "testing finish [2020-01-03 16:13:45] \n",
      "\tHR@1=0.20873  MRR@1=0.20873  NDCG@1=0.20873\n",
      "\tHR@5=0.44316  MRR@5=0.29581  NDCG@5=0.33253\n",
      "\tHR@10=0.54525  MRR@10=0.30947  NDCG@10=0.36557\n",
      "\tHR@20=0.64680  MRR@20=0.31652  NDCG@20=0.39125\n",
      "[2020-01-03 16:13:45] [10/50] 0 mean_batch_loss : 3.870204\n",
      "Start predicting 2020-01-03 16:13:59\n",
      "change best\n",
      "testing finish [2020-01-03 16:14:04] \n",
      "\tHR@1=0.20822  MRR@1=0.20822  NDCG@1=0.20822\n",
      "\tHR@5=0.44459  MRR@5=0.29625  NDCG@5=0.33323\n",
      "\tHR@10=0.54783  MRR@10=0.31009  NDCG@10=0.36667\n",
      "\tHR@20=0.64976  MRR@20=0.31718  NDCG@20=0.39247\n",
      "[2020-01-03 16:14:04] [11/50] 0 mean_batch_loss : 3.947713\n",
      "Start predicting 2020-01-03 16:14:18\n",
      "change best\n",
      "testing finish [2020-01-03 16:14:22] \n",
      "\tHR@1=0.20885  MRR@1=0.20885  NDCG@1=0.20885\n",
      "\tHR@5=0.44683  MRR@5=0.29739  NDCG@5=0.33464\n",
      "\tHR@10=0.55151  MRR@10=0.31135  NDCG@10=0.36848\n",
      "\tHR@20=0.65114  MRR@20=0.31830  NDCG@20=0.39371\n",
      "[2020-01-03 16:14:22] [12/50] 0 mean_batch_loss : 3.884471\n",
      "Start predicting 2020-01-03 16:14:37\n",
      "change best\n",
      "testing finish [2020-01-03 16:14:41] \n",
      "\tHR@1=0.20832  MRR@1=0.20832  NDCG@1=0.20832\n",
      "\tHR@5=0.44878  MRR@5=0.29786  NDCG@5=0.33548\n",
      "\tHR@10=0.55237  MRR@10=0.31174  NDCG@10=0.36904\n",
      "\tHR@20=0.65364  MRR@20=0.31881  NDCG@20=0.39469\n",
      "[2020-01-03 16:14:41] [13/50] 0 mean_batch_loss : 3.893170\n",
      "Start predicting 2020-01-03 16:14:56\n",
      "change best\n",
      "testing finish [2020-01-03 16:15:00] \n",
      "\tHR@1=0.20821  MRR@1=0.20821  NDCG@1=0.20821\n",
      "\tHR@5=0.44995  MRR@5=0.29807  NDCG@5=0.33592\n",
      "\tHR@10=0.55411  MRR@10=0.31200  NDCG@10=0.36963\n",
      "\tHR@20=0.65515  MRR@20=0.31902  NDCG@20=0.39519\n",
      "[2020-01-03 16:15:00] [14/50] 0 mean_batch_loss : 3.857812\n",
      "Start predicting 2020-01-03 16:15:14\n",
      "change best\n",
      "testing finish [2020-01-03 16:15:19] \n",
      "\tHR@1=0.20755  MRR@1=0.20755  NDCG@1=0.20755\n",
      "\tHR@5=0.45199  MRR@5=0.29836  NDCG@5=0.33663\n",
      "\tHR@10=0.55634  MRR@10=0.31229  NDCG@10=0.37039\n",
      "\tHR@20=0.65704  MRR@20=0.31928  NDCG@20=0.39585\n",
      "[2020-01-03 16:15:19] [15/50] 0 mean_batch_loss : 3.888469\n",
      "Start predicting 2020-01-03 16:15:33\n",
      "change best\n",
      "testing finish [2020-01-03 16:15:37] \n",
      "\tHR@1=0.20702  MRR@1=0.20702  NDCG@1=0.20702\n",
      "\tHR@5=0.45284  MRR@5=0.29846  NDCG@5=0.33693\n",
      "\tHR@10=0.55551  MRR@10=0.31222  NDCG@10=0.37019\n",
      "\tHR@20=0.65784  MRR@20=0.31936  NDCG@20=0.39611\n",
      "[2020-01-03 16:15:38] [16/50] 0 mean_batch_loss : 3.767385\n",
      "Start predicting 2020-01-03 16:15:52\n",
      "change best\n",
      "testing finish [2020-01-03 16:15:56] \n",
      "\tHR@1=0.20821  MRR@1=0.20821  NDCG@1=0.20821\n",
      "\tHR@5=0.45291  MRR@5=0.29914  NDCG@5=0.33745\n",
      "\tHR@10=0.55703  MRR@10=0.31306  NDCG@10=0.37115\n",
      "\tHR@20=0.65866  MRR@20=0.32015  NDCG@20=0.39689\n",
      "[2020-01-03 16:15:56] [17/50] 0 mean_batch_loss : 3.780507\n",
      "Start predicting 2020-01-03 16:16:11\n",
      "change best\n",
      "testing finish [2020-01-03 16:16:15] \n",
      "\tHR@1=0.20781  MRR@1=0.20781  NDCG@1=0.20781\n",
      "\tHR@5=0.45278  MRR@5=0.29894  NDCG@5=0.33728\n",
      "\tHR@10=0.55873  MRR@10=0.31310  NDCG@10=0.37157\n",
      "\tHR@20=0.66036  MRR@20=0.32018  NDCG@20=0.39729\n",
      "[2020-01-03 16:16:15] [18/50] 0 mean_batch_loss : 3.635424\n",
      "Start predicting 2020-01-03 16:16:30\n",
      "change best\n",
      "testing finish [2020-01-03 16:16:34] \n",
      "\tHR@1=0.20835  MRR@1=0.20835  NDCG@1=0.20835\n",
      "\tHR@5=0.45388  MRR@5=0.29953  NDCG@5=0.33799\n",
      "\tHR@10=0.55942  MRR@10=0.31365  NDCG@10=0.37216\n",
      "\tHR@20=0.66110  MRR@20=0.32073  NDCG@20=0.39789\n",
      "[2020-01-03 16:16:34] [19/50] 0 mean_batch_loss : 3.688509\n",
      "Start predicting 2020-01-03 16:16:48\n",
      "testing finish [2020-01-03 16:16:53] \n",
      "\tHR@1=0.20745  MRR@1=0.20745  NDCG@1=0.20745\n",
      "\tHR@5=0.45483  MRR@5=0.29927  NDCG@5=0.33803\n",
      "\tHR@10=0.55915  MRR@10=0.31320  NDCG@10=0.37178\n",
      "\tHR@20=0.66067  MRR@20=0.32028  NDCG@20=0.39748\n",
      "[2020-01-03 16:16:53] [20/50] 0 mean_batch_loss : 3.650817\n",
      "Start predicting 2020-01-03 16:17:07\n",
      "testing finish [2020-01-03 16:17:11] \n",
      "\tHR@1=0.20688  MRR@1=0.20688  NDCG@1=0.20688\n",
      "\tHR@5=0.45327  MRR@5=0.29841  NDCG@5=0.33701\n",
      "\tHR@10=0.55915  MRR@10=0.31264  NDCG@10=0.37134\n",
      "\tHR@20=0.66200  MRR@20=0.31980  NDCG@20=0.39738\n",
      "[2020-01-03 16:17:11] [21/50] 0 mean_batch_loss : 3.662157\n",
      "Start predicting 2020-01-03 16:17:26\n",
      "testing finish [2020-01-03 16:17:30] \n",
      "\tHR@1=0.20620  MRR@1=0.20620  NDCG@1=0.20620\n",
      "\tHR@5=0.45422  MRR@5=0.29850  NDCG@5=0.33731\n",
      "\tHR@10=0.55952  MRR@10=0.31265  NDCG@10=0.37146\n",
      "\tHR@20=0.66131  MRR@20=0.31976  NDCG@20=0.39725\n",
      "[2020-01-03 16:17:30] [22/50] 0 mean_batch_loss : 3.728975\n",
      "Start predicting 2020-01-03 16:17:45\n",
      "change best\n",
      "testing finish [2020-01-03 16:17:49] \n",
      "\tHR@1=0.20666  MRR@1=0.20666  NDCG@1=0.20666\n",
      "\tHR@5=0.45499  MRR@5=0.29890  NDCG@5=0.33780\n",
      "\tHR@10=0.55984  MRR@10=0.31297  NDCG@10=0.37179\n",
      "\tHR@20=0.66239  MRR@20=0.32015  NDCG@20=0.39779\n",
      "[2020-01-03 16:17:49] [23/50] 0 mean_batch_loss : 3.751007\n",
      "Start predicting 2020-01-03 16:18:03\n",
      "change best\n",
      "testing finish [2020-01-03 16:18:08] \n",
      "\tHR@1=0.20533  MRR@1=0.20533  NDCG@1=0.20533\n",
      "\tHR@5=0.45545  MRR@5=0.29840  NDCG@5=0.33755\n",
      "\tHR@10=0.56072  MRR@10=0.31249  NDCG@10=0.37163\n",
      "\tHR@20=0.66318  MRR@20=0.31966  NDCG@20=0.39761\n",
      "[2020-01-03 16:18:08] [24/50] 0 mean_batch_loss : 3.773291\n",
      "Start predicting 2020-01-03 16:18:22\n",
      "change best\n",
      "testing finish [2020-01-03 16:18:27] \n",
      "\tHR@1=0.20679  MRR@1=0.20679  NDCG@1=0.20679\n",
      "\tHR@5=0.45598  MRR@5=0.29937  NDCG@5=0.33839\n",
      "\tHR@10=0.56119  MRR@10=0.31346  NDCG@10=0.37246\n",
      "\tHR@20=0.66259  MRR@20=0.32053  NDCG@20=0.39815\n",
      "[2020-01-03 16:18:27] [25/50] 0 mean_batch_loss : 3.466483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting 2020-01-03 16:18:41\n",
      "testing finish [2020-01-03 16:18:45] \n",
      "\tHR@1=0.20628  MRR@1=0.20628  NDCG@1=0.20628\n",
      "\tHR@5=0.45563  MRR@5=0.29919  NDCG@5=0.33819\n",
      "\tHR@10=0.56206  MRR@10=0.31342  NDCG@10=0.37263\n",
      "\tHR@20=0.66225  MRR@20=0.32042  NDCG@20=0.39802\n",
      "[2020-01-03 16:18:45] [26/50] 0 mean_batch_loss : 3.759837\n",
      "Start predicting 2020-01-03 16:19:00\n",
      "testing finish [2020-01-03 16:19:04] \n",
      "\tHR@1=0.20556  MRR@1=0.20556  NDCG@1=0.20556\n",
      "\tHR@5=0.45623  MRR@5=0.29860  NDCG@5=0.33788\n",
      "\tHR@10=0.56150  MRR@10=0.31270  NDCG@10=0.37197\n",
      "\tHR@20=0.66271  MRR@20=0.31980  NDCG@20=0.39765\n",
      "[2020-01-03 16:19:04] [27/50] 0 mean_batch_loss : 3.736045\n",
      "Start predicting 2020-01-03 16:19:19\n",
      "change best\n",
      "testing finish [2020-01-03 16:19:23] \n",
      "\tHR@1=0.20572  MRR@1=0.20572  NDCG@1=0.20572\n",
      "\tHR@5=0.45698  MRR@5=0.29881  NDCG@5=0.33821\n",
      "\tHR@10=0.56239  MRR@10=0.31293  NDCG@10=0.37235\n",
      "\tHR@20=0.66422  MRR@20=0.32002  NDCG@20=0.39813\n",
      "[2020-01-03 16:19:23] [28/50] 0 mean_batch_loss : 3.751197\n",
      "Start predicting 2020-01-03 16:19:37\n",
      "testing finish [2020-01-03 16:19:42] \n",
      "\tHR@1=0.20556  MRR@1=0.20556  NDCG@1=0.20556\n",
      "\tHR@5=0.45545  MRR@5=0.29825  NDCG@5=0.33743\n",
      "\tHR@10=0.56287  MRR@10=0.31266  NDCG@10=0.37224\n",
      "\tHR@20=0.66368  MRR@20=0.31970  NDCG@20=0.39777\n",
      "[2020-01-03 16:19:42] [29/50] 0 mean_batch_loss : 3.619590\n",
      "Start predicting 2020-01-03 16:19:56\n",
      "testing finish [2020-01-03 16:20:00] \n",
      "\tHR@1=0.20431  MRR@1=0.20431  NDCG@1=0.20431\n",
      "\tHR@5=0.45670  MRR@5=0.29798  NDCG@5=0.33753\n",
      "\tHR@10=0.56229  MRR@10=0.31211  NDCG@10=0.37171\n",
      "\tHR@20=0.66374  MRR@20=0.31920  NDCG@20=0.39742\n",
      "[2020-01-03 16:20:00] [30/50] 0 mean_batch_loss : 3.649551\n",
      "Start predicting 2020-01-03 16:20:15\n",
      "testing finish [2020-01-03 16:20:19] \n",
      "\tHR@1=0.20449  MRR@1=0.20449  NDCG@1=0.20449\n",
      "\tHR@5=0.45662  MRR@5=0.29789  NDCG@5=0.33743\n",
      "\tHR@10=0.56392  MRR@10=0.31222  NDCG@10=0.37214\n",
      "\tHR@20=0.66483  MRR@20=0.31924  NDCG@20=0.39767\n",
      "[2020-01-03 16:20:19] [31/50] 0 mean_batch_loss : 3.544811\n",
      "Start predicting 2020-01-03 16:20:34\n",
      "change best\n",
      "testing finish [2020-01-03 16:20:38] \n",
      "\tHR@1=0.20515  MRR@1=0.20515  NDCG@1=0.20515\n",
      "\tHR@5=0.45646  MRR@5=0.29827  NDCG@5=0.33767\n",
      "\tHR@10=0.56260  MRR@10=0.31250  NDCG@10=0.37207\n",
      "\tHR@20=0.66492  MRR@20=0.31961  NDCG@20=0.39795\n",
      "[2020-01-03 16:20:38] [32/50] 0 mean_batch_loss : 3.676731\n",
      "Start predicting 2020-01-03 16:20:53\n",
      "testing finish [2020-01-03 16:20:57] \n",
      "\tHR@1=0.20520  MRR@1=0.20520  NDCG@1=0.20520\n",
      "\tHR@5=0.45652  MRR@5=0.29827  NDCG@5=0.33770\n",
      "\tHR@10=0.56213  MRR@10=0.31241  NDCG@10=0.37190\n",
      "\tHR@20=0.66474  MRR@20=0.31957  NDCG@20=0.39788\n",
      "[2020-01-03 16:20:57] [33/50] 0 mean_batch_loss : 3.756499\n",
      "Start predicting 2020-01-03 16:21:11\n",
      "change best\n",
      "testing finish [2020-01-03 16:21:16] \n",
      "\tHR@1=0.20533  MRR@1=0.20533  NDCG@1=0.20533\n",
      "\tHR@5=0.45578  MRR@5=0.29791  NDCG@5=0.33723\n",
      "\tHR@10=0.56246  MRR@10=0.31218  NDCG@10=0.37176\n",
      "\tHR@20=0.66568  MRR@20=0.31938  NDCG@20=0.39790\n",
      "[2020-01-03 16:21:16] [34/50] 0 mean_batch_loss : 3.601293\n",
      "Start predicting 2020-01-03 16:21:30\n",
      "testing finish [2020-01-03 16:21:34] \n",
      "\tHR@1=0.20449  MRR@1=0.20449  NDCG@1=0.20449\n",
      "\tHR@5=0.45636  MRR@5=0.29790  NDCG@5=0.33738\n",
      "\tHR@10=0.56228  MRR@10=0.31208  NDCG@10=0.37168\n",
      "\tHR@20=0.66446  MRR@20=0.31919  NDCG@20=0.39754\n",
      "[2020-01-03 16:21:34] [35/50] 0 mean_batch_loss : 3.714927\n",
      "Start predicting 2020-01-03 16:21:49\n",
      "testing finish [2020-01-03 16:21:53] \n",
      "\tHR@1=0.20357  MRR@1=0.20357  NDCG@1=0.20357\n",
      "\tHR@5=0.45637  MRR@5=0.29719  NDCG@5=0.33684\n",
      "\tHR@10=0.56269  MRR@10=0.31142  NDCG@10=0.37126\n",
      "\tHR@20=0.66397  MRR@20=0.31851  NDCG@20=0.39695\n",
      "[2020-01-03 16:21:53] [36/50] 0 mean_batch_loss : 3.515736\n",
      "Start predicting 2020-01-03 16:22:08\n",
      "testing finish [2020-01-03 16:22:12] \n",
      "\tHR@1=0.20415  MRR@1=0.20415  NDCG@1=0.20415\n",
      "\tHR@5=0.45637  MRR@5=0.29755  NDCG@5=0.33712\n",
      "\tHR@10=0.56104  MRR@10=0.31159  NDCG@10=0.37104\n",
      "\tHR@20=0.66409  MRR@20=0.31879  NDCG@20=0.39715\n",
      "[2020-01-03 16:22:12] [37/50] 0 mean_batch_loss : 3.603585\n",
      "Start predicting 2020-01-03 16:22:27\n",
      "testing finish [2020-01-03 16:22:31] \n",
      "\tHR@1=0.20467  MRR@1=0.20467  NDCG@1=0.20467\n",
      "\tHR@5=0.45577  MRR@5=0.29789  NDCG@5=0.33723\n",
      "\tHR@10=0.56211  MRR@10=0.31213  NDCG@10=0.37167\n",
      "\tHR@20=0.66491  MRR@20=0.31930  NDCG@20=0.39771\n",
      "[2020-01-03 16:22:31] [38/50] 0 mean_batch_loss : 3.533679\n",
      "Start predicting 2020-01-03 16:22:45\n",
      "testing finish [2020-01-03 16:22:50] \n",
      "\tHR@1=0.20365  MRR@1=0.20365  NDCG@1=0.20365\n",
      "\tHR@5=0.45603  MRR@5=0.29737  NDCG@5=0.33691\n",
      "\tHR@10=0.56344  MRR@10=0.31175  NDCG@10=0.37169\n",
      "\tHR@20=0.66476  MRR@20=0.31879  NDCG@20=0.39732\n",
      "early stopping\n",
      "best model change\n",
      "current model hyper-parameters: session_length=20, hidden_size=100, lr=0.0010,delta=16.0, amsgrad=True, method=general, dropout=0.5, weight_decay=0.000000. \n",
      "\n",
      "current model HR@20=0.66568  MRR@20=0.31938.\n",
      "the best result so far. HR@20=0.66568  MRR@20=0.31938， hyper-parameters: session_length-20, hidden_size-100, lr-0.0010,delta=16.0, amsgrad-True, method-general, dropout-0.5, weight_decay-0.000000. \n",
      "\n",
      "The best result HR@20=0.66568  MRR@20=0.31938, hyper-parameters: session_length-20, hidden_size-100, lr-0.0010,delta=16.0, amsgrad-True, method-general, dropout-0.5, weight_decay-0.000000. \n",
      "over.\n"
     ]
    }
   ],
   "source": [
    "hidden_sizes = [100]\n",
    "dropouts = [0.5]\n",
    "attention_methods = [\"general\"]\n",
    "lrs = [1e-3]\n",
    "session_lengths = [20]\n",
    "weight_decays = [0]\n",
    "patience = 5\n",
    "deltas = [16.0]\n",
    "amsgrads = [True]\n",
    "best_params = \"\"\n",
    "best_all_model = 0.0\n",
    "best_all_hr = 0.0\n",
    "best_all_mrr = 0.0\n",
    "best_all_r1m = 0.0\n",
    "for session_length in session_lengths:\n",
    "    for hidden_size in hidden_sizes:\n",
    "        for amsgrad in amsgrads:\n",
    "            for attention_method in attention_methods:\n",
    "                for dropout in dropouts:\n",
    "                    for weight_decay in weight_decays:\n",
    "                        for lr in lrs:\n",
    "                            for delta in deltas:\n",
    "                                args = {}\n",
    "                                print(\"current model hyper-parameters: session_length=%d, hidden_size=%d, lr=%.4f,delta=%.1f, amsgrad=%s, method=%s, dropout=%.1f, weight_decay=%.6f. \\n\" % (session_length,hidden_size,lr,delta,str(amsgrad),attention_method,dropout,weight_decay))\n",
    "                                args[\"session_length\"] = session_length\n",
    "                                args[\"hidden_size\"] = hidden_size\n",
    "                                args[\"amsgrad\"] = amsgrad\n",
    "                                args[\"method\"] = attention_method\n",
    "                                args[\"dropout\"] = dropout\n",
    "                                args[\"weight_decay\"] = weight_decay\n",
    "                                args[\"lr\"] = lr\n",
    "                                args[\"delta\"] = delta\n",
    "                                args[\"patience\"] = patience\n",
    "                                best_model,best_model_hr,best_model_mrr = train(args)\n",
    "                                if best_model_hr + best_model_mrr > best_all_r1m:\n",
    "                                    print(\"best model change\")\n",
    "                                    best_all_r1m = best_model_hr + best_model_mrr\n",
    "                                    best_all_hr = best_model_hr\n",
    "                                    best_all_mrr = best_model_mrr\n",
    "                                    best_all_model = best_model\n",
    "                                    best_params = \"session_length-%d, hidden_size-%d, lr-%.4f,delta=%.1f, amsgrad-%s, method-%s, dropout-%.1f, weight_decay-%.6f\"%(session_length,hidden_size,lr,delta,str(amsgrad),attention_method,dropout,weight_decay)\n",
    "                                best_model = None\n",
    "                                print(\"current model hyper-parameters: session_length=%d, hidden_size=%d, lr=%.4f,delta=%.1f, amsgrad=%s, method=%s, dropout=%.1f, weight_decay=%.6f. \\n\" % (session_length,hidden_size,lr,delta,str(amsgrad),attention_method,dropout,weight_decay))\n",
    "                                print(\"current model HR@20=%.5f  MRR@20=%.5f.\"%(best_model_hr,best_model_mrr))\n",
    "                                print(\"the best result so far. HR@20=%.5f  MRR@20=%.5f， hyper-parameters: %s. \\n\"%(best_all_hr,best_all_mrr,best_params))\n",
    "print(\"The best result HR@20=%.5f  MRR@20=%.5f, hyper-parameters: %s. \"%(best_all_hr,best_all_mrr,best_params))\n",
    "print(\"over.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T08:22:50.107192Z",
     "start_time": "2020-01-03T08:22:50.102855Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "O8eiYy5UbyFR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.0000],\n",
       "        [0.7284],\n",
       "        [1.4039],\n",
       "        [1.8443],\n",
       "        [2.1944],\n",
       "        [2.4845],\n",
       "        [2.5256],\n",
       "        [2.7007],\n",
       "        [2.5067],\n",
       "        [2.6876],\n",
       "        [2.4980],\n",
       "        [2.5258],\n",
       "        [2.3283],\n",
       "        [2.2659],\n",
       "        [2.1447],\n",
       "        [1.9426],\n",
       "        [1.9965],\n",
       "        [1.8606],\n",
       "        [1.6685],\n",
       "        [1.8002],\n",
       "        [1.8607]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_all_model.position_weights.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-03T08:22:50.112010Z",
     "start_time": "2020-01-03T08:22:50.107995Z"
    }
   },
   "outputs": [],
   "source": [
    "# best_all_model.gate_weights.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "WDP-CE",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "smy",
   "language": "python",
   "name": "smy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
