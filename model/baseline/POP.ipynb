{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T06:27:18.528519Z",
     "start_time": "2019-12-24T06:27:18.369782Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T06:27:18.670956Z",
     "start_time": "2019-12-24T06:27:18.529767Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T06:27:18.675303Z",
     "start_time": "2019-12-24T06:27:18.673555Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T06:27:18.687279Z",
     "start_time": "2019-12-24T06:27:18.676617Z"
    }
   },
   "outputs": [],
   "source": [
    "session_length = 145\n",
    "batch_size = 512\n",
    "plot_num = 500\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T06:27:18.726383Z",
     "start_time": "2019-12-24T06:27:18.695038Z"
    },
    "code_folding": [
     105
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class SessionData(object):\n",
    "    def __init__(self,session_index,session_id,items_indexes):\n",
    "        self.session_index = session_index\n",
    "        self.session_id = session_id\n",
    "        self.item_list = items_indexes\n",
    "\n",
    "    def generate_seq_datas(self,session_length,padding_idx=0,predict_length=1):\n",
    "        sessions = []\n",
    "        if len(self.item_list)<2:\n",
    "            self.item_list.append[self.item_list[0]]\n",
    "        if predict_length==1:\n",
    "            # when session length>=3\n",
    "            for i in range(1,len(self.item_list)-1):\n",
    "#             # when session length >=2\n",
    "#             for i in range(len(self.item_list)-1):\n",
    "                if i <session_length:\n",
    "                    train_data = [0 for _ in range(session_length-i-1)]\n",
    "                    train_data.extend(self.item_list[:i+1])\n",
    "                    train_data.append(self.item_list[i+1])\n",
    "                else:\n",
    "                    train_data = self.item_list[i+1-session_length:i+1]\n",
    "                    train_data.append(self.item_list[i+1])\n",
    "                sessions.append(train_data)\n",
    "        else:\n",
    "\n",
    "            pass\n",
    "        return self.session_index,sessions\n",
    "    def __str__(self):\n",
    "        info = \" session index = {}\\n session id = {} \\n the length of item list= {} \\n the fisrt item index in item list is {}\".format(self.session_index,self.session_id,len(self.item_list),self.item_list[0])\n",
    "        return info\n",
    "class SessionDataSet(object):\n",
    "    def __init__(self,train_file,test_file,padding_idx=0):\n",
    "        super(SessionDataSet,self).__init__()\n",
    "        self.index_count = 0\n",
    "        self.session_count = 0\n",
    "        self.train_count = 0\n",
    "        self.test_count = 0\n",
    "        self.max_session_length = 0\n",
    "\n",
    "        self.padding_idx = padding_idx\n",
    "        self.item2index = dict()\n",
    "        self.index2item = dict()\n",
    "        self.session2index = dict()\n",
    "        self.index2session = dict()\n",
    "        self.item_total_num = dict()\n",
    "        self.item2index[\"<pad>\"] = padding_idx\n",
    "        self.index2item[padding_idx] = \"<pad>\"\n",
    "        self.train_data = self.load_data(train_file)\n",
    "        print(\"training set is loaded, # index: \",len(self.item2index.keys()))\n",
    "        self.train_count = self.session_count\n",
    "        print(\"train_session_num\",self.train_count)\n",
    "        self.test_data = self.load_data(test_file)\n",
    "        print(\"testing set is loaded, # index: \",len(self.index2item.keys()))\n",
    "        print(\"# item\",self.index_count)\n",
    "        self.test_count = self.session_count-self.train_count\n",
    "        print(\"# test session:\",self.test_count)\n",
    "        self.all_training_data = []\n",
    "        self.all_testing_data = []\n",
    "        self.all_meta_training_data = []\n",
    "        self.all_meta_testing_data = []\n",
    "        self.train_session_length = 0\n",
    "        self.test_session_length = 0\n",
    "    \n",
    "    def load_data(self,file_path):\n",
    "        data =  pickle.load(open(file_path, 'rb'))\n",
    "        session_ids = data[0]\n",
    "        session_data = data[1]\n",
    "        session_label = data[2]\n",
    "\n",
    "        result_data = []\n",
    "        lenth = len(session_ids)\n",
    "        print(\"# session\",lenth)\n",
    "\n",
    "        last_session_id = session_ids[0]\n",
    "        \n",
    "        session_item_indexes = []\n",
    "\n",
    "        for item_id in session_data[0]:\n",
    "            if item_id not in self.item2index.keys():\n",
    "                self.index_count+=1\n",
    "                self.item2index[item_id] = self.index_count\n",
    "                self.index2item[self.index_count] = item_id\n",
    "                self.item_total_num[self.index_count] = 0\n",
    "            session_item_indexes.append(self.item2index[item_id])\n",
    "            self.item_total_num[self.item2index[item_id]] += 1\n",
    "        target_item = session_label[0]\n",
    "        if target_item not in self.item2index.keys():\n",
    "            self.index_count+=1\n",
    "            self.item2index[target_item] = self.index_count\n",
    "            self.index2item[self.index_count] = target_item\n",
    "            self.item_total_num[self.index_count] = 0\n",
    "        session_item_indexes.append(self.item2index[target_item])\n",
    "        self.item_total_num[self.item2index[target_item]] += 1\n",
    "\n",
    "        for session_id,items,target_item in zip(session_ids,session_data,session_label):\n",
    "            if session_id!=last_session_id:\n",
    "\n",
    "                self.session_count+=1\n",
    "                self.session2index[last_session_id] = self.session_count\n",
    "                self.index2session[self.session_count] = last_session_id\n",
    "                last_session_id = session_id\n",
    "                if len(session_item_indexes)>self.max_session_length:\n",
    "                    self.max_session_length = len(session_item_indexes)\n",
    "                new_session = SessionData(self.session_count,last_session_id,session_item_indexes)\n",
    "                result_data.append(new_session)\n",
    "                session_item_indexes = []\n",
    "                for item_id in items:\n",
    "                    if item_id not in self.item2index.keys():\n",
    "                        self.index_count+=1\n",
    "                        self.item2index[item_id] = self.index_count\n",
    "                        self.index2item[self.index_count] = item_id\n",
    "                        self.item_total_num[self.index_count] = 0\n",
    "                    session_item_indexes.append(self.item2index[item_id])\n",
    "                    self.item_total_num[self.item2index[item_id]] += 1\n",
    "                if target_item not in self.item2index.keys():\n",
    "                    self.index_count+=1\n",
    "                    self.item2index[target_item] = self.index_count\n",
    "                    self.index2item[self.index_count] = target_item\n",
    "                    self.item_total_num[self.index_count] = 0\n",
    "                session_item_indexes.append(self.item2index[target_item])\n",
    "                self.item_total_num[self.item2index[target_item]] += 1\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        self.session_count+=1\n",
    "        self.session2index[last_session_id] = self.session_count\n",
    "        new_session = SessionData(self.session_count,last_session_id,session_item_indexes)\n",
    "        result_data.append(new_session)\n",
    "        print(\"loaded\")\n",
    "        print(new_session)\n",
    "        \n",
    "        return result_data\n",
    "    \n",
    "\n",
    "    def get_batch(self,batch_size,session_length=10,predict_length=1,all_data=None,phase=\"train\",neg_num=1,sampling_mathod=\"random\"):\n",
    "\n",
    "        if phase == \"train\":\n",
    "            if all_data is None:\n",
    "                all_data = self.get_all_training_data(session_length)\n",
    "            indexes = np.random.permutation(all_data.shape[0])\n",
    "            all_data = all_data[indexes]\n",
    "        else:\n",
    "            if all_data is None:\n",
    "                all_data = self.get_all_testing_data(session_length)\n",
    "        \n",
    "        sindex = 0\n",
    "        eindex = batch_size\n",
    "        while eindex < all_data.shape[0]:\n",
    "            batch = all_data[sindex: eindex]\n",
    "\n",
    "            temp = eindex\n",
    "            eindex = eindex + batch_size\n",
    "            sindex = temp\n",
    "            if phase ==\"train\":\n",
    "                batch = self.divid_and_extend_negative_samples(batch,session_length=session_length,predict_length=predict_length,neg_num=neg_num,method=sampling_mathod)\n",
    "            else:\n",
    "                batch = [batch[:,:session_length],batch[:,session_length:]]\n",
    "            yield batch\n",
    "\n",
    "        if eindex >= all_data.shape[0]:\n",
    "            batch = all_data[sindex:]\n",
    "            if phase ==\"train\":\n",
    "                batch = self.divid_and_extend_negative_samples(batch,session_length=session_length,predict_length=predict_length,neg_num=neg_num,method=sampling_mathod)\n",
    "            else:\n",
    "                batch = [batch[:,:session_length],batch[:,session_length:]]\n",
    "            yield batch\n",
    "    \n",
    "    def divid_and_extend_negative_samples(self,batch_data,session_length,predict_length=1,neg_num=1,method=\"random\"):\n",
    "        \"\"\"\n",
    "        divid and extend negative samples\n",
    "        \"\"\"\n",
    "        neg_items = []\n",
    "        if method == \"random\":\n",
    "            for session_and_target in batch_data:\n",
    "                neg_item = []\n",
    "                for i in range(neg_num):\n",
    "                    rand_item = random.randint(1,self.index_count)\n",
    "                    while rand_item in session_and_target or rand_item in neg_item:\n",
    "                        rand_item = random.randint(1,self.index_count)\n",
    "                    neg_item.append(rand_item)\n",
    "                neg_items.append(neg_item)\n",
    "        else:\n",
    "\n",
    "            total_list = set()\n",
    "            for session in batch_data:\n",
    "                for i in session:\n",
    "                    total_list.add(i) \n",
    "            total_list = list(total_list)\n",
    "            total_list =  sorted(total_list, key=lambda item: self.item_total_num[item],reverse=True)\n",
    "            for i,session in enumerate(batch_data):\n",
    "                np.random.choice(total_list)\n",
    "        session_items = batch_data[:,:session_length]\n",
    "        target_item = batch_data[:,session_length:]\n",
    "        neg_items = np.array(neg_items)\n",
    "        return [session_items,target_item,neg_items]\n",
    "    \n",
    "    def get_all_training_data(self,session_length,predict_length=1):\n",
    "        if len(self.all_training_data)!=0 and self.train_session_length==session_length:\n",
    "#             print(\"The build is complete and there is no need to repeat the build\")\n",
    "            return self.all_training_data\n",
    "        print(\"Start building the all training dataset\")\n",
    "        all_sessions = []\n",
    "        for session_data in self.train_data:\n",
    "            session_index,sessions = session_data.generate_seq_datas(session_length,padding_idx=self.padding_idx)\n",
    "            if sessions is not None:\n",
    "                all_sessions.extend(sessions)\n",
    "        all_sessions = np.array(all_sessions)\n",
    "        self.all_training_data = all_sessions\n",
    "        self.train_session_length=session_length\n",
    "        print(\"The total number of training samples is：\",all_sessions.shape)\n",
    "        return all_sessions\n",
    "    \n",
    "    def get_all_testing_data(self,session_length,predict_length=1):\n",
    "        if len(self.all_testing_data)!=0 and self.test_session_length==session_length:\n",
    "            return self.all_testing_data\n",
    "        all_sessions = []\n",
    "        for session_data in self.test_data:\n",
    "            session_index,sessions = session_data.generate_seq_datas(session_length,padding_idx=self.padding_idx)\n",
    "            if sessions is not None:\n",
    "                all_sessions.extend(sessions)\n",
    "        all_sessions = np.array(all_sessions)\n",
    "        self.all_testing_data = all_sessions\n",
    "        self.test_session_length=session_length\n",
    "        print(\"The total number of testing samples is：\",all_sessions.shape)\n",
    "        return all_sessions\n",
    "    \n",
    "    \n",
    "    def get_train_most_popular_items(self,k):\n",
    "        train_item_total_num = np.array(list(self.train_item_total_num.values()))\n",
    "\n",
    "        top_k_index = np.argsort(train_item_total_num)[::-1][:k]\n",
    "        return np.array(list(self.train_item_total_num.keys()))[top_k_index]\n",
    "        \n",
    "    def get_current_most_popular_items(self,batch_session,k):\n",
    "        item_total_num = np.array(list(self.item_total_num.values()))\n",
    "        for i in range(batch_session.shape[0]):\n",
    "            temp_list = []\n",
    "            for j,item in enumerate(batch_session[i]):\n",
    "                if item in temp_list:\n",
    "                    batch_session[i][j] = 0\n",
    "                else:\n",
    "                    temp_list.append(item)\n",
    "        \n",
    "        session_item_num = item_total_num[batch_session-1]\n",
    "        sorted_session_item = np.argsort(session_item_num)[:,::-1]\n",
    "        session_result = []\n",
    "        if batch_session.shape[1]>=k:\n",
    "            for i in range(batch_session.shape[0]):\n",
    "                session_result.append(batch_session[i][sorted_session_item[i][:k]])\n",
    "        else:\n",
    "            pad_zero = np.zeros((batch_session.shape[0],k-batch_session.shape[1]),dtype=np.int)\n",
    "            for i in range(batch_session.shape[0]):\n",
    "                data = batch_session[i][sorted_session_item[i]]\n",
    "                session_result.append(np.concatenate((np.array(data),pad_zero[i]),-1))\n",
    "        return np.array(session_result)\n",
    "    def __getitem__(self,idx):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T06:27:18.785159Z",
     "start_time": "2019-12-24T06:27:18.727603Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/diginetica_gcsan_my/train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7873dc75ff74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# dataset = SessionDataSet(train_file=\"../../data/retailrocket_gcsan_my/train.txt\",test_file=\"../../data/srgnn/retailrocket_gcsan_my/test.txt\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSessionDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../../data/diginetica_gcsan_my/train.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../../data/srgnn/diginetica_gcsan_my/test.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# dataset = SessionDataSet(train_file=\"../../data/yoochoose1_4_gcsan_my/train.txt\",test_file=\"../../data/srgnn/yoochoose1_4_gcsan_my/test.txt\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# dataset = SessionDataSet(train_file=\"../../data/yoochoose1_64_gcsan_my/train.txt\",test_file=\"../../data/srgnn/yoochoose1_64_gcsan_my/test.txt\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-b5e79daaa528>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, train_file, test_file, padding_idx)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<pad>\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2item\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"<pad>\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training set is loaded, # index: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem2index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-b5e79daaa528>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(self, file_path, is_train)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0msession_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/diginetica_gcsan_my/train.txt'"
     ]
    }
   ],
   "source": [
    "# dataset = SessionDataSet(train_file=\"../../data/retailrocket_gcsan_my/train.txt\",test_file=\"../../data/srgnn/retailrocket_gcsan_my/test.txt\")\n",
    "dataset = SessionDataSet(train_file=\"../../data/diginetica_gcsan_my/train.txt\",test_file=\"../../data/srgnn/diginetica_gcsan_my/test.txt\")\n",
    "# dataset = SessionDataSet(train_file=\"../../data/yoochoose1_4_gcsan_my/train.txt\",test_file=\"../../data/srgnn/yoochoose1_4_gcsan_my/test.txt\")\n",
    "# dataset = SessionDataSet(train_file=\"../../data/yoochoose1_64_gcsan_my/train.txt\",test_file=\"../../data/srgnn/yoochoose1_64_gcsan_my/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T06:27:18.786484Z",
     "start_time": "2019-12-24T06:27:17.337Z"
    }
   },
   "outputs": [],
   "source": [
    "def bpr_loss(r):\n",
    "    return torch.sum(-torch.log(torch.sigmoid(r)))\n",
    "def get_hit_num(pred,y_truth):\n",
    "    \"\"\"\n",
    "        pred: numpy type(batch_size,k) \n",
    "        y_truth: list type (batch_size,groudtruth_num)\n",
    "    \"\"\"\n",
    "\n",
    "    hit_num = 0\n",
    "    for i in range(len(y_truth)):\n",
    "        for value in y_truth[i]:\n",
    "            hit_num += np.sum(pred[i]==value)\n",
    "    return hit_num\n",
    "\n",
    "def get_rr(pred,y_truth):\n",
    "    rr=0.\n",
    "    for i in range(len(y_truth)):\n",
    "        for value in y_truth[i]:\n",
    "            hit_indexes = np.where(pred[i]==value)[0]\n",
    "            for hit_index in hit_indexes:\n",
    "                rr += 1/(hit_index+1)\n",
    "    return rr\n",
    "\n",
    "def get_dcg(pred,y_truth):\n",
    "    y_pred_score = np.zeros_like(pred)\n",
    "\n",
    "    for i in range(len(y_truth)):\n",
    "\n",
    "        for j,y_pred in enumerate(pred[i]):\n",
    "            if y_pred == y_truth[i][0]:\n",
    "                y_pred_score[i][j]=1\n",
    "    gain = 2 ** y_pred_score - 1\n",
    "    discounts = np.tile(np.log2(np.arange(pred.shape[1]) + 2),(len(y_truth),1))\n",
    "    dcg = np.sum(gain / discounts,axis=1)\n",
    "    return dcg\n",
    "\n",
    "def get_ndcg(pred,y_truth):\n",
    "    dcg = get_dcg(pred, y_truth)\n",
    "    idcg = get_dcg(np.concatenate((y_truth,np.zeros_like(pred)[:,:-1]-1),axis=1), y_truth)\n",
    "    ndcg = np.sum(dcg / idcg)\n",
    "\n",
    "    return ndcg\n",
    "\n",
    "def dcg_score(y_pre, y_true, k):\n",
    "    y_pre_score = np.zeros(k)\n",
    "    if len(y_pre) > k:\n",
    "        y_pre = y_pre[:k]\n",
    "    for i in range(len(y_pre)):\n",
    "        pre_tag = y_pre[i]\n",
    "        if pre_tag in y_true:\n",
    "            y_pre_score[i] = 1\n",
    "    gain = 2 ** y_pre_score - 1\n",
    "    discounts = np.log2(np.arange(k) + 2)\n",
    "    return np.sum(gain / discounts)\n",
    "\n",
    "\n",
    "def ndcg_score(y_pre, y_true, k=5):\n",
    "    dcg = dcg_score(y_pre, y_true, k)\n",
    "    idcg = dcg_score(y_true, y_true, k)\n",
    "    return dcg / idcg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POP\n",
    "# CIKM >= 3   \n",
    "        HR@1=0.00052  MRR@1=0.00052  NDCG@1=0.00052\n",
    "        HR@5=0.00248  MRR@5=0.00130  NDCG@5=0.00160\n",
    "        HR@10=0.00549  MRR@10=0.00171  NDCG@10=0.00258\n",
    "        HR@20=0.00847  MRR@20=0.00190  NDCG@20=0.00331\n",
    "\n",
    "# RR >= 3   \n",
    "        HR@1=0.00231  MRR@1=0.00231  NDCG@1=0.00231\n",
    "        HR@5=0.00734  MRR@5=0.00340  NDCG@5=0.00433\n",
    "        HR@10=0.01593  MRR@10=0.00444  NDCG@10=0.00700\n",
    "        HR@20=0.02405  MRR@20=0.00504  NDCG@20=0.00910\n",
    "\n",
    "# RSC64 >= 3   \n",
    "        HR@1=0.01056  MRR@1=0.01056  NDCG@1=0.01056\n",
    "        HR@5=0.02550  MRR@5=0.01691  NDCG@5=0.01909\n",
    "        HR@10=0.04629  MRR@10=0.01958  NDCG@10=0.02570\n",
    "        HR@20=0.05776  MRR@20=0.02038  NDCG@20=0.02861\n",
    "\n",
    "# RSC4 >= 3   \n",
    "        HR@1=0.00109  MRR@1=0.00109  NDCG@1=0.00109\n",
    "        HR@5=0.00303  MRR@5=0.00167  NDCG@5=0.00200\n",
    "        HR@10=0.00718  MRR@10=0.00222  NDCG@10=0.00334\n",
    "        HR@20=0.01342  MRR@20=0.00265  NDCG@20=0.00491\n",
    "\n",
    "# SPOP\n",
    "\n",
    "# CIKM >= 3   \n",
    "        HR@1=0.08426  MRR@1=0.08426  NDCG@1=0.08426\n",
    "        HR@5=0.20156  MRR@5=0.13052  NDCG@5=0.14839\n",
    "        HR@10=0.21536  MRR@10=0.13252  NDCG@10=0.15301\n",
    "        HR@20=0.21683  MRR@20=0.13264  NDCG@20=0.15340\n",
    "\n",
    "# RR >= 3   \n",
    "        HR@1=0.17361  MRR@1=0.17361  NDCG@1=0.17361\n",
    "        HR@5=0.34166  MRR@5=0.24458  NDCG@5=0.26925\n",
    "        HR@10=0.35778  MRR@10=0.24682  NDCG@10=0.27455\n",
    "        HR@20=0.38032  MRR@20=0.24814  NDCG@20=0.27993\n",
    "\n",
    "# RSC64 >= 3   \n",
    "        HR@1=0.10193  MRR@1=0.10193  NDCG@1=0.10193\n",
    "        HR@5=0.24568  MRR@5=0.15783  NDCG@5=0.17988\n",
    "        HR@10=0.27538  MRR@10=0.16201  NDCG@10=0.18969\n",
    "        HR@20=0.28135  MRR@20=0.16248  NDCG@20=0.19127\n",
    "\n",
    "# RSC4 >= 3   \n",
    "        HR@1=0.10037  MRR@1=0.10037  NDCG@1=0.10037\n",
    "        HR@5=0.24524  MRR@5=0.15675  NDCG@5=0.17896\n",
    "        HR@10=0.27545  MRR@10=0.16095  NDCG@10=0.18889\n",
    "        HR@20=0.28135  MRR@20=0.16142  NDCG@20=0.19045"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T06:27:18.787213Z",
     "start_time": "2019-12-24T06:27:17.339Z"
    }
   },
   "outputs": [],
   "source": [
    "def pop():\n",
    "    predict_nums = [1,5,10,20]\n",
    "    session_length = 20\n",
    "    pop_k = dataset.get_train_most_popular_items(predict_nums[-1])\n",
    "    start_test_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(\"start testing\",start_test_time)\n",
    "    rrs = [0 for _ in range(len(predict_nums))]\n",
    "    hit_nums = [0 for _ in range(len(predict_nums))]\n",
    "    ndcgs = [0 for _ in range(len(predict_nums))]\n",
    "    test_num = 0\n",
    "    for i,batch_data in enumerate(dataset.get_batch(batch_size,session_length,phase=\"test\")):\n",
    "#                 print(len(batch_data))\n",
    "#         sessions = torch.tensor(batch_data[0]).to(device)\n",
    "        target_items = np.array(batch_data[1])\n",
    "        test_num += len(target_items)\n",
    "        y_pred = np.tile(pop_k,(batch_size,1))\n",
    "#         print(y_pred.shape,target_items.shape)\n",
    "        for j,predict_num in enumerate(predict_nums):\n",
    "            hit_nums[j]+=get_hit_num(y_pred[:,:predict_num],target_items)\n",
    "            rrs[j]+=get_rr(y_pred[:,:predict_num],target_items)\n",
    "            ndcgs[j]+=get_ndcg(y_pred[:,:predict_num],target_items)\n",
    "#                     print(hit_nums[j],ndcgs[j])\n",
    "    end_test_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    hrs = [hit_num/test_num for hit_num in hit_nums]\n",
    "    mrrs = [rr/test_num for rr in rrs]\n",
    "    mndcgs = [ndcg/test_num for ndcg in ndcgs]\n",
    "    print(\"testing over [%s] \"%end_test_time)\n",
    "    for k,predict_num in enumerate(predict_nums):\n",
    "        print(\"\\tHR@%d=%.5f  MRR@%d=%.5f  NDCG@%d=%.5f\"%(predict_num,hrs[k],predict_num,mrrs[k],predict_num,mndcgs[k]))\n",
    "        \n",
    "def spop():\n",
    "    predict_nums = [1,5,10,20]\n",
    "    session_length = 20\n",
    "    pop_k = dataset.get_train_most_popular_items(predict_nums[-1])\n",
    "    start_test_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(\"start testing\",start_test_time)\n",
    "    rrs = [0 for _ in range(len(predict_nums))]\n",
    "    hit_nums = [0 for _ in range(len(predict_nums))]\n",
    "    ndcgs = [0 for _ in range(len(predict_nums))]\n",
    "    test_num = 0\n",
    "    for i,batch_data in enumerate(dataset.get_batch(batch_size,session_length,phase=\"test\")):\n",
    "#                 print(len(batch_data))\n",
    "#         sessions = torch.tensor(batch_data[0]).to(device)\n",
    "        target_items = np.array(batch_data[1])\n",
    "        test_num += len(target_items)\n",
    "        y_pred = dataset.get_current_most_popular_items(batch_data[0],predict_nums[-1])\n",
    "#         print(y_pred.shape,target_items.shape)\n",
    "        for j,predict_num in enumerate(predict_nums):\n",
    "            hit_nums[j]+=get_hit_num(y_pred[:,:predict_num],target_items)\n",
    "            rrs[j]+=get_rr(y_pred[:,:predict_num],target_items)\n",
    "            ndcgs[j]+=get_ndcg(y_pred[:,:predict_num],target_items)\n",
    "#                     print(hit_nums[j],ndcgs[j])\n",
    "    end_test_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    hrs = [hit_num/test_num for hit_num in hit_nums]\n",
    "    mrrs = [rr/test_num for rr in rrs]\n",
    "    mndcgs = [ndcg/test_num for ndcg in ndcgs]\n",
    "    print(\"testing over [%s] \"%end_test_time)\n",
    "    for k,predict_num in enumerate(predict_nums):\n",
    "        print(\"\\tHR@%d=%.5f  MRR@%d=%.5f  NDCG@%d=%.5f\"%(predict_num,hrs[k],predict_num,mrrs[k],predict_num,mndcgs[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T06:27:18.787845Z",
     "start_time": "2019-12-24T06:27:17.342Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = spop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smy",
   "language": "python",
   "name": "smy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
