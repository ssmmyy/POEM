{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-25T12:33:34.048317Z",
     "start_time": "2019-12-25T12:33:33.608437Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "import copy\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-25T12:33:34.051945Z",
     "start_time": "2019-12-25T12:33:34.049473Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda:0\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-25T12:33:34.067325Z",
     "start_time": "2019-12-25T12:33:34.054197Z"
    }
   },
   "outputs": [],
   "source": [
    "session_length = 19\n",
    "batch_size = 512\n",
    "plot_num = 500\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-25T12:33:34.115916Z",
     "start_time": "2019-12-25T12:33:34.068652Z"
    }
   },
   "outputs": [],
   "source": [
    "# this part is different form POEM\n",
    "class SessionData(object):\n",
    "    def __init__(self,session_index,session_id,items_indexes):\n",
    "        self.session_index = session_index\n",
    "        self.session_id = session_id\n",
    "        self.item_list = items_indexes\n",
    "    def generate_seq_datas(self,session_length,padding_idx=0,predict_length=1):\n",
    "        sessions = []\n",
    "        if len(self.item_list)<2:\n",
    "            self.item_list.append[self.item_list[0]]\n",
    "        if predict_length==1:\n",
    "#             # when session length>=3\n",
    "#             for i in range(1,len(self.item_list)-1):\n",
    "            # when session length >=2\n",
    "            for i in range(len(self.item_list)-1):\n",
    "                if i <session_length:\n",
    "                    train_data = [0 for _ in range(session_length-i-1)]\n",
    "                    train_data.extend(self.item_list[:i+1])\n",
    "                    train_data.append(self.item_list[i+1])\n",
    "                else:\n",
    "                    train_data = self.item_list[i+1-session_length:i+1]\n",
    "                    train_data.append(self.item_list[i+1])\n",
    "                sessions.append(train_data)\n",
    "        else:\n",
    "            # To be continue if necessary\n",
    "            pass\n",
    "        return self.session_index,sessions\n",
    "    def __str__(self):\n",
    "        info = \" session index = {}\\n session id = {} \\n the length of item list= {} \\n the fisrt item index in item list is {}\".format(self.session_index,self.session_id,len(self.item_list),self.item_list[0])\n",
    "        return info\n",
    "class SessionDataSet(object):\n",
    "    def __init__(self,train_file,test_file,padding_idx=0):\n",
    "        super(SessionDataSet,self).__init__()\n",
    "        self.index_count = 0\n",
    "        self.session_count = 0\n",
    "        self.train_count = 0\n",
    "        self.test_count = 0\n",
    "        self.max_session_length = 0\n",
    "\n",
    "        self.padding_idx = padding_idx\n",
    "        self.item2index = dict()\n",
    "        self.index2item = dict()\n",
    "        self.session2index = dict()\n",
    "        self.index2session = dict()\n",
    "        self.item_total_num = dict()\n",
    "        self.item2index[\"<pad>\"] = padding_idx\n",
    "        self.index2item[padding_idx] = \"<pad>\"\n",
    "        self.train_data = self.load_data(train_file)\n",
    "        print(\"training set is loaded, # index: \",len(self.item2index.keys()))\n",
    "        self.train_count = self.session_count\n",
    "        print(\"train_session_num\",self.train_count)\n",
    "        self.test_data = self.load_data(test_file)\n",
    "        print(\"testing set is loaded, # index: \",len(self.index2item.keys()))\n",
    "        print(\"# item\",self.index_count)\n",
    "        self.test_count = self.session_count-self.train_count\n",
    "        print(\"# test session:\",self.test_count)\n",
    "        self.all_training_data = []\n",
    "        self.all_testing_data = []\n",
    "        self.all_meta_training_data = []\n",
    "        self.all_meta_testing_data = []\n",
    "        self.train_session_length = 0\n",
    "        self.test_session_length = 0\n",
    "    \n",
    "    def load_data(self,file_path):\n",
    "        data =  pickle.load(open(file_path, 'rb'))\n",
    "        session_ids = data[0]\n",
    "        session_data = data[1]\n",
    "        session_label = data[2]\n",
    "\n",
    "        result_data = []\n",
    "        lenth = len(session_ids)\n",
    "        print(\"# session\",lenth)\n",
    "\n",
    "        last_session_id = session_ids[0]\n",
    "        \n",
    "        session_item_indexes = []\n",
    "\n",
    "        for item_id in session_data[0]:\n",
    "            if item_id not in self.item2index.keys():\n",
    "                self.index_count+=1\n",
    "                self.item2index[item_id] = self.index_count\n",
    "                self.index2item[self.index_count] = item_id\n",
    "                self.item_total_num[self.index_count] = 0\n",
    "            session_item_indexes.append(self.item2index[item_id])\n",
    "            self.item_total_num[self.item2index[item_id]] += 1\n",
    "        target_item = session_label[0]\n",
    "        if target_item not in self.item2index.keys():\n",
    "            self.index_count+=1\n",
    "            self.item2index[target_item] = self.index_count\n",
    "            self.index2item[self.index_count] = target_item\n",
    "            self.item_total_num[self.index_count] = 0\n",
    "        session_item_indexes.append(self.item2index[target_item])\n",
    "        self.item_total_num[self.item2index[target_item]] += 1\n",
    "\n",
    "        for session_id,items,target_item in zip(session_ids,session_data,session_label):\n",
    "            if session_id!=last_session_id:\n",
    "\n",
    "                self.session_count+=1\n",
    "                self.session2index[last_session_id] = self.session_count\n",
    "                self.index2session[self.session_count] = last_session_id\n",
    "                if len(session_item_indexes)>self.max_session_length:\n",
    "                    self.max_session_length = len(session_item_indexes)\n",
    "                new_session = SessionData(self.session_count,last_session_id,session_item_indexes)\n",
    "                result_data.append(new_session)\n",
    "                last_session_id = session_id\n",
    "                session_item_indexes = []\n",
    "                for item_id in items:\n",
    "                    if item_id not in self.item2index.keys():\n",
    "                        self.index_count+=1\n",
    "                        self.item2index[item_id] = self.index_count\n",
    "                        self.index2item[self.index_count] = item_id\n",
    "                        self.item_total_num[self.index_count] = 0\n",
    "                    session_item_indexes.append(self.item2index[item_id])\n",
    "                    self.item_total_num[self.item2index[item_id]] += 1\n",
    "                if target_item not in self.item2index.keys():\n",
    "                    self.index_count+=1\n",
    "                    self.item2index[target_item] = self.index_count\n",
    "                    self.index2item[self.index_count] = target_item\n",
    "                    self.item_total_num[self.index_count] = 0\n",
    "                session_item_indexes.append(self.item2index[target_item])\n",
    "                self.item_total_num[self.item2index[target_item]] += 1\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        self.session_count+=1\n",
    "        self.session2index[last_session_id] = self.session_count\n",
    "        new_session = SessionData(self.session_count,last_session_id,session_item_indexes)\n",
    "        result_data.append(new_session)\n",
    "        print(\"loaded\")\n",
    "        print(new_session)\n",
    "        \n",
    "        return result_data\n",
    "    \n",
    "\n",
    "    def get_batch(self,batch_size,session_length=10,predict_length=1,all_data=None,phase=\"train\",neg_num=1,sampling_mathod=\"random\"):\n",
    "\n",
    "        if phase == \"train\":\n",
    "            if all_data is None:\n",
    "                all_data = self.get_all_training_data(session_length)\n",
    "            indexes = np.random.permutation(all_data.shape[0])\n",
    "            all_data = all_data[indexes]\n",
    "        else:\n",
    "            if all_data is None:\n",
    "                all_data = self.get_all_testing_data(session_length)\n",
    "        \n",
    "        sindex = 0\n",
    "        eindex = batch_size\n",
    "        while eindex < all_data.shape[0]:\n",
    "            batch = all_data[sindex: eindex]\n",
    "\n",
    "            temp = eindex\n",
    "            eindex = eindex + batch_size\n",
    "            sindex = temp\n",
    "            if phase ==\"train\":\n",
    "                batch = self.divid_and_extend_negative_samples(batch,session_length=session_length,predict_length=predict_length,neg_num=neg_num,method=sampling_mathod)\n",
    "            else:\n",
    "                batch = [batch[:,:session_length],batch[:,session_length:]]\n",
    "            yield batch\n",
    "\n",
    "        if eindex >= all_data.shape[0]:\n",
    "            batch = all_data[sindex:]\n",
    "            if phase ==\"train\":\n",
    "                batch = self.divid_and_extend_negative_samples(batch,session_length=session_length,predict_length=predict_length,neg_num=neg_num,method=sampling_mathod)\n",
    "            else:\n",
    "                batch = [batch[:,:session_length],batch[:,session_length:]]\n",
    "            yield batch\n",
    "    \n",
    "    def divid_and_extend_negative_samples(self,batch_data,session_length,predict_length=1,neg_num=1,method=\"random\"):\n",
    "        \"\"\"\n",
    "        divid and extend negative samples\n",
    "        \"\"\"\n",
    "        neg_items = []\n",
    "        if method == \"random\":\n",
    "            for session_and_target in batch_data:\n",
    "                neg_item = []\n",
    "                for i in range(neg_num):\n",
    "                    rand_item = random.randint(1,self.index_count)\n",
    "                    while rand_item in session_and_target or rand_item in neg_item:\n",
    "                        rand_item = random.randint(1,self.index_count)\n",
    "                    neg_item.append(rand_item)\n",
    "                neg_items.append(neg_item)\n",
    "        else:\n",
    "\n",
    "            total_list = set()\n",
    "            for session in batch_data:\n",
    "                for i in session:\n",
    "                    total_list.add(i) \n",
    "            total_list = list(total_list)\n",
    "            total_list =  sorted(total_list, key=lambda item: self.item_total_num[item],reverse=True)\n",
    "            for i,session in enumerate(batch_data):\n",
    "                np.random.choice(total_list)\n",
    "        session_items = batch_data[:,:session_length]\n",
    "        target_item = batch_data[:,session_length:]\n",
    "        neg_items = np.array(neg_items)\n",
    "        return [session_items,target_item,neg_items]\n",
    "    \n",
    "    def get_all_training_data(self,session_length,predict_length=1):\n",
    "        if len(self.all_training_data)!=0 and self.train_session_length==session_length:\n",
    "#             print(\"The build is complete and there is no need to repeat the build\")\n",
    "            return self.all_training_data\n",
    "        print(\"Start building the all training dataset\")\n",
    "        all_sessions = []\n",
    "        for session_data in self.train_data:\n",
    "            # 前session_length为session，后predict_length为target_item\n",
    "            session_index,sessions = session_data.generate_seq_datas(session_length,padding_idx=self.padding_idx)\n",
    "            if sessions is not None:\n",
    "                all_sessions.extend(sessions)\n",
    "        all_sessions = np.array(all_sessions)\n",
    "        self.all_training_data = all_sessions\n",
    "        self.train_session_length=session_length\n",
    "        print(\"The total number of training samples is：\",all_sessions.shape)\n",
    "        return all_sessions\n",
    "    \n",
    "    def get_all_testing_data(self,session_length,predict_length=1):\n",
    "        if len(self.all_testing_data)!=0 and self.test_session_length==session_length:\n",
    "            return self.all_testing_data\n",
    "        all_sessions = []\n",
    "        for session_data in self.test_data:\n",
    "            session_index,sessions = session_data.generate_seq_datas(session_length,padding_idx=self.padding_idx)\n",
    "            if sessions is not None:\n",
    "                all_sessions.extend(sessions)\n",
    "        all_sessions = np.array(all_sessions)\n",
    "        self.all_testing_data = all_sessions\n",
    "        self.test_session_length=session_length\n",
    "        print(\"The total number of testing samples is：\",all_sessions.shape)\n",
    "        return all_sessions\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-25T12:33:36.014697Z",
     "start_time": "2019-12-25T12:33:34.117112Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# session 749947\n",
      "loaded\n",
      " session index = 294620\n",
      " session id = 1131204 \n",
      " the length of item list= 2 \n",
      " the fisrt item index in item list is 23118\n",
      "training set is loaded, # index:  48990\n",
      "train_session_num 294620\n",
      "# session 28445\n",
      "loaded\n",
      " session index = 306825\n",
      " session id = 1582915 \n",
      " the length of item list= 6 \n",
      " the fisrt item index in item list is 4767\n",
      "testing set is loaded, # index:  48990\n",
      "# item 48989\n",
      "# test session: 12205\n"
     ]
    }
   ],
   "source": [
    "# dataset = SessionDataSet(train_file=\"../../data/retailrocket/train.txt\",test_file=\"../../data/srgnn/retailrocket/test.txt\")\n",
    "# dataset = SessionDataSet(train_file=\"../../data/diginetica/train.txt\",test_file=\"../../data/srgnn/diginetica/test.txt\")\n",
    "dataset = SessionDataSet(train_file=\"../../data/yoochoose1_4/train.txt\",test_file=\"../../data/srgnn/yoochoose1_4/test.txt\")\n",
    "# dataset = SessionDataSet(train_file=\"../../data/yoochoose1_64/train.txt\",test_file=\"../../data/srgnn/yoochoose1_64/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-25T12:33:36.023847Z",
     "start_time": "2019-12-25T12:33:36.015667Z"
    }
   },
   "outputs": [],
   "source": [
    "def bpr_loss(r):\n",
    "    return torch.sum(-torch.log(torch.sigmoid(r)))\n",
    "def get_hit_num(pred,y_truth):\n",
    "    \"\"\"\n",
    "        pred: numpy type(batch_size,k) \n",
    "        y_truth: list type (batch_size,groudtruth_num)\n",
    "    \"\"\"\n",
    "\n",
    "    hit_num = 0\n",
    "    for i in range(len(y_truth)):\n",
    "        for value in y_truth[i]:\n",
    "            hit_num += np.sum(pred[i]==value)\n",
    "    return hit_num\n",
    "\n",
    "def get_rr(pred,y_truth):\n",
    "    rr=0.\n",
    "    for i in range(len(y_truth)):\n",
    "        for value in y_truth[i]:\n",
    "            hit_indexes = np.where(pred[i]==value)[0]\n",
    "            for hit_index in hit_indexes:\n",
    "                rr += 1/(hit_index+1)\n",
    "    return rr\n",
    "\n",
    "def get_dcg(pred,y_truth):\n",
    "    y_pred_score = np.zeros_like(pred)\n",
    "\n",
    "    for i in range(len(y_truth)):\n",
    "\n",
    "        for j,y_pred in enumerate(pred[i]):\n",
    "            if y_pred == y_truth[i][0]:\n",
    "                y_pred_score[i][j]=1\n",
    "    gain = 2 ** y_pred_score - 1\n",
    "    discounts = np.tile(np.log2(np.arange(pred.shape[1]) + 2),(len(y_truth),1))\n",
    "    dcg = np.sum(gain / discounts,axis=1)\n",
    "    return dcg\n",
    "\n",
    "def get_ndcg(pred,y_truth):\n",
    "    dcg = get_dcg(pred, y_truth)\n",
    "    idcg = get_dcg(np.concatenate((y_truth,np.zeros_like(pred)[:,:-1]-1),axis=1), y_truth)\n",
    "    ndcg = np.sum(dcg / idcg)\n",
    "\n",
    "    return ndcg\n",
    "\n",
    "def dcg_score(y_pre, y_true, k):\n",
    "    y_pre_score = np.zeros(k)\n",
    "    if len(y_pre) > k:\n",
    "        y_pre = y_pre[:k]\n",
    "    for i in range(len(y_pre)):\n",
    "        pre_tag = y_pre[i]\n",
    "        if pre_tag in y_true:\n",
    "            y_pre_score[i] = 1\n",
    "    gain = 2 ** y_pre_score - 1\n",
    "    discounts = np.log2(np.arange(k) + 2)\n",
    "    return np.sum(gain / discounts)\n",
    "\n",
    "\n",
    "def ndcg_score(y_pre, y_true, k=5):\n",
    "    dcg = dcg_score(y_pre, y_true, k)\n",
    "    idcg = dcg_score(y_true, y_true, k)\n",
    "    return dcg / idcg\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-25T12:33:36.047343Z",
     "start_time": "2019-12-25T12:33:36.024882Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class NARM(torch.nn.Module):\n",
    "    def __init__(self, itemNum, hidden_size, embedding_dim, batch_size, layerNum = 1,padding_idx=0,posNum=11, dropout=0.5,embedding_dropout=0.25,activate=\"tanh\"):\n",
    "        super(NARM, self).__init__()\n",
    "        self.itemNum = itemNum\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.layerNum = layerNum\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.item_embedding = torch.nn.Embedding(itemNum, self.embedding_dim, padding_idx=padding_idx)\n",
    "        torch.nn.init.constant_(self.item_embedding.weight[0],0)\n",
    "        self.embedding_dropout = torch.nn.Dropout(embedding_dropout)\n",
    "        self.gru = torch.nn.GRU(self.embedding_dim, self.hidden_size, self.layerNum)\n",
    "        self.a_1 = torch.nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.a_2 = torch.nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.v_t = torch.nn.Linear(self.hidden_size, 1, bias=False)\n",
    "        self.ct_dropout = torch.nn.Dropout(dropout)\n",
    "        self.b = torch.nn.Linear(self.embedding_dim, 2 * self.hidden_size, bias=False)\n",
    "        #self.sf = torch.nn.Softmax()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def forward(self, seq, lengths):\n",
    "        seq = seq.t()\n",
    "        hidden = self.init_hidden(seq.size(1))\n",
    "        embs = self.embedding_dropout(self.item_embedding(seq))\n",
    "#         print(\"before\",embs.shape)\n",
    "        embs = pack_padded_sequence(embs, lengths)\n",
    "#         print(\"after\",embs.shape)\n",
    "        gru_out, hidden = self.gru(embs, hidden)\n",
    "\n",
    "        gru_out, lengths = pad_packed_sequence(gru_out)\n",
    "#         print(\"after aa\",gru_out,lengths)\n",
    "        # fetch the last hidden state of last timestamp\n",
    "        ht = hidden[-1]\n",
    "        gru_out = gru_out.permute(1, 0, 2)\n",
    "\n",
    "        c_global = ht\n",
    "        q1 = self.a_1(gru_out.contiguous().view(-1, self.hidden_size)).view(gru_out.size())  \n",
    "        q2 = self.a_2(ht)\n",
    "\n",
    "        mask = torch.where(seq.permute(1, 0) > 0, torch.tensor([1.], device = self.device), torch.tensor([0.], device = self.device))\n",
    "        q2_expand = q2.unsqueeze(1).expand_as(q1)\n",
    "        q2_masked = mask.unsqueeze(2).expand_as(q1) * q2_expand\n",
    "\n",
    "        alpha = self.v_t(torch.sigmoid(q1 + q2_masked).view(-1, self.hidden_size)).view(mask.size())\n",
    "        c_local = torch.sum(alpha.unsqueeze(2).expand_as(gru_out) * gru_out, 1)\n",
    "\n",
    "        c_t = torch.cat([c_local, c_global], 1)\n",
    "        c_t = self.ct_dropout(c_t)\n",
    "        \n",
    "        item_embs = self.item_embedding.weight[1:]\n",
    "        scores = torch.matmul(c_t, self.b(item_embs).permute(1, 0))\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros((self.layerNum, batch_size, self.hidden_size), requires_grad=True).to(self.device)\n",
    "    \n",
    "    def predict_top_k(self,seq, lengths, k=20):\n",
    "        seq = seq.t()\n",
    "        hidden = self.init_hidden(seq.size(1))\n",
    "        embs = self.item_embedding(seq)\n",
    "#         print(\"embs.shape\",embs.shape)\n",
    "        embs = pack_padded_sequence(embs, lengths)\n",
    "        gru_out, hidden = self.gru(embs, hidden)\n",
    "        gru_out, lengths = pad_packed_sequence(gru_out)\n",
    "\n",
    "        # fetch the last hidden state of last timestamp\n",
    "        ht = hidden[-1]\n",
    "        gru_out = gru_out.permute(1, 0, 2)\n",
    "\n",
    "        c_global = ht\n",
    "        q1 = self.a_1(gru_out.contiguous().view(-1, self.hidden_size)).view(gru_out.size())  \n",
    "        q2 = self.a_2(ht)\n",
    "\n",
    "        mask = torch.where(seq.permute(1, 0) > 0, torch.tensor([1.], device = self.device), torch.tensor([0.], device = self.device))\n",
    "        q2_expand = q2.unsqueeze(1).expand_as(q1)\n",
    "        q2_masked = mask.unsqueeze(2).expand_as(q1) * q2_expand\n",
    "\n",
    "        alpha = self.v_t(torch.sigmoid(q1 + q2_masked).view(-1, self.hidden_size)).view(mask.size())\n",
    "        c_local = torch.sum(alpha.unsqueeze(2).expand_as(gru_out) * gru_out, 1)\n",
    "\n",
    "        c_t = torch.cat([c_local, c_global], 1)\n",
    "#         c_t = self.ct_dropout(c_t)\n",
    "        \n",
    "        item_embs = self.item_embedding.weight[1:]\n",
    "        scores = torch.matmul(c_t, self.b(item_embs).permute(1, 0))\n",
    "        result = torch.topk(scores,k,dim=-1)[1]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIKM S >= 2  \n",
    "    HR@20=0.63722  MRR@20=0.29634, session_length=19, hidden_size=100, lr=0.0010, embedding_dim=100, embedding_dropout=0.25, dropout=0.50\n",
    "        HR@1=0.18599  MRR@1=0.18599  NDCG@1=0.18599\n",
    "        HR@5=0.42754  MRR@5=0.27494  NDCG@5=0.31293\n",
    "        HR@10=0.53296  MRR@10=0.28907  NDCG@10=0.34708\n",
    "        HR@20=0.63722  MRR@20=0.29634  NDCG@20=0.37349\n",
    "# RR S >= 2   \n",
    "    HR@20=0.61160  MRR@20=0.34716, session_length=19, hidden_size=100, lr=0.0020, embedding_dim=100, embedding_dropout=0.25, dropout=0.50\n",
    "        HR@1=0.25558  MRR@1=0.25558  NDCG@1=0.25558\n",
    "        HR@5=0.46015  MRR@5=0.33150  NDCG@5=0.36357\n",
    "        HR@10=0.53946  MRR@10=0.34214  NDCG@10=0.38927\n",
    "        HR@20=0.61160  MRR@20=0.34716  NDCG@20=0.40753\n",
    "# RSC64 S >= 2   \n",
    "    HR@20=0.70543  MRR@20=0.30329, session_length=19, hidden_size=100, lr=0.0010, embedding_dim=100, embedding_dropout=0.25, dropout=0.50\n",
    "        HR@1=0.17045  MRR@1=0.17045  NDCG@1=0.17045\n",
    "        HR@5=0.46873  MRR@5=0.27826  NDCG@5=0.32558\n",
    "        HR@10=0.59816  MRR@10=0.29570  NDCG@10=0.36761\n",
    "        HR@20=0.70543  MRR@20=0.30329  NDCG@20=0.39491\n",
    "# RSC4 S >= 2   \n",
    "    HR@20=0.71781  MRR@20=0.30821, hyper-parameters: current model hyper-parameters: session_length=19, hidden_size=100, lr=0.0010, embedding_dim=100, embedding_dropout=0.25, dropout=0.50\n",
    "        HR@1=0.17330  MRR@1=0.17330  NDCG@1=0.17330\n",
    "        HR@5=0.47427  MRR@5=0.28242  NDCG@5=0.33010\n",
    "        HR@10=0.60947  MRR@10=0.30059  NDCG@10=0.37394\n",
    "        HR@20=0.71781  MRR@20=0.30821  NDCG@20=0.40146"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-25T12:33:36.060077Z",
     "start_time": "2019-12-25T12:33:36.048379Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs=50\n",
    "def train(args):\n",
    "    hidden_size = args[\"hidden_size\"] if \"hidden_size\" in args.keys() else 100\n",
    "    embedding_dim = args[\"embedding_dim\"] if \"hidden_size\" in args.keys() else 100\n",
    "    dropout = args[\"dropout\"] if \"dropout\" in args.keys()  else 0.5\n",
    "    embedding_dropout = args[\"embedding_dropout\"] if \"dropout\" in args.keys()  else 0.5\n",
    "    lr = args[\"lr\"] if \"lr\" in args.keys()  else 1e-3\n",
    "    session_length = args[\"session_length\"] if \"session_length\" in args.keys() else 20\n",
    "    model = NARM(hidden_size=hidden_size, embedding_dim=embedding_dim,itemNum=dataset.index_count+1, batch_size=batch_size,posNum=session_length+1, padding_idx=0, dropout=dropout,embedding_dropout=embedding_dropout).to(device)\n",
    "    opti = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "    best_model_hr = 0.0\n",
    "    best_model_mrr = 0.0\n",
    "    best_r1m = 0.0\n",
    "    best_model = None\n",
    "    first_loss = 0.0\n",
    "    predict_nums = [1,5,10,20]\n",
    "    for epoch in range(epochs):\n",
    "        batch_losses = []\n",
    "        epoch_losses = []\n",
    "        for i,batch_data in enumerate(dataset.get_batch(batch_size,session_length,phase=\"train\")):\n",
    "            sessions = torch.tensor(batch_data[0]).to(device)\n",
    "            target_items = torch.tensor(batch_data[1]).squeeze().to(device)-1\n",
    "            result_pos = model(sessions,torch.tensor(session_length).unsqueeze(0).repeat(target_items.shape[0]))\n",
    "            loss = loss_function(result_pos,target_items)\n",
    "            opti.zero_grad()\n",
    "            loss.backward()\n",
    "            opti.step()\n",
    "            batch_losses.append(loss.cpu().detach().numpy())\n",
    "            epoch_losses.append(loss.cpu().detach().numpy())\n",
    "            if i % plot_num == 0:\n",
    "                time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                print(\"[%s] [%d/%d] %d mean_batch_loss : %0.6f\" % (time, epoch+1, epochs, i, np.mean(batch_losses)))\n",
    "                batch_losses = []\n",
    "        with torch.no_grad():\n",
    "            start_test_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(\"Start predicting\",start_test_time)\n",
    "            rrs = [0 for _ in range(len(predict_nums))]\n",
    "            hit_nums = [0 for _ in range(len(predict_nums))]\n",
    "            ndcgs = [0 for _ in range(len(predict_nums))]\n",
    "            for i,batch_data in enumerate(dataset.get_batch(batch_size,session_length,phase=\"test\")):\n",
    "                sessions = torch.tensor(batch_data[0]).to(device)\n",
    "                target_items = np.array(batch_data[1])-1\n",
    "                y_pred = model.predict_top_k(sessions,torch.tensor(session_length).unsqueeze(0).repeat(target_items.shape[0]),20).cpu().numpy()\n",
    "#                 print(y_pred[:2],target_items[:2])\n",
    "                \n",
    "                for j,predict_num in enumerate(predict_nums):\n",
    "                    hit_nums[j]+=get_hit_num(y_pred[:,:predict_num],target_items)\n",
    "                    rrs[j]+=get_rr(y_pred[:,:predict_num],target_items)\n",
    "                    ndcgs[j]+=get_ndcg(y_pred[:,:predict_num],target_items)\n",
    "                    \n",
    "            end_test_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            hrs = [hit_num/len(dataset.all_testing_data) for hit_num in hit_nums]\n",
    "            mrrs = [rr/len(dataset.all_testing_data) for rr in rrs]\n",
    "            mndcgs = [ndcg/len(dataset.all_testing_data) for ndcg in ndcgs]\n",
    "            if hrs[-1] + mrrs[-1] > best_r1m:\n",
    "                print(\"change best\")\n",
    "                best_model = deepcopy(model)\n",
    "                best_model_hr = hrs[-1]\n",
    "                best_model_mrr = mrrs[-1]\n",
    "                best_r1m = hrs[-1] + mrrs[-1]\n",
    "                no_improvement_epoch = 0\n",
    "            else:\n",
    "                no_improvement_epoch +=1\n",
    "            print(\"testing finish [%s] \"%end_test_time)\n",
    "            for k,predict_num in enumerate(predict_nums):\n",
    "                print(\"\\tHR@%d=%.5f  MRR@%d=%.5f  NDCG@%d=%.5f\"%(predict_num,hrs[k],predict_num,mrrs[k],predict_num,mndcgs[k]))\n",
    "        if no_improvement_epoch>=patience:\n",
    "            print(\"early stopping\")\n",
    "            break\n",
    "    return best_model,best_model_hr,best_model_mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-25T13:39:09.512735Z",
     "start_time": "2019-12-25T12:33:36.061237Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current model hyper-parameters: session_length=19, hidden_size=100, lr=0.0030, embedding_dim=100, embedding_dropout=0.25, dropout=0.50\n",
      "\n",
      "Start building the all training dataset\n",
      "The total number of training samples is： (749947, 20)\n",
      "[2019-12-25 20:33:42] [1/50] 0 mean_batch_loss : 19.477875\n",
      "[2019-12-25 20:33:52] [1/50] 500 mean_batch_loss : 9.517089\n",
      "[2019-12-25 20:34:02] [1/50] 1000 mean_batch_loss : 8.867698\n",
      "Start predicting 2019-12-25 20:34:11\n",
      "The total number of testing samples is： (28445, 20)\n",
      "change best\n",
      "testing finish [2019-12-25 20:34:13] \n",
      "\tHR@1=0.26388  MRR@1=0.26388  NDCG@1=0.26388\n",
      "\tHR@5=0.32491  MRR@5=0.28867  NDCG@5=0.29782\n",
      "\tHR@10=0.34133  MRR@10=0.29088  NDCG@10=0.30315\n",
      "\tHR@20=0.35521  MRR@20=0.29186  NDCG@20=0.30668\n",
      "[2019-12-25 20:34:13] [2/50] 0 mean_batch_loss : 8.189659\n",
      "[2019-12-25 20:34:23] [2/50] 500 mean_batch_loss : 7.957202\n",
      "[2019-12-25 20:34:33] [2/50] 1000 mean_batch_loss : 7.712500\n",
      "Start predicting 2019-12-25 20:34:42\n",
      "change best\n",
      "testing finish [2019-12-25 20:34:44] \n",
      "\tHR@1=0.26339  MRR@1=0.26339  NDCG@1=0.26339\n",
      "\tHR@5=0.35975  MRR@5=0.30199  NDCG@5=0.31653\n",
      "\tHR@10=0.38422  MRR@10=0.30526  NDCG@10=0.32445\n",
      "\tHR@20=0.40721  MRR@20=0.30686  NDCG@20=0.33027\n",
      "[2019-12-25 20:34:44] [3/50] 0 mean_batch_loss : 7.153937\n",
      "[2019-12-25 20:34:54] [3/50] 500 mean_batch_loss : 7.071391\n",
      "[2019-12-25 20:35:04] [3/50] 1000 mean_batch_loss : 6.872604\n",
      "Start predicting 2019-12-25 20:35:13\n",
      "change best\n",
      "testing finish [2019-12-25 20:35:15] \n",
      "\tHR@1=0.26163  MRR@1=0.26163  NDCG@1=0.26163\n",
      "\tHR@5=0.38552  MRR@5=0.31003  NDCG@5=0.32897\n",
      "\tHR@10=0.42229  MRR@10=0.31501  NDCG@10=0.34093\n",
      "\tHR@20=0.45562  MRR@20=0.31734  NDCG@20=0.34938\n",
      "[2019-12-25 20:35:15] [4/50] 0 mean_batch_loss : 6.129573\n",
      "[2019-12-25 20:35:25] [4/50] 500 mean_batch_loss : 6.282440\n",
      "[2019-12-25 20:35:35] [4/50] 1000 mean_batch_loss : 6.140389\n",
      "Start predicting 2019-12-25 20:35:43\n",
      "change best\n",
      "testing finish [2019-12-25 20:35:46] \n",
      "\tHR@1=0.25678  MRR@1=0.25678  NDCG@1=0.25678\n",
      "\tHR@5=0.40865  MRR@5=0.31443  NDCG@5=0.33798\n",
      "\tHR@10=0.45505  MRR@10=0.32072  NDCG@10=0.35308\n",
      "\tHR@20=0.50012  MRR@20=0.32388  NDCG@20=0.36451\n",
      "[2019-12-25 20:35:46] [5/50] 0 mean_batch_loss : 5.705936\n",
      "[2019-12-25 20:35:56] [5/50] 500 mean_batch_loss : 5.661586\n",
      "[2019-12-25 20:36:06] [5/50] 1000 mean_batch_loss : 5.600325\n",
      "Start predicting 2019-12-25 20:36:14\n",
      "change best\n",
      "testing finish [2019-12-25 20:36:16] \n",
      "\tHR@1=0.25611  MRR@1=0.25611  NDCG@1=0.25611\n",
      "\tHR@5=0.42183  MRR@5=0.31820  NDCG@5=0.34406\n",
      "\tHR@10=0.47924  MRR@10=0.32593  NDCG@10=0.36269\n",
      "\tHR@20=0.53176  MRR@20=0.32958  NDCG@20=0.37599\n",
      "[2019-12-25 20:36:16] [6/50] 0 mean_batch_loss : 5.199444\n",
      "[2019-12-25 20:36:26] [6/50] 500 mean_batch_loss : 5.244335\n",
      "[2019-12-25 20:36:36] [6/50] 1000 mean_batch_loss : 5.220963\n",
      "Start predicting 2019-12-25 20:36:45\n",
      "change best\n",
      "testing finish [2019-12-25 20:36:47] \n",
      "\tHR@1=0.25347  MRR@1=0.25347  NDCG@1=0.25347\n",
      "\tHR@5=0.43052  MRR@5=0.31989  NDCG@5=0.34750\n",
      "\tHR@10=0.49140  MRR@10=0.32808  NDCG@10=0.36726\n",
      "\tHR@20=0.54765  MRR@20=0.33203  NDCG@20=0.38153\n",
      "[2019-12-25 20:36:47] [7/50] 0 mean_batch_loss : 4.841931\n",
      "[2019-12-25 20:36:57] [7/50] 500 mean_batch_loss : 4.947340\n",
      "[2019-12-25 20:37:07] [7/50] 1000 mean_batch_loss : 4.961912\n",
      "Start predicting 2019-12-25 20:37:15\n",
      "change best\n",
      "testing finish [2019-12-25 20:37:18] \n",
      "\tHR@1=0.25361  MRR@1=0.25361  NDCG@1=0.25361\n",
      "\tHR@5=0.43874  MRR@5=0.32286  NDCG@5=0.35177\n",
      "\tHR@10=0.50353  MRR@10=0.33154  NDCG@10=0.37275\n",
      "\tHR@20=0.56506  MRR@20=0.33584  NDCG@20=0.38835\n",
      "[2019-12-25 20:37:18] [8/50] 0 mean_batch_loss : 4.516107\n",
      "[2019-12-25 20:37:28] [8/50] 500 mean_batch_loss : 4.745313\n",
      "[2019-12-25 20:37:38] [8/50] 1000 mean_batch_loss : 4.776028\n",
      "Start predicting 2019-12-25 20:37:46\n",
      "change best\n",
      "testing finish [2019-12-25 20:37:49] \n",
      "\tHR@1=0.25425  MRR@1=0.25425  NDCG@1=0.25425\n",
      "\tHR@5=0.44328  MRR@5=0.32463  NDCG@5=0.35421\n",
      "\tHR@10=0.51229  MRR@10=0.33392  NDCG@10=0.37661\n",
      "\tHR@20=0.57616  MRR@20=0.33839  NDCG@20=0.39280\n",
      "[2019-12-25 20:37:49] [9/50] 0 mean_batch_loss : 4.620475\n",
      "[2019-12-25 20:37:59] [9/50] 500 mean_batch_loss : 4.593833\n",
      "[2019-12-25 20:38:09] [9/50] 1000 mean_batch_loss : 4.638610\n",
      "Start predicting 2019-12-25 20:38:17\n",
      "change best\n",
      "testing finish [2019-12-25 20:38:19] \n",
      "\tHR@1=0.25361  MRR@1=0.25361  NDCG@1=0.25361\n",
      "\tHR@5=0.44581  MRR@5=0.32497  NDCG@5=0.35509\n",
      "\tHR@10=0.51587  MRR@10=0.33434  NDCG@10=0.37777\n",
      "\tHR@20=0.58281  MRR@20=0.33902  NDCG@20=0.39473\n",
      "[2019-12-25 20:38:19] [10/50] 0 mean_batch_loss : 4.449274\n",
      "[2019-12-25 20:38:29] [10/50] 500 mean_batch_loss : 4.490396\n",
      "[2019-12-25 20:38:39] [10/50] 1000 mean_batch_loss : 4.521799\n",
      "Start predicting 2019-12-25 20:38:48\n",
      "change best\n",
      "testing finish [2019-12-25 20:38:50] \n",
      "\tHR@1=0.25393  MRR@1=0.25393  NDCG@1=0.25393\n",
      "\tHR@5=0.44795  MRR@5=0.32640  NDCG@5=0.35673\n",
      "\tHR@10=0.51854  MRR@10=0.33584  NDCG@10=0.37957\n",
      "\tHR@20=0.58724  MRR@20=0.34062  NDCG@20=0.39696\n",
      "[2019-12-25 20:38:50] [11/50] 0 mean_batch_loss : 4.189518\n",
      "[2019-12-25 20:39:00] [11/50] 500 mean_batch_loss : 4.395432\n",
      "[2019-12-25 20:39:09] [11/50] 1000 mean_batch_loss : 4.445396\n",
      "Start predicting 2019-12-25 20:39:18\n",
      "change best\n",
      "testing finish [2019-12-25 20:39:21] \n",
      "\tHR@1=0.25347  MRR@1=0.25347  NDCG@1=0.25347\n",
      "\tHR@5=0.45228  MRR@5=0.32707  NDCG@5=0.35826\n",
      "\tHR@10=0.52470  MRR@10=0.33673  NDCG@10=0.38168\n",
      "\tHR@20=0.59329  MRR@20=0.34154  NDCG@20=0.39908\n",
      "[2019-12-25 20:39:21] [12/50] 0 mean_batch_loss : 4.438540\n",
      "[2019-12-25 20:39:31] [12/50] 500 mean_batch_loss : 4.322845\n",
      "[2019-12-25 20:39:40] [12/50] 1000 mean_batch_loss : 4.379574\n",
      "Start predicting 2019-12-25 20:39:49\n",
      "change best\n",
      "testing finish [2019-12-25 20:39:51] \n",
      "\tHR@1=0.25361  MRR@1=0.25361  NDCG@1=0.25361\n",
      "\tHR@5=0.45284  MRR@5=0.32733  NDCG@5=0.35859\n",
      "\tHR@10=0.52645  MRR@10=0.33722  NDCG@10=0.38246\n",
      "\tHR@20=0.59557  MRR@20=0.34205  NDCG@20=0.39998\n",
      "[2019-12-25 20:39:51] [13/50] 0 mean_batch_loss : 4.140166\n",
      "[2019-12-25 20:40:01] [13/50] 500 mean_batch_loss : 4.268387\n",
      "[2019-12-25 20:40:10] [13/50] 1000 mean_batch_loss : 4.318870\n",
      "Start predicting 2019-12-25 20:40:20\n",
      "change best\n",
      "testing finish [2019-12-25 20:40:22] \n",
      "\tHR@1=0.25249  MRR@1=0.25249  NDCG@1=0.25249\n",
      "\tHR@5=0.45224  MRR@5=0.32690  NDCG@5=0.35815\n",
      "\tHR@10=0.52786  MRR@10=0.33702  NDCG@10=0.38264\n",
      "\tHR@20=0.59852  MRR@20=0.34194  NDCG@20=0.40051\n",
      "[2019-12-25 20:40:22] [14/50] 0 mean_batch_loss : 4.090778\n",
      "[2019-12-25 20:40:32] [14/50] 500 mean_batch_loss : 4.206364\n",
      "[2019-12-25 20:40:41] [14/50] 1000 mean_batch_loss : 4.279691\n",
      "Start predicting 2019-12-25 20:40:50\n",
      "change best\n",
      "testing finish [2019-12-25 20:40:53] \n",
      "\tHR@1=0.25400  MRR@1=0.25400  NDCG@1=0.25400\n",
      "\tHR@5=0.45628  MRR@5=0.32900  NDCG@5=0.36071\n",
      "\tHR@10=0.52958  MRR@10=0.33888  NDCG@10=0.38451\n",
      "\tHR@20=0.59951  MRR@20=0.34376  NDCG@20=0.40222\n",
      "[2019-12-25 20:40:53] [15/50] 0 mean_batch_loss : 4.466695\n",
      "[2019-12-25 20:41:03] [15/50] 500 mean_batch_loss : 4.171375\n",
      "[2019-12-25 20:41:12] [15/50] 1000 mean_batch_loss : 4.228758\n",
      "Start predicting 2019-12-25 20:41:21\n",
      "change best\n",
      "testing finish [2019-12-25 20:41:23] \n",
      "\tHR@1=0.25442  MRR@1=0.25442  NDCG@1=0.25442\n",
      "\tHR@5=0.45579  MRR@5=0.32911  NDCG@5=0.36068\n",
      "\tHR@10=0.53029  MRR@10=0.33912  NDCG@10=0.38484\n",
      "\tHR@20=0.60172  MRR@20=0.34409  NDCG@20=0.40292\n",
      "[2019-12-25 20:41:24] [16/50] 0 mean_batch_loss : 4.056125\n",
      "[2019-12-25 20:41:34] [16/50] 500 mean_batch_loss : 4.135844\n",
      "[2019-12-25 20:41:43] [16/50] 1000 mean_batch_loss : 4.194915\n",
      "Start predicting 2019-12-25 20:41:52\n",
      "change best\n",
      "testing finish [2019-12-25 20:41:54] \n",
      "\tHR@1=0.25555  MRR@1=0.25555  NDCG@1=0.25555\n",
      "\tHR@5=0.45565  MRR@5=0.32995  NDCG@5=0.36129\n",
      "\tHR@10=0.53204  MRR@10=0.34022  NDCG@10=0.38606\n",
      "\tHR@20=0.60443  MRR@20=0.34530  NDCG@20=0.40443\n",
      "[2019-12-25 20:41:54] [17/50] 0 mean_batch_loss : 4.334820\n",
      "[2019-12-25 20:42:04] [17/50] 500 mean_batch_loss : 4.098192\n",
      "[2019-12-25 20:42:13] [17/50] 1000 mean_batch_loss : 4.166190\n",
      "Start predicting 2019-12-25 20:42:23\n",
      "testing finish [2019-12-25 20:42:25] \n",
      "\tHR@1=0.25512  MRR@1=0.25512  NDCG@1=0.25512\n",
      "\tHR@5=0.45512  MRR@5=0.32979  NDCG@5=0.36105\n",
      "\tHR@10=0.53247  MRR@10=0.34021  NDCG@10=0.38616\n",
      "\tHR@20=0.60411  MRR@20=0.34522  NDCG@20=0.40432\n",
      "[2019-12-25 20:42:25] [18/50] 0 mean_batch_loss : 3.877727\n",
      "[2019-12-25 20:42:34] [18/50] 500 mean_batch_loss : 4.070949\n",
      "[2019-12-25 20:42:44] [18/50] 1000 mean_batch_loss : 4.141719\n",
      "Start predicting 2019-12-25 20:42:53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change best\n",
      "testing finish [2019-12-25 20:42:56] \n",
      "\tHR@1=0.25323  MRR@1=0.25323  NDCG@1=0.25323\n",
      "\tHR@5=0.45867  MRR@5=0.32990  NDCG@5=0.36202\n",
      "\tHR@10=0.53465  MRR@10=0.34009  NDCG@10=0.38664\n",
      "\tHR@20=0.60826  MRR@20=0.34525  NDCG@20=0.40532\n",
      "[2019-12-25 20:42:56] [19/50] 0 mean_batch_loss : 4.145279\n",
      "[2019-12-25 20:43:05] [19/50] 500 mean_batch_loss : 4.036162\n",
      "[2019-12-25 20:43:15] [19/50] 1000 mean_batch_loss : 4.107106\n",
      "Start predicting 2019-12-25 20:43:24\n",
      "testing finish [2019-12-25 20:43:26] \n",
      "\tHR@1=0.25414  MRR@1=0.25414  NDCG@1=0.25414\n",
      "\tHR@5=0.45678  MRR@5=0.32968  NDCG@5=0.36137\n",
      "\tHR@10=0.53517  MRR@10=0.34021  NDCG@10=0.38679\n",
      "\tHR@20=0.60679  MRR@20=0.34521  NDCG@20=0.40494\n",
      "[2019-12-25 20:43:27] [20/50] 0 mean_batch_loss : 3.774957\n",
      "[2019-12-25 20:43:36] [20/50] 500 mean_batch_loss : 4.007042\n",
      "[2019-12-25 20:43:46] [20/50] 1000 mean_batch_loss : 4.084434\n",
      "Start predicting 2019-12-25 20:43:55\n",
      "testing finish [2019-12-25 20:43:57] \n",
      "\tHR@1=0.25298  MRR@1=0.25298  NDCG@1=0.25298\n",
      "\tHR@5=0.45727  MRR@5=0.32900  NDCG@5=0.36098\n",
      "\tHR@10=0.53472  MRR@10=0.33939  NDCG@10=0.38607\n",
      "\tHR@20=0.60770  MRR@20=0.34444  NDCG@20=0.40451\n",
      "[2019-12-25 20:43:57] [21/50] 0 mean_batch_loss : 3.999552\n",
      "[2019-12-25 20:44:06] [21/50] 500 mean_batch_loss : 3.991621\n",
      "[2019-12-25 20:44:16] [21/50] 1000 mean_batch_loss : 4.058061\n",
      "Start predicting 2019-12-25 20:44:26\n",
      "change best\n",
      "testing finish [2019-12-25 20:44:28] \n",
      "\tHR@1=0.25147  MRR@1=0.25147  NDCG@1=0.25147\n",
      "\tHR@5=0.45832  MRR@5=0.32916  NDCG@5=0.36141\n",
      "\tHR@10=0.53524  MRR@10=0.33954  NDCG@10=0.38640\n",
      "\tHR@20=0.60928  MRR@20=0.34473  NDCG@20=0.40518\n",
      "[2019-12-25 20:44:28] [22/50] 0 mean_batch_loss : 3.820809\n",
      "[2019-12-25 20:44:37] [22/50] 500 mean_batch_loss : 3.967176\n",
      "[2019-12-25 20:44:47] [22/50] 1000 mean_batch_loss : 4.035470\n",
      "Start predicting 2019-12-25 20:44:56\n",
      "testing finish [2019-12-25 20:44:59] \n",
      "\tHR@1=0.25326  MRR@1=0.25326  NDCG@1=0.25326\n",
      "\tHR@5=0.45773  MRR@5=0.32961  NDCG@5=0.36156\n",
      "\tHR@10=0.53503  MRR@10=0.34000  NDCG@10=0.38663\n",
      "\tHR@20=0.60742  MRR@20=0.34509  NDCG@20=0.40501\n",
      "[2019-12-25 20:44:59] [23/50] 0 mean_batch_loss : 3.797743\n",
      "[2019-12-25 20:45:08] [23/50] 500 mean_batch_loss : 3.951634\n",
      "[2019-12-25 20:45:18] [23/50] 1000 mean_batch_loss : 4.019031\n",
      "Start predicting 2019-12-25 20:45:27\n",
      "testing finish [2019-12-25 20:45:29] \n",
      "\tHR@1=0.23544  MRR@1=0.23544  NDCG@1=0.23544\n",
      "\tHR@5=0.40236  MRR@5=0.29757  NDCG@5=0.32370\n",
      "\tHR@10=0.47298  MRR@10=0.30708  NDCG@10=0.34662\n",
      "\tHR@20=0.53950  MRR@20=0.31168  NDCG@20=0.36342\n",
      "[2019-12-25 20:45:29] [24/50] 0 mean_batch_loss : 5.539652\n",
      "[2019-12-25 20:45:38] [24/50] 500 mean_batch_loss : 5.397971\n",
      "[2019-12-25 20:45:48] [24/50] 1000 mean_batch_loss : 5.027765\n",
      "Start predicting 2019-12-25 20:45:58\n",
      "testing finish [2019-12-25 20:46:00] \n",
      "\tHR@1=0.25010  MRR@1=0.25010  NDCG@1=0.25010\n",
      "\tHR@5=0.43115  MRR@5=0.31697  NDCG@5=0.34540\n",
      "\tHR@10=0.50508  MRR@10=0.32685  NDCG@10=0.36933\n",
      "\tHR@20=0.57275  MRR@20=0.33155  NDCG@20=0.38644\n",
      "[2019-12-25 20:46:00] [25/50] 0 mean_batch_loss : 4.505845\n",
      "[2019-12-25 20:46:13] [25/50] 500 mean_batch_loss : 4.762660\n",
      "[2019-12-25 20:46:27] [25/50] 1000 mean_batch_loss : 5.591010\n",
      "Start predicting 2019-12-25 20:46:42\n",
      "testing finish [2019-12-25 20:46:45] \n",
      "\tHR@1=0.05969  MRR@1=0.05969  NDCG@1=0.05969\n",
      "\tHR@5=0.15532  MRR@5=0.09397  NDCG@5=0.10919\n",
      "\tHR@10=0.21146  MRR@10=0.10144  NDCG@10=0.12732\n",
      "\tHR@20=0.27263  MRR@20=0.10570  NDCG@20=0.14280\n",
      "[2019-12-25 20:46:45] [26/50] 0 mean_batch_loss : 8.688214\n",
      "[2019-12-25 20:47:00] [26/50] 500 mean_batch_loss : 8.722859\n",
      "[2019-12-25 20:47:16] [26/50] 1000 mean_batch_loss : 8.379075\n",
      "Start predicting 2019-12-25 20:47:31\n",
      "testing finish [2019-12-25 20:47:33] \n",
      "\tHR@1=0.06883  MRR@1=0.06883  NDCG@1=0.06883\n",
      "\tHR@5=0.16425  MRR@5=0.10279  NDCG@5=0.11802\n",
      "\tHR@10=0.21684  MRR@10=0.10983  NDCG@10=0.13505\n",
      "\tHR@20=0.27372  MRR@20=0.11376  NDCG@20=0.14942\n",
      "early stopping\n",
      "best model change\n",
      "current model hyper-parameters: session_length=19, hidden_size=100, lr=0.0030, embedding_dim=100, embedding_dropout=0.25, dropout=0.50\n",
      "\n",
      "current model HR@20=0.60928  MRR@20=0.34473\n",
      "the best result so far. HR@20=0.60928  MRR@20=0.34473, current model hyper-parameters: session_length=19, hidden_size=100, lr=0.0030, embedding_dim=100, embedding_dropout=0.25, dropout=0.50\n",
      " \n",
      "\n",
      "current model hyper-parameters: session_length=19, hidden_size=100, lr=0.0020, embedding_dim=100, embedding_dropout=0.25, dropout=0.50\n",
      "\n",
      "[2019-12-25 20:47:33] [1/50] 0 mean_batch_loss : 15.882689\n",
      "[2019-12-25 20:47:49] [1/50] 500 mean_batch_loss : 9.601093\n",
      "[2019-12-25 20:48:05] [1/50] 1000 mean_batch_loss : 9.045483\n",
      "Start predicting 2019-12-25 20:48:20\n",
      "change best\n",
      "testing finish [2019-12-25 20:48:22] \n",
      "\tHR@1=0.26469  MRR@1=0.26469  NDCG@1=0.26469\n",
      "\tHR@5=0.32406  MRR@5=0.28940  NDCG@5=0.29818\n",
      "\tHR@10=0.33447  MRR@10=0.29080  NDCG@10=0.30155\n",
      "\tHR@20=0.34551  MRR@20=0.29155  NDCG@20=0.30433\n",
      "[2019-12-25 20:48:22] [2/50] 0 mean_batch_loss : 8.134255\n",
      "[2019-12-25 20:48:37] [2/50] 500 mean_batch_loss : 8.249866\n",
      "[2019-12-25 20:48:53] [2/50] 1000 mean_batch_loss : 8.009636\n",
      "Start predicting 2019-12-25 20:49:08\n",
      "change best\n",
      "testing finish [2019-12-25 20:49:11] \n",
      "\tHR@1=0.26817  MRR@1=0.26817  NDCG@1=0.26817\n",
      "\tHR@5=0.34502  MRR@5=0.29942  NDCG@5=0.31093\n",
      "\tHR@10=0.36224  MRR@10=0.30176  NDCG@10=0.31654\n",
      "\tHR@20=0.38003  MRR@20=0.30299  NDCG@20=0.32102\n",
      "[2019-12-25 20:49:11] [3/50] 0 mean_batch_loss : 7.433704\n",
      "[2019-12-25 20:49:26] [3/50] 500 mean_batch_loss : 7.536087\n",
      "[2019-12-25 20:49:42] [3/50] 1000 mean_batch_loss : 7.382501\n",
      "Start predicting 2019-12-25 20:49:56\n",
      "change best\n",
      "testing finish [2019-12-25 20:49:59] \n",
      "\tHR@1=0.26651  MRR@1=0.26651  NDCG@1=0.26651\n",
      "\tHR@5=0.36615  MRR@5=0.30609  NDCG@5=0.32120\n",
      "\tHR@10=0.39213  MRR@10=0.30955  NDCG@10=0.32958\n",
      "\tHR@20=0.41557  MRR@20=0.31117  NDCG@20=0.33551\n",
      "[2019-12-25 20:49:59] [4/50] 0 mean_batch_loss : 7.274817\n",
      "[2019-12-25 20:50:14] [4/50] 500 mean_batch_loss : 6.945007\n",
      "[2019-12-25 20:50:30] [4/50] 1000 mean_batch_loss : 6.827518\n",
      "Start predicting 2019-12-25 20:50:45\n",
      "change best\n",
      "testing finish [2019-12-25 20:50:48] \n",
      "\tHR@1=0.26384  MRR@1=0.26384  NDCG@1=0.26384\n",
      "\tHR@5=0.38580  MRR@5=0.31134  NDCG@5=0.33001\n",
      "\tHR@10=0.42025  MRR@10=0.31598  NDCG@10=0.34119\n",
      "\tHR@20=0.45273  MRR@20=0.31823  NDCG@20=0.34941\n",
      "[2019-12-25 20:50:48] [5/50] 0 mean_batch_loss : 6.414940\n",
      "[2019-12-25 20:51:03] [5/50] 500 mean_batch_loss : 6.394015\n",
      "[2019-12-25 20:51:19] [5/50] 1000 mean_batch_loss : 6.294621\n",
      "Start predicting 2019-12-25 20:51:33\n",
      "change best\n",
      "testing finish [2019-12-25 20:51:36] \n",
      "\tHR@1=0.26208  MRR@1=0.26208  NDCG@1=0.26208\n",
      "\tHR@5=0.40232  MRR@5=0.31588  NDCG@5=0.33751\n",
      "\tHR@10=0.44676  MRR@10=0.32182  NDCG@10=0.35189\n",
      "\tHR@20=0.48360  MRR@20=0.32436  NDCG@20=0.36118\n",
      "[2019-12-25 20:51:36] [6/50] 0 mean_batch_loss : 6.059760\n",
      "[2019-12-25 20:51:51] [6/50] 500 mean_batch_loss : 5.938776\n",
      "[2019-12-25 20:52:07] [6/50] 1000 mean_batch_loss : 5.858527\n",
      "Start predicting 2019-12-25 20:52:22\n",
      "change best\n",
      "testing finish [2019-12-25 20:52:24] \n",
      "\tHR@1=0.26040  MRR@1=0.26040  NDCG@1=0.26040\n",
      "\tHR@5=0.41540  MRR@5=0.31968  NDCG@5=0.34363\n",
      "\tHR@10=0.46578  MRR@10=0.32647  NDCG@10=0.35999\n",
      "\tHR@20=0.51123  MRR@20=0.32968  NDCG@20=0.37155\n",
      "[2019-12-25 20:52:25] [7/50] 0 mean_batch_loss : 5.684238\n",
      "[2019-12-25 20:52:39] [7/50] 500 mean_batch_loss : 5.552372\n",
      "[2019-12-25 20:52:55] [7/50] 1000 mean_batch_loss : 5.525625\n",
      "Start predicting 2019-12-25 20:53:10\n",
      "change best\n",
      "testing finish [2019-12-25 20:53:13] \n",
      "\tHR@1=0.25962  MRR@1=0.25962  NDCG@1=0.25962\n",
      "\tHR@5=0.42211  MRR@5=0.32102  NDCG@5=0.34628\n",
      "\tHR@10=0.48012  MRR@10=0.32882  NDCG@10=0.36510\n",
      "\tHR@20=0.53043  MRR@20=0.33235  NDCG@20=0.37785\n",
      "[2019-12-25 20:53:13] [8/50] 0 mean_batch_loss : 5.219958\n",
      "[2019-12-25 20:53:28] [8/50] 500 mean_batch_loss : 5.255276\n",
      "[2019-12-25 20:53:44] [8/50] 1000 mean_batch_loss : 5.246912\n",
      "Start predicting 2019-12-25 20:53:59\n",
      "change best\n",
      "testing finish [2019-12-25 20:54:01] \n",
      "\tHR@1=0.25931  MRR@1=0.25931  NDCG@1=0.25931\n",
      "\tHR@5=0.42805  MRR@5=0.32266  NDCG@5=0.34896\n",
      "\tHR@10=0.48912  MRR@10=0.33081  NDCG@10=0.36870\n",
      "\tHR@20=0.54414  MRR@20=0.33468  NDCG@20=0.38268\n",
      "[2019-12-25 20:54:01] [9/50] 0 mean_batch_loss : 5.096253\n",
      "[2019-12-25 20:54:16] [9/50] 500 mean_batch_loss : 5.023085\n",
      "[2019-12-25 20:54:32] [9/50] 1000 mean_batch_loss : 5.032872\n",
      "Start predicting 2019-12-25 20:54:47\n",
      "change best\n",
      "testing finish [2019-12-25 20:54:50] \n",
      "\tHR@1=0.25614  MRR@1=0.25614  NDCG@1=0.25614\n",
      "\tHR@5=0.43656  MRR@5=0.32341  NDCG@5=0.35162\n",
      "\tHR@10=0.50114  MRR@10=0.33206  NDCG@10=0.37254\n",
      "\tHR@20=0.55873  MRR@20=0.33610  NDCG@20=0.38715\n",
      "[2019-12-25 20:54:50] [10/50] 0 mean_batch_loss : 4.777730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-12-25 20:55:05] [10/50] 500 mean_batch_loss : 4.855823\n",
      "[2019-12-25 20:55:21] [10/50] 1000 mean_batch_loss : 4.874726\n",
      "Start predicting 2019-12-25 20:55:36\n",
      "change best\n",
      "testing finish [2019-12-25 20:55:38] \n",
      "\tHR@1=0.25632  MRR@1=0.25632  NDCG@1=0.25632\n",
      "\tHR@5=0.44110  MRR@5=0.32507  NDCG@5=0.35399\n",
      "\tHR@10=0.50526  MRR@10=0.33368  NDCG@10=0.37478\n",
      "\tHR@20=0.56709  MRR@20=0.33800  NDCG@20=0.39045\n",
      "[2019-12-25 20:55:39] [11/50] 0 mean_batch_loss : 4.647040\n",
      "[2019-12-25 20:55:54] [11/50] 500 mean_batch_loss : 4.717923\n",
      "[2019-12-25 20:56:09] [11/50] 1000 mean_batch_loss : 4.736305\n",
      "Start predicting 2019-12-25 20:56:24\n",
      "change best\n",
      "testing finish [2019-12-25 20:56:27] \n",
      "\tHR@1=0.25611  MRR@1=0.25611  NDCG@1=0.25611\n",
      "\tHR@5=0.44127  MRR@5=0.32516  NDCG@5=0.35412\n",
      "\tHR@10=0.51134  MRR@10=0.33463  NDCG@10=0.37689\n",
      "\tHR@20=0.57402  MRR@20=0.33901  NDCG@20=0.39278\n",
      "[2019-12-25 20:56:27] [12/50] 0 mean_batch_loss : 4.658372\n",
      "[2019-12-25 20:56:42] [12/50] 500 mean_batch_loss : 4.598761\n",
      "[2019-12-25 20:56:58] [12/50] 1000 mean_batch_loss : 4.634290\n",
      "Start predicting 2019-12-25 20:57:13\n",
      "change best\n",
      "testing finish [2019-12-25 20:57:15] \n",
      "\tHR@1=0.25526  MRR@1=0.25526  NDCG@1=0.25526\n",
      "\tHR@5=0.44539  MRR@5=0.32608  NDCG@5=0.35583\n",
      "\tHR@10=0.51587  MRR@10=0.33558  NDCG@10=0.37872\n",
      "\tHR@20=0.57859  MRR@20=0.33994  NDCG@20=0.39459\n",
      "[2019-12-25 20:57:16] [13/50] 0 mean_batch_loss : 4.481632\n",
      "[2019-12-25 20:57:31] [13/50] 500 mean_batch_loss : 4.514126\n",
      "[2019-12-25 20:57:47] [13/50] 1000 mean_batch_loss : 4.541315\n",
      "Start predicting 2019-12-25 20:58:01\n",
      "change best\n",
      "testing finish [2019-12-25 20:58:04] \n",
      "\tHR@1=0.25572  MRR@1=0.25572  NDCG@1=0.25572\n",
      "\tHR@5=0.44830  MRR@5=0.32746  NDCG@5=0.35759\n",
      "\tHR@10=0.51865  MRR@10=0.33689  NDCG@10=0.38038\n",
      "\tHR@20=0.58569  MRR@20=0.34158  NDCG@20=0.39738\n",
      "[2019-12-25 20:58:04] [14/50] 0 mean_batch_loss : 4.370964\n",
      "[2019-12-25 20:58:20] [14/50] 500 mean_batch_loss : 4.432680\n",
      "[2019-12-25 20:58:35] [14/50] 1000 mean_batch_loss : 4.470108\n",
      "Start predicting 2019-12-25 20:58:50\n",
      "change best\n",
      "testing finish [2019-12-25 20:58:52] \n",
      "\tHR@1=0.25526  MRR@1=0.25526  NDCG@1=0.25526\n",
      "\tHR@5=0.44830  MRR@5=0.32769  NDCG@5=0.35780\n",
      "\tHR@10=0.52266  MRR@10=0.33762  NDCG@10=0.38184\n",
      "\tHR@20=0.58980  MRR@20=0.34231  NDCG@20=0.39886\n",
      "[2019-12-25 20:58:52] [15/50] 0 mean_batch_loss : 4.362771\n",
      "[2019-12-25 20:59:09] [15/50] 500 mean_batch_loss : 4.363166\n",
      "[2019-12-25 20:59:24] [15/50] 1000 mean_batch_loss : 4.417001\n",
      "Start predicting 2019-12-25 20:59:38\n",
      "change best\n",
      "testing finish [2019-12-25 20:59:41] \n",
      "\tHR@1=0.25505  MRR@1=0.25505  NDCG@1=0.25505\n",
      "\tHR@5=0.44943  MRR@5=0.32750  NDCG@5=0.35790\n",
      "\tHR@10=0.52498  MRR@10=0.33764  NDCG@10=0.38239\n",
      "\tHR@20=0.59518  MRR@20=0.34254  NDCG@20=0.40017\n",
      "[2019-12-25 20:59:41] [16/50] 0 mean_batch_loss : 4.208313\n",
      "[2019-12-25 20:59:57] [16/50] 500 mean_batch_loss : 4.309540\n",
      "[2019-12-25 21:00:12] [16/50] 1000 mean_batch_loss : 4.351667\n",
      "Start predicting 2019-12-25 21:00:27\n",
      "change best\n",
      "testing finish [2019-12-25 21:00:30] \n",
      "\tHR@1=0.25713  MRR@1=0.25713  NDCG@1=0.25713\n",
      "\tHR@5=0.45316  MRR@5=0.33020  NDCG@5=0.36086\n",
      "\tHR@10=0.52744  MRR@10=0.34011  NDCG@10=0.38488\n",
      "\tHR@20=0.59501  MRR@20=0.34482  NDCG@20=0.40198\n",
      "[2019-12-25 21:00:30] [17/50] 0 mean_batch_loss : 4.418394\n",
      "[2019-12-25 21:00:46] [17/50] 500 mean_batch_loss : 4.270882\n",
      "[2019-12-25 21:01:01] [17/50] 1000 mean_batch_loss : 4.304752\n",
      "Start predicting 2019-12-25 21:01:16\n",
      "change best\n",
      "testing finish [2019-12-25 21:01:19] \n",
      "\tHR@1=0.25498  MRR@1=0.25498  NDCG@1=0.25498\n",
      "\tHR@5=0.45238  MRR@5=0.32845  NDCG@5=0.35935\n",
      "\tHR@10=0.52821  MRR@10=0.33872  NDCG@10=0.38402\n",
      "\tHR@20=0.59926  MRR@20=0.34364  NDCG@20=0.40197\n",
      "[2019-12-25 21:01:19] [18/50] 0 mean_batch_loss : 4.208925\n",
      "[2019-12-25 21:01:35] [18/50] 500 mean_batch_loss : 4.212235\n",
      "[2019-12-25 21:01:50] [18/50] 1000 mean_batch_loss : 4.265811\n",
      "Start predicting 2019-12-25 21:02:05\n",
      "change best\n",
      "testing finish [2019-12-25 21:02:08] \n",
      "\tHR@1=0.25541  MRR@1=0.25541  NDCG@1=0.25541\n",
      "\tHR@5=0.45505  MRR@5=0.32951  NDCG@5=0.36080\n",
      "\tHR@10=0.53117  MRR@10=0.33973  NDCG@10=0.38547\n",
      "\tHR@20=0.60169  MRR@20=0.34468  NDCG@20=0.40337\n",
      "[2019-12-25 21:02:08] [19/50] 0 mean_batch_loss : 4.271255\n",
      "[2019-12-25 21:02:24] [19/50] 500 mean_batch_loss : 4.171189\n",
      "[2019-12-25 21:02:40] [19/50] 1000 mean_batch_loss : 4.231466\n",
      "Start predicting 2019-12-25 21:02:54\n",
      "change best\n",
      "testing finish [2019-12-25 21:02:57] \n",
      "\tHR@1=0.25758  MRR@1=0.25758  NDCG@1=0.25758\n",
      "\tHR@5=0.45523  MRR@5=0.33121  NDCG@5=0.36214\n",
      "\tHR@10=0.53081  MRR@10=0.34136  NDCG@10=0.38665\n",
      "\tHR@20=0.60137  MRR@20=0.34636  NDCG@20=0.40462\n",
      "[2019-12-25 21:02:57] [20/50] 0 mean_batch_loss : 4.222351\n",
      "[2019-12-25 21:03:14] [20/50] 500 mean_batch_loss : 4.153252\n",
      "[2019-12-25 21:03:29] [20/50] 1000 mean_batch_loss : 4.188991\n",
      "Start predicting 2019-12-25 21:03:44\n",
      "change best\n",
      "testing finish [2019-12-25 21:03:47] \n",
      "\tHR@1=0.25692  MRR@1=0.25692  NDCG@1=0.25692\n",
      "\tHR@5=0.45481  MRR@5=0.33124  NDCG@5=0.36208\n",
      "\tHR@10=0.53313  MRR@10=0.34168  NDCG@10=0.38740\n",
      "\tHR@20=0.60271  MRR@20=0.34653  NDCG@20=0.40502\n",
      "[2019-12-25 21:03:47] [21/50] 0 mean_batch_loss : 3.908792\n",
      "[2019-12-25 21:04:03] [21/50] 500 mean_batch_loss : 4.106956\n",
      "[2019-12-25 21:04:18] [21/50] 1000 mean_batch_loss : 4.179512\n",
      "Start predicting 2019-12-25 21:04:34\n",
      "change best\n",
      "testing finish [2019-12-25 21:04:36] \n",
      "\tHR@1=0.25600  MRR@1=0.25600  NDCG@1=0.25600\n",
      "\tHR@5=0.45572  MRR@5=0.33040  NDCG@5=0.36166\n",
      "\tHR@10=0.53197  MRR@10=0.34066  NDCG@10=0.38640\n",
      "\tHR@20=0.60464  MRR@20=0.34575  NDCG@20=0.40484\n",
      "[2019-12-25 21:04:36] [22/50] 0 mean_batch_loss : 3.923015\n",
      "[2019-12-25 21:04:52] [22/50] 500 mean_batch_loss : 4.083965\n",
      "[2019-12-25 21:05:07] [22/50] 1000 mean_batch_loss : 4.137760\n",
      "Start predicting 2019-12-25 21:05:22\n",
      "change best\n",
      "testing finish [2019-12-25 21:05:25] \n",
      "\tHR@1=0.25632  MRR@1=0.25632  NDCG@1=0.25632\n",
      "\tHR@5=0.45776  MRR@5=0.33127  NDCG@5=0.36280\n",
      "\tHR@10=0.53194  MRR@10=0.34121  NDCG@10=0.38682\n",
      "\tHR@20=0.60510  MRR@20=0.34632  NDCG@20=0.40536\n",
      "[2019-12-25 21:05:25] [23/50] 0 mean_batch_loss : 4.144608\n",
      "[2019-12-25 21:05:41] [23/50] 500 mean_batch_loss : 4.063234\n",
      "[2019-12-25 21:05:57] [23/50] 1000 mean_batch_loss : 4.125571\n",
      "Start predicting 2019-12-25 21:06:12\n",
      "change best\n",
      "testing finish [2019-12-25 21:06:14] \n",
      "\tHR@1=0.25228  MRR@1=0.25228  NDCG@1=0.25228\n",
      "\tHR@5=0.45727  MRR@5=0.32929  NDCG@5=0.36123\n",
      "\tHR@10=0.53630  MRR@10=0.33988  NDCG@10=0.38684\n",
      "\tHR@20=0.60900  MRR@20=0.34496  NDCG@20=0.40526\n",
      "[2019-12-25 21:06:14] [24/50] 0 mean_batch_loss : 3.936186\n",
      "[2019-12-25 21:06:31] [24/50] 500 mean_batch_loss : 4.035724\n",
      "[2019-12-25 21:06:47] [24/50] 1000 mean_batch_loss : 4.091141\n",
      "Start predicting 2019-12-25 21:07:01\n",
      "change best\n",
      "testing finish [2019-12-25 21:07:04] \n",
      "\tHR@1=0.25505  MRR@1=0.25505  NDCG@1=0.25505\n",
      "\tHR@5=0.45850  MRR@5=0.33068  NDCG@5=0.36255\n",
      "\tHR@10=0.53746  MRR@10=0.34123  NDCG@10=0.38809\n",
      "\tHR@20=0.60816  MRR@20=0.34614  NDCG@20=0.40598\n",
      "[2019-12-25 21:07:04] [25/50] 0 mean_batch_loss : 4.012836\n",
      "[2019-12-25 21:07:20] [25/50] 500 mean_batch_loss : 4.019788\n",
      "[2019-12-25 21:07:37] [25/50] 1000 mean_batch_loss : 4.066530\n",
      "Start predicting 2019-12-25 21:07:51\n",
      "testing finish [2019-12-25 21:07:54] \n",
      "\tHR@1=0.25614  MRR@1=0.25614  NDCG@1=0.25614\n",
      "\tHR@5=0.45794  MRR@5=0.33080  NDCG@5=0.36248\n",
      "\tHR@10=0.53714  MRR@10=0.34138  NDCG@10=0.38810\n",
      "\tHR@20=0.60700  MRR@20=0.34624  NDCG@20=0.40578\n",
      "[2019-12-25 21:07:54] [26/50] 0 mean_batch_loss : 3.916645\n",
      "[2019-12-25 21:08:10] [26/50] 500 mean_batch_loss : 3.995116\n",
      "[2019-12-25 21:08:27] [26/50] 1000 mean_batch_loss : 4.048849\n",
      "Start predicting 2019-12-25 21:08:41\n",
      "change best\n",
      "testing finish [2019-12-25 21:08:44] \n",
      "\tHR@1=0.25716  MRR@1=0.25716  NDCG@1=0.25716\n",
      "\tHR@5=0.45980  MRR@5=0.33266  NDCG@5=0.36437\n",
      "\tHR@10=0.53658  MRR@10=0.34291  NDCG@10=0.38920\n",
      "\tHR@20=0.60865  MRR@20=0.34794  NDCG@20=0.40745\n",
      "[2019-12-25 21:08:44] [27/50] 0 mean_batch_loss : 3.927423\n",
      "[2019-12-25 21:09:00] [27/50] 500 mean_batch_loss : 3.983639\n",
      "[2019-12-25 21:09:17] [27/50] 1000 mean_batch_loss : 4.024110\n",
      "Start predicting 2019-12-25 21:09:31\n",
      "testing finish [2019-12-25 21:09:34] \n",
      "\tHR@1=0.25618  MRR@1=0.25618  NDCG@1=0.25618\n",
      "\tHR@5=0.45878  MRR@5=0.33154  NDCG@5=0.36326\n",
      "\tHR@10=0.53668  MRR@10=0.34199  NDCG@10=0.38851\n",
      "\tHR@20=0.60861  MRR@20=0.34698  NDCG@20=0.40669\n",
      "[2019-12-25 21:09:34] [28/50] 0 mean_batch_loss : 3.898450\n",
      "[2019-12-25 21:09:53] [28/50] 500 mean_batch_loss : 3.958076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-12-25 21:10:11] [28/50] 1000 mean_batch_loss : 4.006223\n",
      "Start predicting 2019-12-25 21:10:26\n",
      "change best\n",
      "testing finish [2019-12-25 21:10:29] \n",
      "\tHR@1=0.25590  MRR@1=0.25590  NDCG@1=0.25590\n",
      "\tHR@5=0.46068  MRR@5=0.33208  NDCG@5=0.36414\n",
      "\tHR@10=0.53799  MRR@10=0.34247  NDCG@10=0.38920\n",
      "\tHR@20=0.61030  MRR@20=0.34752  NDCG@20=0.40753\n",
      "[2019-12-25 21:10:29] [29/50] 0 mean_batch_loss : 3.672319\n",
      "[2019-12-25 21:10:46] [29/50] 500 mean_batch_loss : 3.944469\n",
      "[2019-12-25 21:11:04] [29/50] 1000 mean_batch_loss : 3.985588\n",
      "Start predicting 2019-12-25 21:11:16\n",
      "change best\n",
      "testing finish [2019-12-25 21:11:19] \n",
      "\tHR@1=0.25688  MRR@1=0.25688  NDCG@1=0.25688\n",
      "\tHR@5=0.46128  MRR@5=0.33285  NDCG@5=0.36486\n",
      "\tHR@10=0.53623  MRR@10=0.34283  NDCG@10=0.38908\n",
      "\tHR@20=0.61020  MRR@20=0.34800  NDCG@20=0.40782\n",
      "[2019-12-25 21:11:19] [30/50] 0 mean_batch_loss : 3.839088\n",
      "[2019-12-25 21:11:34] [30/50] 500 mean_batch_loss : 3.913616\n",
      "[2019-12-25 21:11:49] [30/50] 1000 mean_batch_loss : 3.984500\n",
      "Start predicting 2019-12-25 21:12:02\n",
      "testing finish [2019-12-25 21:12:05] \n",
      "\tHR@1=0.25421  MRR@1=0.25421  NDCG@1=0.25421\n",
      "\tHR@5=0.45758  MRR@5=0.33014  NDCG@5=0.36194\n",
      "\tHR@10=0.53827  MRR@10=0.34090  NDCG@10=0.38802\n",
      "\tHR@20=0.61072  MRR@20=0.34598  NDCG@20=0.40640\n",
      "[2019-12-25 21:12:05] [31/50] 0 mean_batch_loss : 3.902689\n",
      "[2019-12-25 21:12:23] [31/50] 500 mean_batch_loss : 3.906341\n",
      "[2019-12-25 21:12:40] [31/50] 1000 mean_batch_loss : 3.959720\n",
      "Start predicting 2019-12-25 21:12:55\n",
      "testing finish [2019-12-25 21:12:58] \n",
      "\tHR@1=0.25305  MRR@1=0.25305  NDCG@1=0.25305\n",
      "\tHR@5=0.45934  MRR@5=0.33012  NDCG@5=0.36235\n",
      "\tHR@10=0.53946  MRR@10=0.34087  NDCG@10=0.38832\n",
      "\tHR@20=0.61097  MRR@20=0.34587  NDCG@20=0.40645\n",
      "[2019-12-25 21:12:58] [32/50] 0 mean_batch_loss : 3.629615\n",
      "[2019-12-25 21:13:16] [32/50] 500 mean_batch_loss : 3.899211\n",
      "[2019-12-25 21:13:33] [32/50] 1000 mean_batch_loss : 3.951054\n",
      "Start predicting 2019-12-25 21:13:49\n",
      "change best\n",
      "testing finish [2019-12-25 21:13:52] \n",
      "\tHR@1=0.25558  MRR@1=0.25558  NDCG@1=0.25558\n",
      "\tHR@5=0.46015  MRR@5=0.33150  NDCG@5=0.36357\n",
      "\tHR@10=0.53946  MRR@10=0.34214  NDCG@10=0.38927\n",
      "\tHR@20=0.61160  MRR@20=0.34716  NDCG@20=0.40753\n",
      "[2019-12-25 21:13:52] [33/50] 0 mean_batch_loss : 3.932837\n",
      "[2019-12-25 21:14:09] [33/50] 500 mean_batch_loss : 3.888209\n",
      "[2019-12-25 21:14:26] [33/50] 1000 mean_batch_loss : 3.936187\n",
      "Start predicting 2019-12-25 21:14:41\n",
      "testing finish [2019-12-25 21:14:44] \n",
      "\tHR@1=0.25519  MRR@1=0.25519  NDCG@1=0.25519\n",
      "\tHR@5=0.45874  MRR@5=0.33106  NDCG@5=0.36290\n",
      "\tHR@10=0.53820  MRR@10=0.34173  NDCG@10=0.38866\n",
      "\tHR@20=0.61065  MRR@20=0.34682  NDCG@20=0.40705\n",
      "[2019-12-25 21:14:44] [34/50] 0 mean_batch_loss : 4.046741\n",
      "[2019-12-25 21:15:02] [34/50] 500 mean_batch_loss : 3.866389\n",
      "[2019-12-25 21:15:19] [34/50] 1000 mean_batch_loss : 3.918450\n",
      "Start predicting 2019-12-25 21:15:35\n",
      "change best\n",
      "testing finish [2019-12-25 21:15:37] \n",
      "\tHR@1=0.25477  MRR@1=0.25477  NDCG@1=0.25477\n",
      "\tHR@5=0.46001  MRR@5=0.33109  NDCG@5=0.36323\n",
      "\tHR@10=0.53911  MRR@10=0.34171  NDCG@10=0.38887\n",
      "\tHR@20=0.61308  MRR@20=0.34687  NDCG@20=0.40760\n",
      "[2019-12-25 21:15:37] [35/50] 0 mean_batch_loss : 3.617732\n",
      "[2019-12-25 21:15:55] [35/50] 500 mean_batch_loss : 3.855811\n",
      "[2019-12-25 21:16:12] [35/50] 1000 mean_batch_loss : 3.912682\n",
      "Start predicting 2019-12-25 21:16:28\n",
      "testing finish [2019-12-25 21:16:30] \n",
      "\tHR@1=0.25576  MRR@1=0.25576  NDCG@1=0.25576\n",
      "\tHR@5=0.45927  MRR@5=0.33139  NDCG@5=0.36327\n",
      "\tHR@10=0.53981  MRR@10=0.34224  NDCG@10=0.38941\n",
      "\tHR@20=0.61213  MRR@20=0.34728  NDCG@20=0.40773\n",
      "[2019-12-25 21:16:30] [36/50] 0 mean_batch_loss : 3.677981\n",
      "[2019-12-25 21:16:48] [36/50] 500 mean_batch_loss : 3.849202\n",
      "[2019-12-25 21:17:06] [36/50] 1000 mean_batch_loss : 3.895550\n",
      "Start predicting 2019-12-25 21:17:20\n",
      "testing finish [2019-12-25 21:17:22] \n",
      "\tHR@1=0.25530  MRR@1=0.25530  NDCG@1=0.25530\n",
      "\tHR@5=0.46103  MRR@5=0.33179  NDCG@5=0.36400\n",
      "\tHR@10=0.53922  MRR@10=0.34225  NDCG@10=0.38932\n",
      "\tHR@20=0.61185  MRR@20=0.34731  NDCG@20=0.40770\n",
      "[2019-12-25 21:17:22] [37/50] 0 mean_batch_loss : 3.825289\n",
      "[2019-12-25 21:17:40] [37/50] 500 mean_batch_loss : 3.837232\n",
      "[2019-12-25 21:17:58] [37/50] 1000 mean_batch_loss : 3.882271\n",
      "Start predicting 2019-12-25 21:18:12\n",
      "testing finish [2019-12-25 21:18:15] \n",
      "\tHR@1=0.25597  MRR@1=0.25597  NDCG@1=0.25597\n",
      "\tHR@5=0.45945  MRR@5=0.33131  NDCG@5=0.36324\n",
      "\tHR@10=0.53925  MRR@10=0.34204  NDCG@10=0.38913\n",
      "\tHR@20=0.61252  MRR@20=0.34713  NDCG@20=0.40766\n",
      "[2019-12-25 21:18:15] [38/50] 0 mean_batch_loss : 3.834473\n",
      "[2019-12-25 21:18:30] [38/50] 500 mean_batch_loss : 3.812669\n",
      "[2019-12-25 21:18:45] [38/50] 1000 mean_batch_loss : 3.883937\n",
      "Start predicting 2019-12-25 21:18:57\n",
      "testing finish [2019-12-25 21:18:59] \n",
      "\tHR@1=0.25298  MRR@1=0.25298  NDCG@1=0.25298\n",
      "\tHR@5=0.45980  MRR@5=0.32941  NDCG@5=0.36190\n",
      "\tHR@10=0.53872  MRR@10=0.34000  NDCG@10=0.38747\n",
      "\tHR@20=0.61136  MRR@20=0.34510  NDCG@20=0.40591\n",
      "[2019-12-25 21:18:59] [39/50] 0 mean_batch_loss : 3.524789\n",
      "[2019-12-25 21:19:13] [39/50] 500 mean_batch_loss : 3.812638\n",
      "[2019-12-25 21:19:28] [39/50] 1000 mean_batch_loss : 3.859467\n",
      "Start predicting 2019-12-25 21:19:40\n",
      "testing finish [2019-12-25 21:19:43] \n",
      "\tHR@1=0.25664  MRR@1=0.25664  NDCG@1=0.25664\n",
      "\tHR@5=0.46103  MRR@5=0.33189  NDCG@5=0.36405\n",
      "\tHR@10=0.54002  MRR@10=0.34245  NDCG@10=0.38960\n",
      "\tHR@20=0.61023  MRR@20=0.34736  NDCG@20=0.40741\n",
      "early stopping\n",
      "best model change\n",
      "current model hyper-parameters: session_length=19, hidden_size=100, lr=0.0020, embedding_dim=100, embedding_dropout=0.25, dropout=0.50\n",
      "\n",
      "current model HR@20=0.61308  MRR@20=0.34687\n",
      "the best result so far. HR@20=0.61308  MRR@20=0.34687, current model hyper-parameters: session_length=19, hidden_size=100, lr=0.0020, embedding_dim=100, embedding_dropout=0.25, dropout=0.50\n",
      " \n",
      "\n",
      "current model hyper-parameters: session_length=19, hidden_size=100, lr=0.0050, embedding_dim=100, embedding_dropout=0.25, dropout=0.50\n",
      "\n",
      "[2019-12-25 21:19:43] [1/50] 0 mean_batch_loss : 23.925138\n",
      "[2019-12-25 21:19:57] [1/50] 500 mean_batch_loss : 9.491842\n",
      "[2019-12-25 21:20:12] [1/50] 1000 mean_batch_loss : 8.744322\n",
      "Start predicting 2019-12-25 21:20:24\n",
      "change best\n",
      "testing finish [2019-12-25 21:20:26] \n",
      "\tHR@1=0.25252  MRR@1=0.25252  NDCG@1=0.25252\n",
      "\tHR@5=0.33190  MRR@5=0.28445  NDCG@5=0.29640\n",
      "\tHR@10=0.35293  MRR@10=0.28730  NDCG@10=0.30325\n",
      "\tHR@20=0.37223  MRR@20=0.28864  NDCG@20=0.30813\n",
      "[2019-12-25 21:20:26] [2/50] 0 mean_batch_loss : 7.863735\n",
      "[2019-12-25 21:20:40] [2/50] 500 mean_batch_loss : 7.588056\n",
      "[2019-12-25 21:20:55] [2/50] 1000 mean_batch_loss : 7.232212\n",
      "Start predicting 2019-12-25 21:21:07\n",
      "change best\n",
      "testing finish [2019-12-25 21:21:10] \n",
      "\tHR@1=0.25203  MRR@1=0.25203  NDCG@1=0.25203\n",
      "\tHR@5=0.38021  MRR@5=0.30149  NDCG@5=0.32120\n",
      "\tHR@10=0.41803  MRR@10=0.30661  NDCG@10=0.33351\n",
      "\tHR@20=0.45414  MRR@20=0.30912  NDCG@20=0.34265\n",
      "[2019-12-25 21:21:10] [3/50] 0 mean_batch_loss : 6.187826\n",
      "[2019-12-25 21:21:24] [3/50] 500 mean_batch_loss : 6.290649\n",
      "[2019-12-25 21:21:38] [3/50] 1000 mean_batch_loss : 6.099824\n",
      "Start predicting 2019-12-25 21:21:50\n",
      "change best\n",
      "testing finish [2019-12-25 21:21:53] \n",
      "\tHR@1=0.24848  MRR@1=0.24848  NDCG@1=0.24848\n",
      "\tHR@5=0.40471  MRR@5=0.30796  NDCG@5=0.33216\n",
      "\tHR@10=0.45991  MRR@10=0.31547  NDCG@10=0.35015\n",
      "\tHR@20=0.51106  MRR@20=0.31906  NDCG@20=0.36313\n",
      "[2019-12-25 21:21:53] [4/50] 0 mean_batch_loss : 5.551528\n",
      "[2019-12-25 21:22:07] [4/50] 500 mean_batch_loss : 5.480915\n",
      "[2019-12-25 21:22:22] [4/50] 1000 mean_batch_loss : 5.411844\n",
      "Start predicting 2019-12-25 21:22:34\n",
      "change best\n",
      "testing finish [2019-12-25 21:22:36] \n",
      "\tHR@1=0.24683  MRR@1=0.24683  NDCG@1=0.24683\n",
      "\tHR@5=0.42461  MRR@5=0.31316  NDCG@5=0.34095\n",
      "\tHR@10=0.48824  MRR@10=0.32174  NDCG@10=0.36162\n",
      "\tHR@20=0.54523  MRR@20=0.32575  NDCG@20=0.37609\n",
      "[2019-12-25 21:22:36] [5/50] 0 mean_batch_loss : 5.115784\n",
      "[2019-12-25 21:22:50] [5/50] 500 mean_batch_loss : 5.017624\n",
      "[2019-12-25 21:23:05] [5/50] 1000 mean_batch_loss : 5.031029\n",
      "Start predicting 2019-12-25 21:23:18\n",
      "change best\n",
      "testing finish [2019-12-25 21:23:20] \n",
      "\tHR@1=0.24785  MRR@1=0.24785  NDCG@1=0.24785\n",
      "\tHR@5=0.43185  MRR@5=0.31607  NDCG@5=0.34492\n",
      "\tHR@10=0.50132  MRR@10=0.32548  NDCG@10=0.36752\n",
      "\tHR@20=0.56207  MRR@20=0.32974  NDCG@20=0.38294\n",
      "[2019-12-25 21:23:20] [6/50] 0 mean_batch_loss : 4.734052\n",
      "[2019-12-25 21:23:34] [6/50] 500 mean_batch_loss : 4.751334\n",
      "[2019-12-25 21:23:49] [6/50] 1000 mean_batch_loss : 4.809378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting 2019-12-25 21:24:02\n",
      "change best\n",
      "testing finish [2019-12-25 21:24:04] \n",
      "\tHR@1=0.24619  MRR@1=0.24619  NDCG@1=0.24619\n",
      "\tHR@5=0.43635  MRR@5=0.31737  NDCG@5=0.34706\n",
      "\tHR@10=0.50786  MRR@10=0.32694  NDCG@10=0.37021\n",
      "\tHR@20=0.57669  MRR@20=0.33173  NDCG@20=0.38763\n",
      "[2019-12-25 21:24:04] [7/50] 0 mean_batch_loss : 4.589965\n",
      "[2019-12-25 21:24:17] [7/50] 500 mean_batch_loss : 4.585111\n",
      "[2019-12-25 21:24:32] [7/50] 1000 mean_batch_loss : 4.639550\n",
      "Start predicting 2019-12-25 21:24:46\n",
      "change best\n",
      "testing finish [2019-12-25 21:24:48] \n",
      "\tHR@1=0.24728  MRR@1=0.24728  NDCG@1=0.24728\n",
      "\tHR@5=0.44103  MRR@5=0.31906  NDCG@5=0.34944\n",
      "\tHR@10=0.51492  MRR@10=0.32904  NDCG@10=0.37346\n",
      "\tHR@20=0.58365  MRR@20=0.33381  NDCG@20=0.39084\n",
      "[2019-12-25 21:24:48] [8/50] 0 mean_batch_loss : 4.465520\n",
      "[2019-12-25 21:25:00] [8/50] 500 mean_batch_loss : 4.455627\n",
      "[2019-12-25 21:25:14] [8/50] 1000 mean_batch_loss : 4.543701\n",
      "Start predicting 2019-12-25 21:25:28\n",
      "change best\n",
      "testing finish [2019-12-25 21:25:30] \n",
      "\tHR@1=0.24788  MRR@1=0.24788  NDCG@1=0.24788\n",
      "\tHR@5=0.44282  MRR@5=0.32059  NDCG@5=0.35107\n",
      "\tHR@10=0.51893  MRR@10=0.33071  NDCG@10=0.37565\n",
      "\tHR@20=0.58991  MRR@20=0.33564  NDCG@20=0.39359\n",
      "[2019-12-25 21:25:30] [9/50] 0 mean_batch_loss : 4.245637\n",
      "[2019-12-25 21:25:42] [9/50] 500 mean_batch_loss : 4.364799\n",
      "[2019-12-25 21:25:56] [9/50] 1000 mean_batch_loss : 4.443307\n",
      "Start predicting 2019-12-25 21:26:10\n",
      "change best\n",
      "testing finish [2019-12-25 21:26:12] \n",
      "\tHR@1=0.24858  MRR@1=0.24858  NDCG@1=0.24858\n",
      "\tHR@5=0.44454  MRR@5=0.32156  NDCG@5=0.35222\n",
      "\tHR@10=0.52006  MRR@10=0.33167  NDCG@10=0.37668\n",
      "\tHR@20=0.59258  MRR@20=0.33671  NDCG@20=0.39502\n",
      "[2019-12-25 21:26:12] [10/50] 0 mean_batch_loss : 4.352625\n",
      "[2019-12-25 21:26:24] [10/50] 500 mean_batch_loss : 4.303748\n",
      "[2019-12-25 21:26:38] [10/50] 1000 mean_batch_loss : 4.370664\n",
      "Start predicting 2019-12-25 21:26:52\n",
      "change best\n",
      "testing finish [2019-12-25 21:26:54] \n",
      "\tHR@1=0.24834  MRR@1=0.24834  NDCG@1=0.24834\n",
      "\tHR@5=0.44785  MRR@5=0.32251  NDCG@5=0.35376\n",
      "\tHR@10=0.52512  MRR@10=0.33287  NDCG@10=0.37878\n",
      "\tHR@20=0.59701  MRR@20=0.33785  NDCG@20=0.39695\n",
      "[2019-12-25 21:26:55] [11/50] 0 mean_batch_loss : 4.334111\n",
      "[2019-12-25 21:27:07] [11/50] 500 mean_batch_loss : 4.241456\n",
      "[2019-12-25 21:27:21] [11/50] 1000 mean_batch_loss : 4.317182\n",
      "Start predicting 2019-12-25 21:27:35\n",
      "change best\n",
      "testing finish [2019-12-25 21:27:37] \n",
      "\tHR@1=0.24816  MRR@1=0.24816  NDCG@1=0.24816\n",
      "\tHR@5=0.45164  MRR@5=0.32387  NDCG@5=0.35573\n",
      "\tHR@10=0.52610  MRR@10=0.33380  NDCG@10=0.37981\n",
      "\tHR@20=0.59954  MRR@20=0.33891  NDCG@20=0.39839\n",
      "[2019-12-25 21:27:38] [12/50] 0 mean_batch_loss : 4.097750\n",
      "[2019-12-25 21:27:51] [12/50] 500 mean_batch_loss : 4.191981\n",
      "[2019-12-25 21:28:04] [12/50] 1000 mean_batch_loss : 4.265437\n",
      "Start predicting 2019-12-25 21:28:18\n",
      "testing finish [2019-12-25 21:28:20] \n",
      "\tHR@1=0.24830  MRR@1=0.24830  NDCG@1=0.24830\n",
      "\tHR@5=0.44837  MRR@5=0.32276  NDCG@5=0.35408\n",
      "\tHR@10=0.52586  MRR@10=0.33316  NDCG@10=0.37920\n",
      "\tHR@20=0.59796  MRR@20=0.33822  NDCG@20=0.39749\n",
      "[2019-12-25 21:28:20] [13/50] 0 mean_batch_loss : 3.998987\n",
      "[2019-12-25 21:28:34] [13/50] 500 mean_batch_loss : 4.158485\n",
      "[2019-12-25 21:28:47] [13/50] 1000 mean_batch_loss : 4.228821\n",
      "Start predicting 2019-12-25 21:29:01\n",
      "change best\n",
      "testing finish [2019-12-25 21:29:03] \n",
      "\tHR@1=0.24823  MRR@1=0.24823  NDCG@1=0.24823\n",
      "\tHR@5=0.45080  MRR@5=0.32336  NDCG@5=0.35512\n",
      "\tHR@10=0.52846  MRR@10=0.33376  NDCG@10=0.38027\n",
      "\tHR@20=0.60218  MRR@20=0.33889  NDCG@20=0.39892\n",
      "[2019-12-25 21:29:03] [14/50] 0 mean_batch_loss : 4.235703\n",
      "[2019-12-25 21:29:17] [14/50] 500 mean_batch_loss : 4.113607\n",
      "[2019-12-25 21:29:30] [14/50] 1000 mean_batch_loss : 4.194200\n",
      "Start predicting 2019-12-25 21:29:44\n",
      "change best\n",
      "testing finish [2019-12-25 21:29:46] \n",
      "\tHR@1=0.24707  MRR@1=0.24707  NDCG@1=0.24707\n",
      "\tHR@5=0.45098  MRR@5=0.32327  NDCG@5=0.35512\n",
      "\tHR@10=0.52944  MRR@10=0.33387  NDCG@10=0.38063\n",
      "\tHR@20=0.60257  MRR@20=0.33898  NDCG@20=0.39916\n",
      "[2019-12-25 21:29:46] [15/50] 0 mean_batch_loss : 4.101045\n",
      "[2019-12-25 21:30:01] [15/50] 500 mean_batch_loss : 4.078676\n",
      "[2019-12-25 21:30:13] [15/50] 1000 mean_batch_loss : 4.168988\n",
      "Start predicting 2019-12-25 21:30:27\n",
      "change best\n",
      "testing finish [2019-12-25 21:30:29] \n",
      "\tHR@1=0.25080  MRR@1=0.25080  NDCG@1=0.25080\n",
      "\tHR@5=0.45463  MRR@5=0.32606  NDCG@5=0.35809\n",
      "\tHR@10=0.52997  MRR@10=0.33610  NDCG@10=0.38243\n",
      "\tHR@20=0.60341  MRR@20=0.34122  NDCG@20=0.40103\n",
      "[2019-12-25 21:30:30] [16/50] 0 mean_batch_loss : 4.120343\n",
      "[2019-12-25 21:30:44] [16/50] 500 mean_batch_loss : 4.058647\n",
      "[2019-12-25 21:30:57] [16/50] 1000 mean_batch_loss : 4.144545\n",
      "Start predicting 2019-12-25 21:31:10\n",
      "testing finish [2019-12-25 21:31:12] \n",
      "\tHR@1=0.24866  MRR@1=0.24866  NDCG@1=0.24866\n",
      "\tHR@5=0.45214  MRR@5=0.32425  NDCG@5=0.35613\n",
      "\tHR@10=0.52860  MRR@10=0.33446  NDCG@10=0.38086\n",
      "\tHR@20=0.60165  MRR@20=0.33956  NDCG@20=0.39938\n",
      "[2019-12-25 21:31:13] [17/50] 0 mean_batch_loss : 4.148270\n",
      "[2019-12-25 21:31:27] [17/50] 500 mean_batch_loss : 4.031617\n",
      "[2019-12-25 21:31:40] [17/50] 1000 mean_batch_loss : 4.107780\n",
      "Start predicting 2019-12-25 21:31:53\n",
      "testing finish [2019-12-25 21:31:56] \n",
      "\tHR@1=0.24781  MRR@1=0.24781  NDCG@1=0.24781\n",
      "\tHR@5=0.45189  MRR@5=0.32395  NDCG@5=0.35586\n",
      "\tHR@10=0.53004  MRR@10=0.33443  NDCG@10=0.38118\n",
      "\tHR@20=0.60362  MRR@20=0.33953  NDCG@20=0.39977\n",
      "[2019-12-25 21:31:56] [18/50] 0 mean_batch_loss : 3.903816\n",
      "[2019-12-25 21:32:11] [18/50] 500 mean_batch_loss : 4.007720\n",
      "[2019-12-25 21:32:24] [18/50] 1000 mean_batch_loss : 4.086127\n",
      "Start predicting 2019-12-25 21:32:37\n",
      "change best\n",
      "testing finish [2019-12-25 21:32:39] \n",
      "\tHR@1=0.24989  MRR@1=0.24989  NDCG@1=0.24989\n",
      "\tHR@5=0.45396  MRR@5=0.32626  NDCG@5=0.35812\n",
      "\tHR@10=0.53001  MRR@10=0.33647  NDCG@10=0.38277\n",
      "\tHR@20=0.60306  MRR@20=0.34158  NDCG@20=0.40129\n",
      "[2019-12-25 21:32:39] [19/50] 0 mean_batch_loss : 3.835967\n",
      "[2019-12-25 21:32:54] [19/50] 500 mean_batch_loss : 3.987976\n",
      "[2019-12-25 21:33:07] [19/50] 1000 mean_batch_loss : 4.066261\n",
      "Start predicting 2019-12-25 21:33:20\n",
      "testing finish [2019-12-25 21:33:22] \n",
      "\tHR@1=0.24799  MRR@1=0.24799  NDCG@1=0.24799\n",
      "\tHR@5=0.45330  MRR@5=0.32425  NDCG@5=0.35642\n",
      "\tHR@10=0.53099  MRR@10=0.33464  NDCG@10=0.38156\n",
      "\tHR@20=0.60352  MRR@20=0.33973  NDCG@20=0.39997\n",
      "[2019-12-25 21:33:22] [20/50] 0 mean_batch_loss : 3.854407\n",
      "[2019-12-25 21:33:37] [20/50] 500 mean_batch_loss : 3.980983\n",
      "[2019-12-25 21:33:50] [20/50] 1000 mean_batch_loss : 4.053464\n",
      "Start predicting 2019-12-25 21:34:03\n",
      "change best\n",
      "testing finish [2019-12-25 21:34:05] \n",
      "\tHR@1=0.24915  MRR@1=0.24915  NDCG@1=0.24915\n",
      "\tHR@5=0.45242  MRR@5=0.32458  NDCG@5=0.35645\n",
      "\tHR@10=0.53120  MRR@10=0.33516  NDCG@10=0.38198\n",
      "\tHR@20=0.60457  MRR@20=0.34031  NDCG@20=0.40061\n",
      "[2019-12-25 21:34:06] [21/50] 0 mean_batch_loss : 4.140986\n",
      "[2019-12-25 21:34:20] [21/50] 500 mean_batch_loss : 3.948486\n",
      "[2019-12-25 21:34:33] [21/50] 1000 mean_batch_loss : 4.029251\n",
      "Start predicting 2019-12-25 21:34:46\n",
      "change best\n",
      "testing finish [2019-12-25 21:34:49] \n",
      "\tHR@1=0.25038  MRR@1=0.25038  NDCG@1=0.25038\n",
      "\tHR@5=0.45544  MRR@5=0.32610  NDCG@5=0.35831\n",
      "\tHR@10=0.53335  MRR@10=0.33651  NDCG@10=0.38351\n",
      "\tHR@20=0.60359  MRR@20=0.34143  NDCG@20=0.40134\n",
      "[2019-12-25 21:34:49] [22/50] 0 mean_batch_loss : 3.770952\n",
      "[2019-12-25 21:35:03] [22/50] 500 mean_batch_loss : 3.933373\n",
      "[2019-12-25 21:35:17] [22/50] 1000 mean_batch_loss : 4.011672\n",
      "Start predicting 2019-12-25 21:35:29\n",
      "change best\n",
      "testing finish [2019-12-25 21:35:32] \n",
      "\tHR@1=0.24964  MRR@1=0.24964  NDCG@1=0.24964\n",
      "\tHR@5=0.45354  MRR@5=0.32558  NDCG@5=0.35749\n",
      "\tHR@10=0.53211  MRR@10=0.33608  NDCG@10=0.38291\n",
      "\tHR@20=0.60573  MRR@20=0.34123  NDCG@20=0.40157\n",
      "[2019-12-25 21:35:32] [23/50] 0 mean_batch_loss : 3.708098\n",
      "[2019-12-25 21:35:47] [23/50] 500 mean_batch_loss : 3.917150\n",
      "[2019-12-25 21:36:00] [23/50] 1000 mean_batch_loss : 3.997158\n",
      "Start predicting 2019-12-25 21:36:13\n",
      "testing finish [2019-12-25 21:36:15] \n",
      "\tHR@1=0.24971  MRR@1=0.24971  NDCG@1=0.24971\n",
      "\tHR@5=0.45446  MRR@5=0.32577  NDCG@5=0.35784\n",
      "\tHR@10=0.53370  MRR@10=0.33642  NDCG@10=0.38354\n",
      "\tHR@20=0.60527  MRR@20=0.34142  NDCG@20=0.40168\n",
      "[2019-12-25 21:36:15] [24/50] 0 mean_batch_loss : 3.583512\n",
      "[2019-12-25 21:36:30] [24/50] 500 mean_batch_loss : 3.903495\n",
      "[2019-12-25 21:36:44] [24/50] 1000 mean_batch_loss : 3.985165\n",
      "Start predicting 2019-12-25 21:36:57\n",
      "testing finish [2019-12-25 21:36:59] \n",
      "\tHR@1=0.24848  MRR@1=0.24848  NDCG@1=0.24848\n",
      "\tHR@5=0.45425  MRR@5=0.32460  NDCG@5=0.35689\n",
      "\tHR@10=0.53373  MRR@10=0.33526  NDCG@10=0.38264\n",
      "\tHR@20=0.60401  MRR@20=0.34016  NDCG@20=0.40044\n",
      "[2019-12-25 21:36:59] [25/50] 0 mean_batch_loss : 3.806480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-12-25 21:37:13] [25/50] 500 mean_batch_loss : 3.885161\n",
      "[2019-12-25 21:37:27] [25/50] 1000 mean_batch_loss : 4.017419\n",
      "Start predicting 2019-12-25 21:37:40\n",
      "testing finish [2019-12-25 21:37:43] \n",
      "\tHR@1=0.24869  MRR@1=0.24869  NDCG@1=0.24869\n",
      "\tHR@5=0.45259  MRR@5=0.32457  NDCG@5=0.35648\n",
      "\tHR@10=0.53092  MRR@10=0.33507  NDCG@10=0.38186\n",
      "\tHR@20=0.60390  MRR@20=0.34019  NDCG@20=0.40037\n",
      "[2019-12-25 21:37:43] [26/50] 0 mean_batch_loss : 3.937309\n",
      "[2019-12-25 21:37:57] [26/50] 500 mean_batch_loss : 3.865547\n",
      "[2019-12-25 21:38:11] [26/50] 1000 mean_batch_loss : 3.954139\n",
      "Start predicting 2019-12-25 21:38:23\n",
      "testing finish [2019-12-25 21:38:26] \n",
      "\tHR@1=0.24946  MRR@1=0.24946  NDCG@1=0.24946\n",
      "\tHR@5=0.45080  MRR@5=0.32398  NDCG@5=0.35558\n",
      "\tHR@10=0.53036  MRR@10=0.33464  NDCG@10=0.38134\n",
      "\tHR@20=0.60285  MRR@20=0.33970  NDCG@20=0.39971\n",
      "[2019-12-25 21:38:26] [27/50] 0 mean_batch_loss : 3.513484\n",
      "[2019-12-25 21:38:40] [27/50] 500 mean_batch_loss : 3.860877\n",
      "[2019-12-25 21:38:54] [27/50] 1000 mean_batch_loss : 3.946738\n",
      "Start predicting 2019-12-25 21:39:07\n",
      "testing finish [2019-12-25 21:39:09] \n",
      "\tHR@1=0.24866  MRR@1=0.24866  NDCG@1=0.24866\n",
      "\tHR@5=0.45358  MRR@5=0.32487  NDCG@5=0.35695\n",
      "\tHR@10=0.53377  MRR@10=0.33564  NDCG@10=0.38295\n",
      "\tHR@20=0.60545  MRR@20=0.34061  NDCG@20=0.40107\n",
      "early stopping\n",
      "current model hyper-parameters: session_length=19, hidden_size=100, lr=0.0050, embedding_dim=100, embedding_dropout=0.25, dropout=0.50\n",
      "\n",
      "current model HR@20=0.60573  MRR@20=0.34123\n",
      "the best result so far. HR@20=0.60573  MRR@20=0.34687, current model hyper-parameters: session_length=19, hidden_size=100, lr=0.0020, embedding_dim=100, embedding_dropout=0.25, dropout=0.50\n",
      " \n",
      "\n",
      "The best result HR@20=0.61308  MRR@20=0.34687, hyper-parameters: current model hyper-parameters: session_length=19, hidden_size=100, lr=0.0020, embedding_dim=100, embedding_dropout=0.25, dropout=0.50\n",
      ". \n",
      "over.\n"
     ]
    }
   ],
   "source": [
    "hidden_sizes = [100]\n",
    "embedding_dims = [100]\n",
    "dropouts = [0.5]\n",
    "embedding_dropouts = [0.25]\n",
    "lrs = [3e-3,2e-3,5e-3]\n",
    "session_lengths = [19]\n",
    "patience = 5\n",
    "best_params = \"\"\n",
    "best_all_model = 0.0\n",
    "best_all_hr = 0.0\n",
    "best_all_mrr = 0.0\n",
    "best_all_r1m = 0.0\n",
    "for session_length in session_lengths:\n",
    "    for hidden_size in hidden_sizes:\n",
    "        for embedding_dim in embedding_dims:\n",
    "            for dropout in dropouts:\n",
    "                for embedding_dropout in embedding_dropouts:\n",
    "                    for lr in lrs:\n",
    "                        args = {}\n",
    "                        print(\"current model hyper-parameters: session_length=%d, hidden_size=%d, lr=%.4f, embedding_dim=%d, embedding_dropout=%.2f, dropout=%.2f\\n\" % (session_length,hidden_size,lr,embedding_dim,embedding_dropout,dropout))\n",
    "                        args[\"session_length\"] = session_length\n",
    "                        args[\"hidden_size\"] = hidden_size\n",
    "                        args[\"embedding_dim\"] = embedding_dim\n",
    "                        args[\"dropout\"] = dropout\n",
    "                        args[\"embedding_dropout\"] = embedding_dropout\n",
    "                        args[\"patience\"] = patience\n",
    "                        args[\"lr\"] = lr\n",
    "                        best_model,best_model_hr,best_model_mrr = train(args)\n",
    "                        if best_model_hr + best_model_mrr > best_all_r1m:\n",
    "                            print(\"best model change\")\n",
    "                            best_all_r1m = best_model_hr + best_model_mrr\n",
    "                            best_all_hr = best_model_hr\n",
    "                            best_all_mrr = best_model_mrr\n",
    "                            best_all_model = best_model\n",
    "                            best_params = \"current model hyper-parameters: session_length=%d, hidden_size=%d, lr=%.4f, embedding_dim=%d, embedding_dropout=%.2f, dropout=%.2f\\n\" % (session_length,hidden_size,lr,embedding_dim,embedding_dropout,dropout)\n",
    "                        best_model = None\n",
    "                        print(\"current model hyper-parameters: session_length=%d, hidden_size=%d, lr=%.4f, embedding_dim=%d, embedding_dropout=%.2f, dropout=%.2f\\n\" % (session_length,hidden_size,lr,embedding_dim,embedding_dropout,dropout))\n",
    "                        print(\"current model HR@20=%.5f  MRR@20=%.5f\"%(best_model_hr,best_model_mrr))\n",
    "                        print(\"the best result so far. HR@20=%.5f  MRR@20=%.5f, %s \\n\"%(best_model_hr,best_all_mrr,best_params))\n",
    "print(\"The best result HR@20=%.5f  MRR@20=%.5f, hyper-parameters: %s. \"%(best_all_hr,best_all_mrr,best_params))\n",
    "print(\"over.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smy",
   "language": "python",
   "name": "smy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
