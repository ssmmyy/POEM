{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T07:43:04.428258Z",
     "start_time": "2019-12-23T07:43:04.244922Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T07:43:04.766798Z",
     "start_time": "2019-12-23T07:43:04.429644Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T07:43:04.771896Z",
     "start_time": "2019-12-23T07:43:04.769329Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda:0\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T07:43:04.780464Z",
     "start_time": "2019-12-23T07:43:04.773127Z"
    }
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T07:43:04.787136Z",
     "start_time": "2019-12-23T07:43:04.781736Z"
    }
   },
   "outputs": [],
   "source": [
    "session_length = 70\n",
    "batch_size = 512\n",
    "plot_num = 500\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T07:43:04.863356Z",
     "start_time": "2019-12-23T07:43:04.788610Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class SessionData(object):\n",
    "    def __init__(self,session_index,session_id,items_indexes):\n",
    "        self.session_index = session_index\n",
    "        self.session_id = session_id\n",
    "        self.item_list = items_indexes\n",
    "\n",
    "    def generate_seq_datas(self,session_length,padding_idx=0,predict_length=1):\n",
    "        sessions = []\n",
    "        if len(self.item_list)<2:\n",
    "            self.item_list.append[self.item_list[0]]\n",
    "        if predict_length==1:\n",
    "            # when session length>=3\n",
    "            for i in range(1,len(self.item_list)-1):\n",
    "#             # when session length >=2\n",
    "#             for i in range(len(self.item_list)-1):\n",
    "                if i <session_length:\n",
    "                    train_data = [0 for _ in range(session_length-i-1)]\n",
    "                    train_data.extend(self.item_list[:i+1])\n",
    "                    train_data.append(self.item_list[i+1])\n",
    "                else:\n",
    "                    train_data = self.item_list[i+1-session_length:i+1]\n",
    "                    train_data.append(self.item_list[i+1])\n",
    "                sessions.append(train_data)\n",
    "        else:\n",
    "\n",
    "            pass\n",
    "        return self.session_index,sessions\n",
    "    def __str__(self):\n",
    "        info = \" session index = {}\\n session id = {} \\n the length of item list= {} \\n the fisrt item index in item list is {}\".format(self.session_index,self.session_id,len(self.item_list),self.item_list[0])\n",
    "        return info\n",
    "class SessionDataSet(object):\n",
    "    def __init__(self,train_file,test_file,padding_idx=0):\n",
    "        super(SessionDataSet,self).__init__()\n",
    "        self.index_count = 0\n",
    "        self.session_count = 0\n",
    "        self.train_count = 0\n",
    "        self.test_count = 0\n",
    "        self.max_session_length = 0\n",
    "\n",
    "        self.padding_idx = padding_idx\n",
    "        self.item2index = dict()\n",
    "        self.index2item = dict()\n",
    "        self.session2index = dict()\n",
    "        self.index2session = dict()\n",
    "        self.item_total_num = dict()\n",
    "        self.item2index[\"<pad>\"] = padding_idx\n",
    "        self.index2item[padding_idx] = \"<pad>\"\n",
    "        self.train_data = self.load_data(train_file)\n",
    "        print(\"training set is loaded, # index: \",len(self.item2index.keys()))\n",
    "        self.train_count = self.session_count\n",
    "        print(\"train_session_num\",self.train_count)\n",
    "        self.test_data = self.load_data(test_file)\n",
    "        print(\"testing set is loaded, # index: \",len(self.index2item.keys()))\n",
    "        print(\"# item\",self.index_count)\n",
    "        self.test_count = self.session_count-self.train_count\n",
    "        print(\"# test session:\",self.test_count)\n",
    "        self.all_training_data = []\n",
    "        self.all_testing_data = []\n",
    "        self.all_meta_training_data = []\n",
    "        self.all_meta_testing_data = []\n",
    "        self.train_session_length = 0\n",
    "        self.test_session_length = 0\n",
    "    \n",
    "    def load_data(self,file_path):\n",
    "        data =  pickle.load(open(file_path, 'rb'))\n",
    "        session_ids = data[0]\n",
    "        session_data = data[1]\n",
    "        session_label = data[2]\n",
    "\n",
    "        result_data = []\n",
    "        lenth = len(session_ids)\n",
    "        print(\"# session\",lenth)\n",
    "\n",
    "        last_session_id = session_ids[0]\n",
    "        \n",
    "        session_item_indexes = []\n",
    "\n",
    "        for item_id in session_data[0]:\n",
    "            if item_id not in self.item2index.keys():\n",
    "                self.index_count+=1\n",
    "                self.item2index[item_id] = self.index_count\n",
    "                self.index2item[self.index_count] = item_id\n",
    "                self.item_total_num[self.index_count] = 0\n",
    "            session_item_indexes.append(self.item2index[item_id])\n",
    "            self.item_total_num[self.item2index[item_id]] += 1\n",
    "        target_item = session_label[0]\n",
    "        if target_item not in self.item2index.keys():\n",
    "            self.index_count+=1\n",
    "            self.item2index[target_item] = self.index_count\n",
    "            self.index2item[self.index_count] = target_item\n",
    "            self.item_total_num[self.index_count] = 0\n",
    "        session_item_indexes.append(self.item2index[target_item])\n",
    "        self.item_total_num[self.item2index[target_item]] += 1\n",
    "\n",
    "        for session_id,items,target_item in zip(session_ids,session_data,session_label):\n",
    "            if session_id!=last_session_id:\n",
    "\n",
    "                self.session_count+=1\n",
    "                self.session2index[last_session_id] = self.session_count\n",
    "                self.index2session[self.session_count] = last_session_id\n",
    "                if len(session_item_indexes)>self.max_session_length:\n",
    "                    self.max_session_length = len(session_item_indexes)\n",
    "                new_session = SessionData(self.session_count,last_session_id,session_item_indexes)\n",
    "                result_data.append(new_session)\n",
    "                last_session_id = session_id\n",
    "                session_item_indexes = []\n",
    "                for item_id in items:\n",
    "                    if item_id not in self.item2index.keys():\n",
    "                        self.index_count+=1\n",
    "                        self.item2index[item_id] = self.index_count\n",
    "                        self.index2item[self.index_count] = item_id\n",
    "                        self.item_total_num[self.index_count] = 0\n",
    "                    session_item_indexes.append(self.item2index[item_id])\n",
    "                    self.item_total_num[self.item2index[item_id]] += 1\n",
    "                if target_item not in self.item2index.keys():\n",
    "                    self.index_count+=1\n",
    "                    self.item2index[target_item] = self.index_count\n",
    "                    self.index2item[self.index_count] = target_item\n",
    "                    self.item_total_num[self.index_count] = 0\n",
    "                session_item_indexes.append(self.item2index[target_item])\n",
    "                self.item_total_num[self.item2index[target_item]] += 1\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        self.session_count+=1\n",
    "        self.session2index[last_session_id] = self.session_count\n",
    "        new_session = SessionData(self.session_count,last_session_id,session_item_indexes)\n",
    "        result_data.append(new_session)\n",
    "        print(\"loaded\")\n",
    "        print(new_session)\n",
    "        \n",
    "        return result_data\n",
    "    \n",
    "\n",
    "    def get_batch(self,batch_size,session_length=10,predict_length=1,all_data=None,phase=\"train\",neg_num=1,sampling_mathod=\"random\"):\n",
    "\n",
    "        if phase == \"train\":\n",
    "            if all_data is None:\n",
    "                all_data = self.get_all_training_data(session_length)\n",
    "            indexes = np.random.permutation(all_data.shape[0])\n",
    "            all_data = all_data[indexes]\n",
    "        else:\n",
    "            if all_data is None:\n",
    "                all_data = self.get_all_testing_data(session_length)\n",
    "        \n",
    "        sindex = 0\n",
    "        eindex = batch_size\n",
    "        while eindex < all_data.shape[0]:\n",
    "            batch = all_data[sindex: eindex]\n",
    "\n",
    "            temp = eindex\n",
    "            eindex = eindex + batch_size\n",
    "            sindex = temp\n",
    "            if phase ==\"train\":\n",
    "                batch = self.divid_and_extend_negative_samples(batch,session_length=session_length,predict_length=predict_length,neg_num=neg_num,method=sampling_mathod)\n",
    "            else:\n",
    "                batch = [batch[:,:session_length],batch[:,session_length:]]\n",
    "            yield batch\n",
    "\n",
    "        if eindex >= all_data.shape[0]:\n",
    "            batch = all_data[sindex:]\n",
    "            if phase ==\"train\":\n",
    "                batch = self.divid_and_extend_negative_samples(batch,session_length=session_length,predict_length=predict_length,neg_num=neg_num,method=sampling_mathod)\n",
    "            else:\n",
    "                batch = [batch[:,:session_length],batch[:,session_length:]]\n",
    "            yield batch\n",
    "    \n",
    "    def divid_and_extend_negative_samples(self,batch_data,session_length,predict_length=1,neg_num=1,method=\"random\"):\n",
    "        \"\"\"\n",
    "        divid and extend negative samples\n",
    "        \"\"\"\n",
    "        neg_items = []\n",
    "        if method == \"random\":\n",
    "            for session_and_target in batch_data:\n",
    "                neg_item = []\n",
    "                for i in range(neg_num):\n",
    "                    rand_item = random.randint(1,self.index_count)\n",
    "                    while rand_item in session_and_target or rand_item in neg_item:\n",
    "                        rand_item = random.randint(1,self.index_count)\n",
    "                    neg_item.append(rand_item)\n",
    "                neg_items.append(neg_item)\n",
    "        else:\n",
    "\n",
    "            total_list = set()\n",
    "            for session in batch_data:\n",
    "                for i in session:\n",
    "                    total_list.add(i) \n",
    "            total_list = list(total_list)\n",
    "            total_list =  sorted(total_list, key=lambda item: self.item_total_num[item],reverse=True)\n",
    "            for i,session in enumerate(batch_data):\n",
    "                np.random.choice(total_list)\n",
    "        session_items = batch_data[:,:session_length]\n",
    "        target_item = batch_data[:,session_length:]\n",
    "        neg_items = np.array(neg_items)\n",
    "        return [session_items,target_item,neg_items]\n",
    "    \n",
    "    def get_all_training_data(self,session_length,predict_length=1):\n",
    "        if len(self.all_training_data)!=0 and self.train_session_length==session_length:\n",
    "#             print(\"The build is complete and there is no need to repeat the build\")\n",
    "            return self.all_training_data\n",
    "        print(\"Start building the all training dataset\")\n",
    "        all_sessions = []\n",
    "        for session_data in self.train_data:\n",
    "            session_index,sessions = session_data.generate_seq_datas(session_length,padding_idx=self.padding_idx)\n",
    "            if sessions is not None:\n",
    "                all_sessions.extend(sessions)\n",
    "        all_sessions = np.array(all_sessions)\n",
    "        self.all_training_data = all_sessions\n",
    "        self.train_session_length=session_length\n",
    "        print(\"The total number of training samples is：\",all_sessions.shape)\n",
    "        return all_sessions\n",
    "    \n",
    "    def get_all_testing_data(self,session_length,predict_length=1):\n",
    "        if len(self.all_testing_data)!=0 and self.test_session_length==session_length:\n",
    "            return self.all_testing_data\n",
    "        all_sessions = []\n",
    "        for session_data in self.test_data:\n",
    "            session_index,sessions = session_data.generate_seq_datas(session_length,padding_idx=self.padding_idx)\n",
    "            if sessions is not None:\n",
    "                all_sessions.extend(sessions)\n",
    "        all_sessions = np.array(all_sessions)\n",
    "        self.all_testing_data = all_sessions\n",
    "        self.test_session_length=session_length\n",
    "        print(\"The total number of testing samples is：\",all_sessions.shape)\n",
    "        return all_sessions\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T07:43:06.476640Z",
     "start_time": "2019-12-23T07:43:04.864680Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# session 433648\n",
      "loaded\n",
      " session index = 126168\n",
      " session id = 946108 \n",
      " the length of item list= 3 \n",
      " the fisrt item index in item list is 3314\n",
      "training set is loaded, # index:  36969\n",
      "train_session_num 126168\n",
      "# session 15132\n",
      "loaded\n",
      " session index = 130903\n",
      " session id = 1582915 \n",
      " the length of item list= 6 \n",
      " the fisrt item index in item list is 12498\n",
      "testing set is loaded, # index:  36969\n",
      "# item 36968\n",
      "# test session: 4735\n"
     ]
    }
   ],
   "source": [
    "# dataset = SessionDataSet(train_file=\"../../data/retailrocket_gcsan_my/train.txt\",test_file=\"../../data/srgnn/retailrocket_gcsan_my/test.txt\")\n",
    "dataset = SessionDataSet(train_file=\"../../data/diginetica_gcsan_my/train.txt\",test_file=\"../../data/srgnn/diginetica_gcsan_my/test.txt\")\n",
    "# dataset = SessionDataSet(train_file=\"../../data/yoochoose1_4_gcsan_my/train.txt\",test_file=\"../../data/srgnn/yoochoose1_4_gcsan_my/test.txt\")\n",
    "# dataset = SessionDataSet(train_file=\"../../data/yoochoose1_64_gcsan_my/train.txt\",test_file=\"../../data/srgnn/yoochoose1_64_gcsan_my/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T07:43:06.486426Z",
     "start_time": "2019-12-23T07:43:06.478017Z"
    }
   },
   "outputs": [],
   "source": [
    "def bpr_loss(r):\n",
    "    return torch.sum(-torch.log(torch.sigmoid(r)))\n",
    "def get_hit_num(pred,y_truth):\n",
    "    \"\"\"\n",
    "        pred: numpy type(batch_size,k) \n",
    "        y_truth: list type (batch_size,groudtruth_num)\n",
    "    \"\"\"\n",
    "\n",
    "    hit_num = 0\n",
    "    for i in range(len(y_truth)):\n",
    "        for value in y_truth[i]:\n",
    "            hit_num += np.sum(pred[i]==value)\n",
    "    return hit_num\n",
    "\n",
    "def get_rr(pred,y_truth):\n",
    "    rr=0.\n",
    "    for i in range(len(y_truth)):\n",
    "        for value in y_truth[i]:\n",
    "            hit_indexes = np.where(pred[i]==value)[0]\n",
    "            for hit_index in hit_indexes:\n",
    "                rr += 1/(hit_index+1)\n",
    "    return rr\n",
    "\n",
    "def get_dcg(pred,y_truth):\n",
    "    y_pred_score = np.zeros_like(pred)\n",
    "\n",
    "    for i in range(len(y_truth)):\n",
    "\n",
    "        for j,y_pred in enumerate(pred[i]):\n",
    "            if y_pred == y_truth[i][0]:\n",
    "                y_pred_score[i][j]=1\n",
    "    gain = 2 ** y_pred_score - 1\n",
    "    discounts = np.tile(np.log2(np.arange(pred.shape[1]) + 2),(len(y_truth),1))\n",
    "    dcg = np.sum(gain / discounts,axis=1)\n",
    "    return dcg\n",
    "\n",
    "def get_ndcg(pred,y_truth):\n",
    "    dcg = get_dcg(pred, y_truth)\n",
    "    idcg = get_dcg(np.concatenate((y_truth,np.zeros_like(pred)[:,:-1]-1),axis=1), y_truth)\n",
    "    ndcg = np.sum(dcg / idcg)\n",
    "\n",
    "    return ndcg\n",
    "\n",
    "def dcg_score(y_pre, y_true, k):\n",
    "    y_pre_score = np.zeros(k)\n",
    "    if len(y_pre) > k:\n",
    "        y_pre = y_pre[:k]\n",
    "    for i in range(len(y_pre)):\n",
    "        pre_tag = y_pre[i]\n",
    "        if pre_tag in y_true:\n",
    "            y_pre_score[i] = 1\n",
    "    gain = 2 ** y_pre_score - 1\n",
    "    discounts = np.log2(np.arange(k) + 2)\n",
    "    return np.sum(gain / discounts)\n",
    "\n",
    "\n",
    "def ndcg_score(y_pre, y_true, k=5):\n",
    "    dcg = dcg_score(y_pre, y_true, k)\n",
    "    idcg = dcg_score(y_true, y_true, k)\n",
    "    return dcg / idcg\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T07:43:06.504614Z",
     "start_time": "2019-12-23T07:43:06.487394Z"
    }
   },
   "outputs": [],
   "source": [
    "# Attention Layer\n",
    "class Attention(torch.nn.Module):\n",
    "    def __init__(self, method=\"specific\", hidden_size=64):\n",
    "        super(Attention, self).__init__()\n",
    "        self.config = list()\n",
    "        self.method = method\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        if self.method == \"dot\":\n",
    "        \n",
    "            self.query = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "            self.key = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "\n",
    "        elif self.method == \"general\":\n",
    "            self.attention = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "\n",
    "        elif self.method == \"concat\":\n",
    "            self.attention = torch.nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "            self.v = torch.nn.Parameter(torch.FloatTensor(self.hidden_size))\n",
    "            \n",
    "        elif self.method == \"specific\":\n",
    "            self.W0 = torch.nn.Linear(self.hidden_size,1,bias=False)\n",
    "            torch.nn.init.normal_(self.W0.weight,0,0.05)\n",
    "            self.W1 = torch.nn.Linear(self.hidden_size,self.hidden_size,bias=False)\n",
    "            torch.nn.init.normal_(self.W1.weight,0,0.05)\n",
    "            self.W2 = torch.nn.Linear(self.hidden_size,self.hidden_size,bias=False)\n",
    "            torch.nn.init.normal_(self.W2.weight,0,0.05)\n",
    "            self.W3 = torch.nn.Linear(self.hidden_size,self.hidden_size,bias=False)\n",
    "            torch.nn.init.normal_(self.W3.weight,0,0.05)\n",
    "            self.b = torch.nn.Parameter(torch.FloatTensor(self.hidden_size))\n",
    "\n",
    "                               \n",
    "            \n",
    "\n",
    "    def dot_score(self, hidden, encoder_output,weights=None):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "\n",
    "    def general_score(self, hidden, encoder_output,weights=None):\n",
    "        energy = self.attention(encoder_output)\n",
    "        return torch.sum(hidden * energy, dim=2)\n",
    "\n",
    "\n",
    "    def concat_score(self, hidden, encoder_output,weights=None):\n",
    "        energy = self.attention(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
    "        return torch.sum(self.v * energy, dim=2)\n",
    "    \n",
    "\n",
    "    def specific_score(self,session,x_t,m_s,mask=None,weights=None):\n",
    "        if weights is None:\n",
    "            if mask is None:\n",
    "                W1Xi = self.W1(session)\n",
    "                W2Xt = self.W2(x_t).unsqueeze(1).repeat((1,session.shape[1],1))\n",
    "                W3Ms = self.W3(m_s).unsqueeze(1).repeat((1,session.shape[1],1))\n",
    "                energy = self.W0(torch.sigmoid(W1Xi+W2Xt+W3Ms+self.b)).repeat((1,1,session.shape[2]))\n",
    "    #         energy = energy*mask\n",
    "            else:\n",
    "    #         print(session.shape,x_t.shape,m_s.shape)\n",
    "                W1Xi = self.W1(session)*mask\n",
    "                W2Xt = self.W2(x_t).unsqueeze(1).repeat((1,session.shape[1],1))*mask\n",
    "                W3Ms = self.W3(m_s).unsqueeze(1).repeat((1,session.shape[1],1))*mask\n",
    "                energy = self.W0(torch.sigmoid(W1Xi+W2Xt+W3Ms+self.b)).repeat((1,1,session.shape[2]))*mask\n",
    "    #         energy = energy*mask\n",
    "        else:\n",
    "            key = 1\n",
    "            if mask is None:\n",
    "                W1Xi = torch.matmul(session,weights[key].t())\n",
    "                key +=1\n",
    "                W2Xt = (torch.matmul(x_t,weights[key].t())).unsqueeze(1).repeat((1,session.shape[1],1))\n",
    "                key +=1\n",
    "                W3Ms = (torch.matmul(x_t,weights[key].t())).unsqueeze(1).repeat((1,session.shape[1],1))\n",
    "                energy = torch.matmul(torch.sigmoid(W1Xi+W2Xt+W3Ms+weights[key+1]),weights[0].t()).repeat((1,1,session.shape[2]))\n",
    "    #         energy = energy*mask\n",
    "            else:\n",
    "    #         print(session.shape,x_t.shape,m_s.shape)\n",
    "                W1Xi = torch.matmul(session,weights[key].t())*mask\n",
    "                key +=1\n",
    "                W2Xt = (torch.matmul(x_t,weights[key].t())).unsqueeze(1).repeat((1,session.shape[1],1))*mask\n",
    "                key +=1\n",
    "                W3Ms = (torch.matmul(x_t,weights[key].t())).unsqueeze(1).repeat((1,session.shape[1],1))*mask\n",
    "                energy = torch.matmul(torch.sigmoid(W1Xi+W2Xt+W3Ms+weights[key+1]),weights[0].t()).repeat((1,1,session.shape[2]))*mask\n",
    "        return torch.sum(energy*session,dim=1)\n",
    "            \n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs=None,x_t=None,mask=None):\n",
    "\n",
    "        if self.method == \"general\":\n",
    "            attention_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == \"concat\":\n",
    "            attention_energies = self.concat_score(hidden, encoder_outputs)\n",
    "        elif self.method == \"dot\":\n",
    "            attention_energies = self.dot_score(hidden, encoder_outputs)\n",
    "        elif self.method == \"specific\":\n",
    "            session = hidden\n",
    "            m_s = encoder_outputs\n",
    "            return self.specific_score(session,x_t,m_s,mask)\n",
    "\n",
    "        attention_energies = attention_energies.t()\n",
    "\n",
    "        return F.softmax(attention_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T07:43:06.516883Z",
     "start_time": "2019-12-23T07:43:06.506066Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class STAMP(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=64, itemNum=0, posNum=0, padding_idx=0, dropout=0.5,\n",
    "                 activate=\"relu\"):\n",
    "        super(STAMP, self).__init__()\n",
    "        self.padding_idx = padding_idx\n",
    "        self.hidden_size = hidden_size\n",
    "        if activate == \"sigmoid\":\n",
    "            self.activate = torch.sigmoid\n",
    "        elif activate == \"tanh\":\n",
    "            self.activate = torch.tanh\n",
    "        else:\n",
    "            self.activate = torch.relu\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.item_embedding = torch.nn.Embedding(itemNum, hidden_size, padding_idx=self.padding_idx,max_norm=1.5)\n",
    "        torch.nn.init.normal_(self.item_embedding.weight,0,0.002)\n",
    "        torch.nn.init.constant_(self.item_embedding.weight[0],0)\n",
    "        self.attention = Attention(method=\"specific\",hidden_size=hidden_size)\n",
    "        self.left_mlp1 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        torch.nn.init.normal_(self.left_mlp1.weight,0,0.05)\n",
    "        torch.nn.init.constant_(self.left_mlp1.bias,0)\n",
    "        self.right_mlp1 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        torch.nn.init.normal_(self.left_mlp1.weight,0,0.05)\n",
    "        torch.nn.init.constant_(self.right_mlp1.bias,0)\n",
    "        torch.nn.utils.clip_grad_norm_(self.parameters(),max_norm=110)\n",
    "        \n",
    "    def forward(self, session):\n",
    " \n",
    "        mask = (session!=self.padding_idx).float()\n",
    "\n",
    "        length = torch.sum(mask,1).unsqueeze(1).repeat((1,self.hidden_size))\n",
    "\n",
    "        mask = mask.unsqueeze(2).repeat((1,1,self.hidden_size))\n",
    "        session_item_vecs = self.item_embedding(session) * mask\n",
    "        mean_session = torch.sum(session_item_vecs, dim=1)/length\n",
    "\n",
    "        compute_output = self.attention(session_item_vecs,mean_session,session_item_vecs[:,-1])\n",
    "        left_output = self.dropout(self.activate(self.left_mlp1(compute_output)))\n",
    "        right_output = self.dropout(self.activate(self.right_mlp1(session_item_vecs[:,-1])))\n",
    "\n",
    "        result = torch.matmul(left_output* right_output,self.item_embedding.weight[1:].t())\n",
    "\n",
    "        return result\n",
    "\n",
    "    def predict_top_k(self, session, k=20):\n",
    "\n",
    "        mask = (session!=0).float()\n",
    "\n",
    "        length = torch.sum(mask,1).unsqueeze(1).repeat((1,self.hidden_size))\n",
    "\n",
    "        mask = mask.unsqueeze(2).repeat((1,1,self.hidden_size))\n",
    "        session_item_vecs = self.item_embedding(session) * mask\n",
    "        mean_session = torch.sum(session_item_vecs, dim=1)/length\n",
    "        compute_output = self.attention(session_item_vecs,mean_session,session_item_vecs[:,-1])\n",
    "        left_output =self.activate(self.left_mlp1(compute_output))\n",
    "\n",
    "        right_output = self.activate(self.right_mlp1(session_item_vecs[:,-1]))\n",
    "\n",
    "        result = torch.matmul(left_output * right_output,self.item_embedding.weight[1:].t())\n",
    "\n",
    "        result = torch.topk(result,k,dim=1)[1]\n",
    "        return result\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIKM S >= 3   \n",
    "    HR@20=0.62734  MRR@20=0.28766, session_length=20, hidden_size=100, lr=0.0030, dropout=0.00\n",
    "        HR@1=0.17622  MRR@1=0.17622  NDCG@1=0.17622\n",
    "        HR@5=0.42020  MRR@5=0.26644  NDCG@5=0.30474\n",
    "        HR@10=0.52549  MRR@10=0.28054  NDCG@10=0.33883\n",
    "        HR@20=0.62734  MRR@20=0.28766  NDCG@20=0.36465\n",
    "# RR S >= 3   \n",
    "    HR@20=0.50515  MRR@20=0.24373, session_length=20, hidden_size=100, lr=0.0030, dropout=0.00\n",
    "        HR@1=0.16316  MRR@1=0.16316  NDCG@1=0.16316\n",
    "        HR@5=0.34159  MRR@5=0.22700  NDCG@5=0.25543\n",
    "        HR@10=0.42625  MRR@10=0.23831  NDCG@10=0.28282\n",
    "        HR@20=0.50515  MRR@20=0.24373  NDCG@20=0.30270\n",
    "# RSC64 S >= 3   \n",
    "    HR@20=0.68598  MRR@20=0.28300, session_length=20, hidden_size=100, lr=0.0030, dropout=0.30\n",
    "        HR@1=0.15628  MRR@1=0.15628  NDCG@1=0.15628\n",
    "        HR@5=0.43936  MRR@5=0.25709  NDCG@5=0.30230\n",
    "        HR@10=0.57278  MRR@10=0.27502  NDCG@10=0.34557\n",
    "        HR@20=0.68598  MRR@20=0.28300  NDCG@20=0.37435\n",
    "\n",
    "# RSC4 S >= 3   \n",
    "    HR@20=0.70622  MRR@20=0.28808, session_length=20, hidden_size=100, lr=0.0010, dropout=0.00\n",
    "        HR@1=0.15651  MRR@1=0.15651  NDCG@1=0.15651\n",
    "        HR@5=0.44922  MRR@5=0.26098  NDCG@5=0.30766\n",
    "        HR@10=0.59014  MRR@10=0.27991  NDCG@10=0.35335\n",
    "        HR@20=0.70622  MRR@20=0.28808  NDCG@20=0.38284"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T07:43:06.543722Z",
     "start_time": "2019-12-23T07:43:06.518194Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs=50\n",
    "def train(args):\n",
    "    hidden_size = args[\"hidden_size\"] if \"hidden_size\" in args.keys() else 100\n",
    "    dropout = args[\"dropout\"] if \"dropout\" in args.keys()  else 0.5\n",
    "    lr = args[\"lr\"] if \"lr\" in args.keys()  else 3e-3\n",
    "    session_length = args[\"session_length\"] if \"session_length\" in args.keys() else 20\n",
    "    model = STAMP(hidden_size=hidden_size, itemNum=dataset.index_count+1, posNum=11, padding_idx=0, dropout=0,\n",
    "                 activate=\"tanh\").to(device)\n",
    "    opti = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "    best_model_hr = 0.0\n",
    "    best_model_mrr = 0.0\n",
    "    best_r1m = 0.0\n",
    "    best_model = None\n",
    "    first_loss = 0.0\n",
    "    predict_nums = [1,5,10,20]\n",
    "    for epoch in range(epochs):\n",
    "        batch_losses = []\n",
    "        epoch_losses = []\n",
    "        for i,batch_data in enumerate(dataset.get_batch(batch_size,session_length,phase=\"train\")):\n",
    "            sessions = torch.tensor(batch_data[0]).to(device)\n",
    "            target_items = torch.tensor(batch_data[1]).squeeze().to(device)-1\n",
    "            result_pos = model(sessions)\n",
    "            loss = loss_function(result_pos,target_items)\n",
    "            opti.zero_grad()\n",
    "            loss.backward()\n",
    "            norm = torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm=110)\n",
    "#             print(norm)\n",
    "            opti.step()\n",
    "            batch_losses.append(loss.cpu().detach().numpy())\n",
    "            epoch_losses.append(loss.cpu().detach().numpy())\n",
    "            if i % plot_num == 0:\n",
    "                time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                print(\"[%s] [%d/%d] %d mean_batch_loss : %0.6f\" % (time, epoch+1, epochs, i, np.mean(batch_losses)))\n",
    "                batch_losses = []\n",
    "        with torch.no_grad():\n",
    "            start_test_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(\"Start predicting\",start_test_time)\n",
    "            rrs = [0 for _ in range(len(predict_nums))]\n",
    "            hit_nums = [0 for _ in range(len(predict_nums))]\n",
    "            ndcgs = [0 for _ in range(len(predict_nums))]\n",
    "            for i,batch_data in enumerate(dataset.get_batch(batch_size,session_length,phase=\"test\")):\n",
    "                sessions = torch.tensor(batch_data[0]).to(device)\n",
    "                target_items = np.array(batch_data[1])-1\n",
    "                y_pred = model.predict_top_k(sessions,20).cpu().numpy()\n",
    "#                 print(y_pred[:2],target_items[:2])\n",
    "                for j,predict_num in enumerate(predict_nums):\n",
    "                    hit_nums[j]+=get_hit_num(y_pred[:,:predict_num],target_items)\n",
    "                    rrs[j]+=get_rr(y_pred[:,:predict_num],target_items)\n",
    "                    ndcgs[j]+=get_ndcg(y_pred[:,:predict_num],target_items)\n",
    "                    \n",
    "            end_test_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            hrs = [hit_num/len(dataset.all_testing_data) for hit_num in hit_nums]\n",
    "            mrrs = [rr/len(dataset.all_testing_data) for rr in rrs]\n",
    "            mndcgs = [ndcg/len(dataset.all_testing_data) for ndcg in ndcgs]\n",
    "            if hrs[-1] + mrrs[-1] > best_r1m:\n",
    "                print(\"change best\")\n",
    "                best_model = deepcopy(model)\n",
    "                best_model_hr = hrs[-1]\n",
    "                best_model_mrr = mrrs[-1]\n",
    "                best_r1m = hrs[-1] + mrrs[-1]\n",
    "                no_improvement_epoch = 0\n",
    "            else:\n",
    "                no_improvement_epoch +=1\n",
    "            print(\"testing finish [%s] \"%end_test_time)\n",
    "            for k,predict_num in enumerate(predict_nums):\n",
    "                print(\"\\tHR@%d=%.5f  MRR@%d=%.5f  NDCG@%d=%.5f\"%(predict_num,hrs[k],predict_num,mrrs[k],predict_num,mndcgs[k]))\n",
    "        if no_improvement_epoch>=patience:\n",
    "            print(\"early stopping\")\n",
    "            break\n",
    "    return best_model,best_model_hr,best_model_mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T08:58:06.678224Z",
     "start_time": "2019-12-23T07:43:06.545092Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current model hyper-parameters: session_length=20, hidden_size=100, lr=0.0030, dropout=0.00\n",
      "\n",
      "Start building the all training dataset\n",
      "The total number of training samples is： (433648, 21)\n",
      "[2019-12-23 15:43:16] [1/50] 0 mean_batch_loss : 10.517811\n",
      "[2019-12-23 15:43:39] [1/50] 500 mean_batch_loss : 9.589497\n",
      "Start predicting 2019-12-23 15:43:59\n",
      "The total number of testing samples is： (15132, 21)\n",
      "change best\n",
      "testing finish [2019-12-23 15:44:01] \n",
      "\tHR@1=0.05967  MRR@1=0.05967  NDCG@1=0.05967\n",
      "\tHR@5=0.14876  MRR@5=0.09109  NDCG@5=0.10536\n",
      "\tHR@10=0.19859  MRR@10=0.09774  NDCG@10=0.12147\n",
      "\tHR@20=0.25264  MRR@20=0.10145  NDCG@20=0.13510\n",
      "[2019-12-23 15:44:01] [2/50] 0 mean_batch_loss : 6.657148\n",
      "[2019-12-23 15:44:33] [2/50] 500 mean_batch_loss : 6.313400\n",
      "Start predicting 2019-12-23 15:44:54\n",
      "change best\n",
      "testing finish [2019-12-23 15:44:55] \n",
      "\tHR@1=0.11439  MRR@1=0.11439  NDCG@1=0.11439\n",
      "\tHR@5=0.26004  MRR@5=0.16586  NDCG@5=0.18917\n",
      "\tHR@10=0.33135  MRR@10=0.17535  NDCG@10=0.21220\n",
      "\tHR@20=0.40404  MRR@20=0.18045  NDCG@20=0.23065\n",
      "[2019-12-23 15:44:55] [3/50] 0 mean_batch_loss : 5.165871\n",
      "[2019-12-23 15:45:28] [3/50] 500 mean_batch_loss : 5.046360\n",
      "Start predicting 2019-12-23 15:45:49\n",
      "change best\n",
      "testing finish [2019-12-23 15:45:50] \n",
      "\tHR@1=0.13501  MRR@1=0.13501  NDCG@1=0.13501\n",
      "\tHR@5=0.29401  MRR@5=0.19307  NDCG@5=0.21817\n",
      "\tHR@10=0.37160  MRR@10=0.20345  NDCG@10=0.24328\n",
      "\tHR@20=0.45103  MRR@20=0.20897  NDCG@20=0.26338\n",
      "[2019-12-23 15:45:50] [4/50] 0 mean_batch_loss : 4.340450\n",
      "[2019-12-23 15:46:23] [4/50] 500 mean_batch_loss : 4.591411\n",
      "Start predicting 2019-12-23 15:46:45\n",
      "change best\n",
      "testing finish [2019-12-23 15:46:46] \n",
      "\tHR@1=0.14070  MRR@1=0.14070  NDCG@1=0.14070\n",
      "\tHR@5=0.30313  MRR@5=0.20037  NDCG@5=0.22596\n",
      "\tHR@10=0.38547  MRR@10=0.21138  NDCG@10=0.25261\n",
      "\tHR@20=0.46055  MRR@20=0.21666  NDCG@20=0.27168\n",
      "[2019-12-23 15:46:46] [5/50] 0 mean_batch_loss : 4.225935\n",
      "[2019-12-23 15:47:19] [5/50] 500 mean_batch_loss : 4.368682\n",
      "Start predicting 2019-12-23 15:47:41\n",
      "change best\n",
      "testing finish [2019-12-23 15:47:42] \n",
      "\tHR@1=0.14592  MRR@1=0.14592  NDCG@1=0.14592\n",
      "\tHR@5=0.31734  MRR@5=0.20802  NDCG@5=0.23517\n",
      "\tHR@10=0.39512  MRR@10=0.21843  NDCG@10=0.26036\n",
      "\tHR@20=0.46795  MRR@20=0.22351  NDCG@20=0.27880\n",
      "[2019-12-23 15:47:42] [6/50] 0 mean_batch_loss : 3.865406\n",
      "[2019-12-23 15:48:15] [6/50] 500 mean_batch_loss : 4.239305\n",
      "Start predicting 2019-12-23 15:48:35\n",
      "change best\n",
      "testing finish [2019-12-23 15:48:37] \n",
      "\tHR@1=0.14512  MRR@1=0.14512  NDCG@1=0.14512\n",
      "\tHR@5=0.32600  MRR@5=0.21093  NDCG@5=0.23954\n",
      "\tHR@10=0.40054  MRR@10=0.22086  NDCG@10=0.26363\n",
      "\tHR@20=0.47753  MRR@20=0.22620  NDCG@20=0.28308\n",
      "[2019-12-23 15:48:37] [7/50] 0 mean_batch_loss : 3.924218\n",
      "[2019-12-23 15:49:09] [7/50] 500 mean_batch_loss : 4.143859\n",
      "Start predicting 2019-12-23 15:49:31\n",
      "change best\n",
      "testing finish [2019-12-23 15:49:33] \n",
      "\tHR@1=0.15358  MRR@1=0.15358  NDCG@1=0.15358\n",
      "\tHR@5=0.32639  MRR@5=0.21580  NDCG@5=0.24325\n",
      "\tHR@10=0.40609  MRR@10=0.22652  NDCG@10=0.26911\n",
      "\tHR@20=0.47720  MRR@20=0.23151  NDCG@20=0.28716\n",
      "[2019-12-23 15:49:33] [8/50] 0 mean_batch_loss : 3.825764\n",
      "[2019-12-23 15:50:05] [8/50] 500 mean_batch_loss : 4.074640\n",
      "Start predicting 2019-12-23 15:50:27\n",
      "change best\n",
      "testing finish [2019-12-23 15:50:29] \n",
      "\tHR@1=0.15200  MRR@1=0.15200  NDCG@1=0.15200\n",
      "\tHR@5=0.32891  MRR@5=0.21613  NDCG@5=0.24414\n",
      "\tHR@10=0.40841  MRR@10=0.22681  NDCG@10=0.26992\n",
      "\tHR@20=0.48064  MRR@20=0.23184  NDCG@20=0.28822\n",
      "[2019-12-23 15:50:29] [9/50] 0 mean_batch_loss : 3.805151\n",
      "[2019-12-23 15:51:00] [9/50] 500 mean_batch_loss : 4.021628\n",
      "Start predicting 2019-12-23 15:51:22\n",
      "change best\n",
      "testing finish [2019-12-23 15:51:23] \n",
      "\tHR@1=0.15722  MRR@1=0.15722  NDCG@1=0.15722\n",
      "\tHR@5=0.32811  MRR@5=0.22038  NDCG@5=0.24721\n",
      "\tHR@10=0.40741  MRR@10=0.23102  NDCG@10=0.27291\n",
      "\tHR@20=0.48473  MRR@20=0.23639  NDCG@20=0.29247\n",
      "[2019-12-23 15:51:23] [10/50] 0 mean_batch_loss : 3.857992\n",
      "[2019-12-23 15:51:54] [10/50] 500 mean_batch_loss : 3.977085\n",
      "Start predicting 2019-12-23 15:52:16\n",
      "testing finish [2019-12-23 15:52:18] \n",
      "\tHR@1=0.15735  MRR@1=0.15735  NDCG@1=0.15735\n",
      "\tHR@5=0.33314  MRR@5=0.22147  NDCG@5=0.24923\n",
      "\tHR@10=0.40933  MRR@10=0.23172  NDCG@10=0.27395\n",
      "\tHR@20=0.48288  MRR@20=0.23691  NDCG@20=0.29265\n",
      "[2019-12-23 15:52:18] [11/50] 0 mean_batch_loss : 3.750574\n",
      "[2019-12-23 15:52:49] [11/50] 500 mean_batch_loss : 3.935761\n",
      "Start predicting 2019-12-23 15:53:10\n",
      "change best\n",
      "testing finish [2019-12-23 15:53:11] \n",
      "\tHR@1=0.15708  MRR@1=0.15708  NDCG@1=0.15708\n",
      "\tHR@5=0.33056  MRR@5=0.22039  NDCG@5=0.24778\n",
      "\tHR@10=0.41052  MRR@10=0.23111  NDCG@10=0.27369\n",
      "\tHR@20=0.48685  MRR@20=0.23643  NDCG@20=0.29302\n",
      "[2019-12-23 15:53:11] [12/50] 0 mean_batch_loss : 3.576113\n",
      "[2019-12-23 15:53:43] [12/50] 500 mean_batch_loss : 3.902329\n",
      "Start predicting 2019-12-23 15:54:04\n",
      "change best\n",
      "testing finish [2019-12-23 15:54:06] \n",
      "\tHR@1=0.15880  MRR@1=0.15880  NDCG@1=0.15880\n",
      "\tHR@5=0.33155  MRR@5=0.22128  NDCG@5=0.24868\n",
      "\tHR@10=0.40920  MRR@10=0.23164  NDCG@10=0.27378\n",
      "\tHR@20=0.48725  MRR@20=0.23711  NDCG@20=0.29358\n",
      "[2019-12-23 15:54:06] [13/50] 0 mean_batch_loss : 3.779014\n",
      "[2019-12-23 15:54:38] [13/50] 500 mean_batch_loss : 3.878234\n",
      "Start predicting 2019-12-23 15:54:59\n",
      "testing finish [2019-12-23 15:55:01] \n",
      "\tHR@1=0.15623  MRR@1=0.15623  NDCG@1=0.15623\n",
      "\tHR@5=0.33228  MRR@5=0.22003  NDCG@5=0.24792\n",
      "\tHR@10=0.41336  MRR@10=0.23091  NDCG@10=0.27420\n",
      "\tHR@20=0.48804  MRR@20=0.23612  NDCG@20=0.29312\n",
      "[2019-12-23 15:55:01] [14/50] 0 mean_batch_loss : 3.605918\n",
      "[2019-12-23 15:55:34] [14/50] 500 mean_batch_loss : 3.846968\n",
      "Start predicting 2019-12-23 15:55:55\n",
      "change best\n",
      "testing finish [2019-12-23 15:55:56] \n",
      "\tHR@1=0.15933  MRR@1=0.15933  NDCG@1=0.15933\n",
      "\tHR@5=0.33499  MRR@5=0.22324  NDCG@5=0.25102\n",
      "\tHR@10=0.41138  MRR@10=0.23335  NDCG@10=0.27565\n",
      "\tHR@20=0.48910  MRR@20=0.23874  NDCG@20=0.29528\n",
      "[2019-12-23 15:55:56] [15/50] 0 mean_batch_loss : 3.657718\n",
      "[2019-12-23 15:56:28] [15/50] 500 mean_batch_loss : 3.824385\n",
      "Start predicting 2019-12-23 15:56:49\n",
      "testing finish [2019-12-23 15:56:51] \n",
      "\tHR@1=0.15788  MRR@1=0.15788  NDCG@1=0.15788\n",
      "\tHR@5=0.32983  MRR@5=0.22024  NDCG@5=0.24748\n",
      "\tHR@10=0.41277  MRR@10=0.23145  NDCG@10=0.27444\n",
      "\tHR@20=0.48817  MRR@20=0.23671  NDCG@20=0.29353\n",
      "[2019-12-23 15:56:51] [16/50] 0 mean_batch_loss : 3.525068\n",
      "[2019-12-23 15:57:20] [16/50] 500 mean_batch_loss : 3.805443\n",
      "Start predicting 2019-12-23 15:57:40\n",
      "testing finish [2019-12-23 15:57:41] \n",
      "\tHR@1=0.16026  MRR@1=0.16026  NDCG@1=0.16026\n",
      "\tHR@5=0.32976  MRR@5=0.22201  NDCG@5=0.24880\n",
      "\tHR@10=0.40933  MRR@10=0.23270  NDCG@10=0.27460\n",
      "\tHR@20=0.48573  MRR@20=0.23800  NDCG@20=0.29391\n",
      "[2019-12-23 15:57:41] [17/50] 0 mean_batch_loss : 3.661749\n",
      "[2019-12-23 15:58:11] [17/50] 500 mean_batch_loss : 3.792287\n",
      "Start predicting 2019-12-23 15:58:32\n",
      "testing finish [2019-12-23 15:58:33] \n",
      "\tHR@1=0.15768  MRR@1=0.15768  NDCG@1=0.15768\n",
      "\tHR@5=0.33419  MRR@5=0.22183  NDCG@5=0.24976\n",
      "\tHR@10=0.41164  MRR@10=0.23218  NDCG@10=0.27481\n",
      "\tHR@20=0.48804  MRR@20=0.23751  NDCG@20=0.29416\n",
      "[2019-12-23 15:58:33] [18/50] 0 mean_batch_loss : 3.374840\n",
      "[2019-12-23 15:59:07] [18/50] 500 mean_batch_loss : 3.775352\n",
      "Start predicting 2019-12-23 15:59:27\n",
      "change best\n",
      "testing finish [2019-12-23 15:59:29] \n",
      "\tHR@1=0.16184  MRR@1=0.16184  NDCG@1=0.16184\n",
      "\tHR@5=0.33472  MRR@5=0.22485  NDCG@5=0.25216\n",
      "\tHR@10=0.41501  MRR@10=0.23566  NDCG@10=0.27823\n",
      "\tHR@20=0.48725  MRR@20=0.24067  NDCG@20=0.29649\n",
      "[2019-12-23 15:59:29] [19/50] 0 mean_batch_loss : 3.393895\n",
      "[2019-12-23 15:59:58] [19/50] 500 mean_batch_loss : 3.761901\n",
      "Start predicting 2019-12-23 16:00:20\n",
      "change best\n",
      "testing finish [2019-12-23 16:00:22] \n",
      "\tHR@1=0.15893  MRR@1=0.15893  NDCG@1=0.15893\n",
      "\tHR@5=0.33452  MRR@5=0.22350  NDCG@5=0.25113\n",
      "\tHR@10=0.41462  MRR@10=0.23428  NDCG@10=0.27712\n",
      "\tHR@20=0.49339  MRR@20=0.23976  NDCG@20=0.29706\n",
      "[2019-12-23 16:00:22] [20/50] 0 mean_batch_loss : 3.455323\n",
      "[2019-12-23 16:00:51] [20/50] 500 mean_batch_loss : 3.744373\n",
      "Start predicting 2019-12-23 16:01:11\n",
      "testing finish [2019-12-23 16:01:13] \n",
      "\tHR@1=0.15609  MRR@1=0.15609  NDCG@1=0.15609\n",
      "\tHR@5=0.33518  MRR@5=0.22099  NDCG@5=0.24936\n",
      "\tHR@10=0.41462  MRR@10=0.23166  NDCG@10=0.27513\n",
      "\tHR@20=0.48949  MRR@20=0.23686  NDCG@20=0.29406\n",
      "[2019-12-23 16:01:13] [21/50] 0 mean_batch_loss : 3.591885\n",
      "[2019-12-23 16:01:46] [21/50] 500 mean_batch_loss : 3.737971\n",
      "Start predicting 2019-12-23 16:02:07\n",
      "testing finish [2019-12-23 16:02:09] \n",
      "\tHR@1=0.15907  MRR@1=0.15907  NDCG@1=0.15907\n",
      "\tHR@5=0.33439  MRR@5=0.22262  NDCG@5=0.25038\n",
      "\tHR@10=0.41389  MRR@10=0.23330  NDCG@10=0.27616\n",
      "\tHR@20=0.48916  MRR@20=0.23856  NDCG@20=0.29524\n",
      "[2019-12-23 16:02:09] [22/50] 0 mean_batch_loss : 3.531067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-12-23 16:02:41] [22/50] 500 mean_batch_loss : 3.724159\n",
      "Start predicting 2019-12-23 16:03:04\n",
      "testing finish [2019-12-23 16:03:05] \n",
      "\tHR@1=0.15715  MRR@1=0.15715  NDCG@1=0.15715\n",
      "\tHR@5=0.33637  MRR@5=0.22199  NDCG@5=0.25040\n",
      "\tHR@10=0.41310  MRR@10=0.23223  NDCG@10=0.27521\n",
      "\tHR@20=0.48949  MRR@20=0.23755  NDCG@20=0.29456\n",
      "[2019-12-23 16:03:05] [23/50] 0 mean_batch_loss : 3.528182\n",
      "[2019-12-23 16:03:37] [23/50] 500 mean_batch_loss : 3.713212\n",
      "Start predicting 2019-12-23 16:03:59\n",
      "testing finish [2019-12-23 16:04:00] \n",
      "\tHR@1=0.15437  MRR@1=0.15437  NDCG@1=0.15437\n",
      "\tHR@5=0.33386  MRR@5=0.21969  NDCG@5=0.24808\n",
      "\tHR@10=0.41488  MRR@10=0.23068  NDCG@10=0.27445\n",
      "\tHR@20=0.49002  MRR@20=0.23594  NDCG@20=0.29350\n",
      "[2019-12-23 16:04:00] [24/50] 0 mean_batch_loss : 3.359228\n",
      "[2019-12-23 16:04:32] [24/50] 500 mean_batch_loss : 3.707527\n",
      "Start predicting 2019-12-23 16:04:54\n",
      "testing finish [2019-12-23 16:04:56] \n",
      "\tHR@1=0.15437  MRR@1=0.15437  NDCG@1=0.15437\n",
      "\tHR@5=0.33518  MRR@5=0.22101  NDCG@5=0.24943\n",
      "\tHR@10=0.41587  MRR@10=0.23187  NDCG@10=0.27562\n",
      "\tHR@20=0.49154  MRR@20=0.23714  NDCG@20=0.29477\n",
      "[2019-12-23 16:04:56] [25/50] 0 mean_batch_loss : 3.431666\n",
      "[2019-12-23 16:05:28] [25/50] 500 mean_batch_loss : 3.701595\n",
      "Start predicting 2019-12-23 16:05:50\n",
      "testing finish [2019-12-23 16:05:52] \n",
      "\tHR@1=0.15841  MRR@1=0.15841  NDCG@1=0.15841\n",
      "\tHR@5=0.33703  MRR@5=0.22319  NDCG@5=0.25148\n",
      "\tHR@10=0.41356  MRR@10=0.23333  NDCG@10=0.27615\n",
      "\tHR@20=0.49161  MRR@20=0.23876  NDCG@20=0.29590\n",
      "[2019-12-23 16:05:52] [26/50] 0 mean_batch_loss : 3.545569\n",
      "[2019-12-23 16:06:24] [26/50] 500 mean_batch_loss : 3.692960\n",
      "Start predicting 2019-12-23 16:06:45\n",
      "testing finish [2019-12-23 16:06:47] \n",
      "\tHR@1=0.15913  MRR@1=0.15913  NDCG@1=0.15913\n",
      "\tHR@5=0.33499  MRR@5=0.22315  NDCG@5=0.25094\n",
      "\tHR@10=0.41607  MRR@10=0.23401  NDCG@10=0.27721\n",
      "\tHR@20=0.49194  MRR@20=0.23924  NDCG@20=0.29635\n",
      "[2019-12-23 16:06:47] [27/50] 0 mean_batch_loss : 3.405789\n",
      "[2019-12-23 16:07:19] [27/50] 500 mean_batch_loss : 3.689766\n",
      "Start predicting 2019-12-23 16:07:39\n",
      "change best\n",
      "testing finish [2019-12-23 16:07:41] \n",
      "\tHR@1=0.15920  MRR@1=0.15920  NDCG@1=0.15920\n",
      "\tHR@5=0.33717  MRR@5=0.22394  NDCG@5=0.25208\n",
      "\tHR@10=0.41554  MRR@10=0.23447  NDCG@10=0.27750\n",
      "\tHR@20=0.49346  MRR@20=0.23995  NDCG@20=0.29730\n",
      "[2019-12-23 16:07:41] [28/50] 0 mean_batch_loss : 3.242536\n",
      "[2019-12-23 16:08:14] [28/50] 500 mean_batch_loss : 3.675264\n",
      "Start predicting 2019-12-23 16:08:36\n",
      "change best\n",
      "testing finish [2019-12-23 16:08:37] \n",
      "\tHR@1=0.16257  MRR@1=0.16257  NDCG@1=0.16257\n",
      "\tHR@5=0.33366  MRR@5=0.22479  NDCG@5=0.25185\n",
      "\tHR@10=0.41383  MRR@10=0.23550  NDCG@10=0.27778\n",
      "\tHR@20=0.49299  MRR@20=0.24106  NDCG@20=0.29788\n",
      "[2019-12-23 16:08:37] [29/50] 0 mean_batch_loss : 3.605252\n",
      "[2019-12-23 16:09:08] [29/50] 500 mean_batch_loss : 3.675766\n",
      "Start predicting 2019-12-23 16:09:30\n",
      "testing finish [2019-12-23 16:09:32] \n",
      "\tHR@1=0.15893  MRR@1=0.15893  NDCG@1=0.15893\n",
      "\tHR@5=0.33307  MRR@5=0.22223  NDCG@5=0.24978\n",
      "\tHR@10=0.41330  MRR@10=0.23300  NDCG@10=0.27579\n",
      "\tHR@20=0.49187  MRR@20=0.23845  NDCG@20=0.29565\n",
      "[2019-12-23 16:09:32] [30/50] 0 mean_batch_loss : 3.587443\n",
      "[2019-12-23 16:10:04] [30/50] 500 mean_batch_loss : 3.670305\n",
      "Start predicting 2019-12-23 16:10:24\n",
      "change best\n",
      "testing finish [2019-12-23 16:10:26] \n",
      "\tHR@1=0.16019  MRR@1=0.16019  NDCG@1=0.16019\n",
      "\tHR@5=0.33875  MRR@5=0.22431  NDCG@5=0.25271\n",
      "\tHR@10=0.41620  MRR@10=0.23473  NDCG@10=0.27784\n",
      "\tHR@20=0.49399  MRR@20=0.24017  NDCG@20=0.29756\n",
      "[2019-12-23 16:10:26] [31/50] 0 mean_batch_loss : 3.345303\n",
      "[2019-12-23 16:10:57] [31/50] 500 mean_batch_loss : 3.665643\n",
      "Start predicting 2019-12-23 16:11:18\n",
      "change best\n",
      "testing finish [2019-12-23 16:11:19] \n",
      "\tHR@1=0.16316  MRR@1=0.16316  NDCG@1=0.16316\n",
      "\tHR@5=0.33717  MRR@5=0.22614  NDCG@5=0.25371\n",
      "\tHR@10=0.41587  MRR@10=0.23670  NDCG@10=0.27921\n",
      "\tHR@20=0.49379  MRR@20=0.24216  NDCG@20=0.29898\n",
      "[2019-12-23 16:11:19] [32/50] 0 mean_batch_loss : 3.577837\n",
      "[2019-12-23 16:11:50] [32/50] 500 mean_batch_loss : 3.658350\n",
      "Start predicting 2019-12-23 16:12:12\n",
      "testing finish [2019-12-23 16:12:14] \n",
      "\tHR@1=0.16052  MRR@1=0.16052  NDCG@1=0.16052\n",
      "\tHR@5=0.33921  MRR@5=0.22499  NDCG@5=0.25335\n",
      "\tHR@10=0.41607  MRR@10=0.23533  NDCG@10=0.27829\n",
      "\tHR@20=0.49187  MRR@20=0.24059  NDCG@20=0.29745\n",
      "[2019-12-23 16:12:14] [33/50] 0 mean_batch_loss : 3.342662\n",
      "[2019-12-23 16:12:45] [33/50] 500 mean_batch_loss : 3.650262\n",
      "Start predicting 2019-12-23 16:13:06\n",
      "testing finish [2019-12-23 16:13:08] \n",
      "\tHR@1=0.15999  MRR@1=0.15999  NDCG@1=0.15999\n",
      "\tHR@5=0.33776  MRR@5=0.22437  NDCG@5=0.25254\n",
      "\tHR@10=0.41640  MRR@10=0.23482  NDCG@10=0.27792\n",
      "\tHR@20=0.49412  MRR@20=0.24023  NDCG@20=0.29759\n",
      "[2019-12-23 16:13:08] [34/50] 0 mean_batch_loss : 3.429247\n",
      "[2019-12-23 16:13:40] [34/50] 500 mean_batch_loss : 3.647234\n",
      "Start predicting 2019-12-23 16:14:01\n",
      "testing finish [2019-12-23 16:14:03] \n",
      "\tHR@1=0.15993  MRR@1=0.15993  NDCG@1=0.15993\n",
      "\tHR@5=0.33796  MRR@5=0.22385  NDCG@5=0.25216\n",
      "\tHR@10=0.41891  MRR@10=0.23476  NDCG@10=0.27845\n",
      "\tHR@20=0.49471  MRR@20=0.24000  NDCG@20=0.29757\n",
      "[2019-12-23 16:14:03] [35/50] 0 mean_batch_loss : 3.319715\n",
      "[2019-12-23 16:14:34] [35/50] 500 mean_batch_loss : 3.640193\n",
      "Start predicting 2019-12-23 16:14:55\n",
      "testing finish [2019-12-23 16:14:57] \n",
      "\tHR@1=0.16178  MRR@1=0.16178  NDCG@1=0.16178\n",
      "\tHR@5=0.33598  MRR@5=0.22487  NDCG@5=0.25247\n",
      "\tHR@10=0.41501  MRR@10=0.23548  NDCG@10=0.27810\n",
      "\tHR@20=0.49280  MRR@20=0.24086  NDCG@20=0.29774\n",
      "[2019-12-23 16:14:57] [36/50] 0 mean_batch_loss : 3.297270\n",
      "[2019-12-23 16:15:26] [36/50] 500 mean_batch_loss : 3.638417\n",
      "Start predicting 2019-12-23 16:15:47\n",
      "change best\n",
      "testing finish [2019-12-23 16:15:48] \n",
      "\tHR@1=0.16211  MRR@1=0.16211  NDCG@1=0.16211\n",
      "\tHR@5=0.34014  MRR@5=0.22658  NDCG@5=0.25480\n",
      "\tHR@10=0.41957  MRR@10=0.23720  NDCG@10=0.28050\n",
      "\tHR@20=0.49458  MRR@20=0.24244  NDCG@20=0.29950\n",
      "[2019-12-23 16:15:48] [37/50] 0 mean_batch_loss : 3.246331\n",
      "[2019-12-23 16:16:20] [37/50] 500 mean_batch_loss : 3.637404\n",
      "Start predicting 2019-12-23 16:16:41\n",
      "change best\n",
      "testing finish [2019-12-23 16:16:43] \n",
      "\tHR@1=0.16151  MRR@1=0.16151  NDCG@1=0.16151\n",
      "\tHR@5=0.33803  MRR@5=0.22501  NDCG@5=0.25307\n",
      "\tHR@10=0.41713  MRR@10=0.23560  NDCG@10=0.27868\n",
      "\tHR@20=0.49676  MRR@20=0.24111  NDCG@20=0.29879\n",
      "[2019-12-23 16:16:43] [38/50] 0 mean_batch_loss : 3.285411\n",
      "[2019-12-23 16:17:16] [38/50] 500 mean_batch_loss : 3.628700\n",
      "Start predicting 2019-12-23 16:17:38\n",
      "testing finish [2019-12-23 16:17:39] \n",
      "\tHR@1=0.15834  MRR@1=0.15834  NDCG@1=0.15834\n",
      "\tHR@5=0.33948  MRR@5=0.22383  NDCG@5=0.25255\n",
      "\tHR@10=0.41766  MRR@10=0.23430  NDCG@10=0.27787\n",
      "\tHR@20=0.49531  MRR@20=0.23975  NDCG@20=0.29758\n",
      "[2019-12-23 16:17:39] [39/50] 0 mean_batch_loss : 3.288052\n",
      "[2019-12-23 16:18:10] [39/50] 500 mean_batch_loss : 3.623744\n",
      "Start predicting 2019-12-23 16:18:32\n",
      "testing finish [2019-12-23 16:18:33] \n",
      "\tHR@1=0.16118  MRR@1=0.16118  NDCG@1=0.16118\n",
      "\tHR@5=0.33849  MRR@5=0.22573  NDCG@5=0.25376\n",
      "\tHR@10=0.41918  MRR@10=0.23662  NDCG@10=0.27998\n",
      "\tHR@20=0.49346  MRR@20=0.24178  NDCG@20=0.29876\n",
      "[2019-12-23 16:18:33] [40/50] 0 mean_batch_loss : 3.543046\n",
      "[2019-12-23 16:19:01] [40/50] 500 mean_batch_loss : 3.619195\n",
      "Start predicting 2019-12-23 16:19:22\n",
      "testing finish [2019-12-23 16:19:23] \n",
      "\tHR@1=0.16151  MRR@1=0.16151  NDCG@1=0.16151\n",
      "\tHR@5=0.34107  MRR@5=0.22624  NDCG@5=0.25474\n",
      "\tHR@10=0.42215  MRR@10=0.23708  NDCG@10=0.28099\n",
      "\tHR@20=0.49451  MRR@20=0.24206  NDCG@20=0.29923\n",
      "[2019-12-23 16:19:23] [41/50] 0 mean_batch_loss : 3.344008\n",
      "[2019-12-23 16:19:56] [41/50] 500 mean_batch_loss : 3.617458\n",
      "Start predicting 2019-12-23 16:20:17\n",
      "change best\n",
      "testing finish [2019-12-23 16:20:18] \n",
      "\tHR@1=0.15887  MRR@1=0.15887  NDCG@1=0.15887\n",
      "\tHR@5=0.33974  MRR@5=0.22494  NDCG@5=0.25348\n",
      "\tHR@10=0.41852  MRR@10=0.23540  NDCG@10=0.27890\n",
      "\tHR@20=0.49927  MRR@20=0.24101  NDCG@20=0.29932\n",
      "[2019-12-23 16:20:18] [42/50] 0 mean_batch_loss : 3.284003\n",
      "[2019-12-23 16:20:50] [42/50] 500 mean_batch_loss : 3.609793\n",
      "Start predicting 2019-12-23 16:21:12\n",
      "change best\n",
      "testing finish [2019-12-23 16:21:13] \n",
      "\tHR@1=0.15979  MRR@1=0.15979  NDCG@1=0.15979\n",
      "\tHR@5=0.33809  MRR@5=0.22528  NDCG@5=0.25336\n",
      "\tHR@10=0.42156  MRR@10=0.23647  NDCG@10=0.28040\n",
      "\tHR@20=0.49941  MRR@20=0.24183  NDCG@20=0.30003\n",
      "[2019-12-23 16:21:13] [43/50] 0 mean_batch_loss : 3.393976\n",
      "[2019-12-23 16:21:44] [43/50] 500 mean_batch_loss : 3.612610\n",
      "Start predicting 2019-12-23 16:22:06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing finish [2019-12-23 16:22:07] \n",
      "\tHR@1=0.16515  MRR@1=0.16515  NDCG@1=0.16515\n",
      "\tHR@5=0.34067  MRR@5=0.22839  NDCG@5=0.25627\n",
      "\tHR@10=0.41984  MRR@10=0.23901  NDCG@10=0.28193\n",
      "\tHR@20=0.49637  MRR@20=0.24434  NDCG@20=0.30131\n",
      "[2019-12-23 16:22:08] [44/50] 0 mean_batch_loss : 3.113177\n",
      "[2019-12-23 16:22:38] [44/50] 500 mean_batch_loss : 3.606771\n",
      "Start predicting 2019-12-23 16:22:59\n",
      "testing finish [2019-12-23 16:23:01] \n",
      "\tHR@1=0.16197  MRR@1=0.16197  NDCG@1=0.16197\n",
      "\tHR@5=0.33743  MRR@5=0.22575  NDCG@5=0.25350\n",
      "\tHR@10=0.41627  MRR@10=0.23635  NDCG@10=0.27907\n",
      "\tHR@20=0.49577  MRR@20=0.24193  NDCG@20=0.29924\n",
      "[2019-12-23 16:23:01] [45/50] 0 mean_batch_loss : 3.416957\n",
      "[2019-12-23 16:23:32] [45/50] 500 mean_batch_loss : 3.601614\n",
      "Start predicting 2019-12-23 16:23:55\n",
      "testing finish [2019-12-23 16:23:56] \n",
      "\tHR@1=0.16369  MRR@1=0.16369  NDCG@1=0.16369\n",
      "\tHR@5=0.34093  MRR@5=0.22762  NDCG@5=0.25575\n",
      "\tHR@10=0.41898  MRR@10=0.23804  NDCG@10=0.28098\n",
      "\tHR@20=0.49570  MRR@20=0.24335  NDCG@20=0.30037\n",
      "[2019-12-23 16:23:56] [46/50] 0 mean_batch_loss : 3.336593\n",
      "[2019-12-23 16:24:27] [46/50] 500 mean_batch_loss : 3.604549\n",
      "Start predicting 2019-12-23 16:24:49\n",
      "testing finish [2019-12-23 16:24:51] \n",
      "\tHR@1=0.16363  MRR@1=0.16363  NDCG@1=0.16363\n",
      "\tHR@5=0.33756  MRR@5=0.22649  NDCG@5=0.25406\n",
      "\tHR@10=0.41753  MRR@10=0.23719  NDCG@10=0.27995\n",
      "\tHR@20=0.49498  MRR@20=0.24259  NDCG@20=0.29956\n",
      "[2019-12-23 16:24:51] [47/50] 0 mean_batch_loss : 3.332481\n",
      "[2019-12-23 16:25:20] [47/50] 500 mean_batch_loss : 3.606254\n",
      "Start predicting 2019-12-23 16:25:41\n",
      "testing finish [2019-12-23 16:25:43] \n",
      "\tHR@1=0.16396  MRR@1=0.16396  NDCG@1=0.16396\n",
      "\tHR@5=0.33941  MRR@5=0.22785  NDCG@5=0.25559\n",
      "\tHR@10=0.42043  MRR@10=0.23867  NDCG@10=0.28179\n",
      "\tHR@20=0.49405  MRR@20=0.24375  NDCG@20=0.30038\n",
      "[2019-12-23 16:25:43] [48/50] 0 mean_batch_loss : 3.315392\n",
      "[2019-12-23 16:26:15] [48/50] 500 mean_batch_loss : 3.592682\n",
      "Start predicting 2019-12-23 16:26:36\n",
      "testing finish [2019-12-23 16:26:37] \n",
      "\tHR@1=0.15993  MRR@1=0.15993  NDCG@1=0.15993\n",
      "\tHR@5=0.33617  MRR@5=0.22509  NDCG@5=0.25275\n",
      "\tHR@10=0.41720  MRR@10=0.23600  NDCG@10=0.27905\n",
      "\tHR@20=0.49412  MRR@20=0.24137  NDCG@20=0.29854\n",
      "[2019-12-23 16:26:38] [49/50] 0 mean_batch_loss : 3.255973\n",
      "[2019-12-23 16:27:09] [49/50] 500 mean_batch_loss : 3.591167\n",
      "Start predicting 2019-12-23 16:27:31\n",
      "testing finish [2019-12-23 16:27:32] \n",
      "\tHR@1=0.16416  MRR@1=0.16416  NDCG@1=0.16416\n",
      "\tHR@5=0.34027  MRR@5=0.22865  NDCG@5=0.25642\n",
      "\tHR@10=0.41984  MRR@10=0.23931  NDCG@10=0.28219\n",
      "\tHR@20=0.49372  MRR@20=0.24443  NDCG@20=0.30086\n",
      "[2019-12-23 16:27:32] [50/50] 0 mean_batch_loss : 3.517131\n",
      "[2019-12-23 16:28:04] [50/50] 500 mean_batch_loss : 3.590233\n",
      "Start predicting 2019-12-23 16:28:26\n",
      "testing finish [2019-12-23 16:28:27] \n",
      "\tHR@1=0.16376  MRR@1=0.16376  NDCG@1=0.16376\n",
      "\tHR@5=0.33915  MRR@5=0.22739  NDCG@5=0.25518\n",
      "\tHR@10=0.42043  MRR@10=0.23827  NDCG@10=0.28150\n",
      "\tHR@20=0.49577  MRR@20=0.24354  NDCG@20=0.30059\n",
      "best model change\n",
      "current model hyper-parameters: session_length=20, hidden_size=100, lr=0.0030, dropout=0.00\n",
      "\n",
      "current model Recall@20=0.49941  MRR@20=0.24183\n",
      "the best result so far. Recall@20=0.49941  MRR@20=0.24183，其参数session_length=20, hidden_size=100, lr=0.0030, dropout=0.00\n",
      " \n",
      "\n",
      "current model hyper-parameters: session_length=20, hidden_size=100, lr=0.0010, dropout=0.00\n",
      "\n",
      "[2019-12-23 16:28:27] [1/50] 0 mean_batch_loss : 10.517811\n",
      "[2019-12-23 16:28:56] [1/50] 500 mean_batch_loss : 9.955890\n",
      "Start predicting 2019-12-23 16:29:17\n",
      "change best\n",
      "testing finish [2019-12-23 16:29:19] \n",
      "\tHR@1=0.00760  MRR@1=0.00760  NDCG@1=0.00760\n",
      "\tHR@5=0.02174  MRR@5=0.01239  NDCG@5=0.01469\n",
      "\tHR@10=0.03106  MRR@10=0.01361  NDCG@10=0.01768\n",
      "\tHR@20=0.04137  MRR@20=0.01430  NDCG@20=0.02026\n",
      "[2019-12-23 16:29:19] [2/50] 0 mean_batch_loss : 9.148385\n",
      "[2019-12-23 16:29:49] [2/50] 500 mean_batch_loss : 8.735043\n",
      "Start predicting 2019-12-23 16:30:09\n",
      "change best\n",
      "testing finish [2019-12-23 16:30:10] \n",
      "\tHR@1=0.03331  MRR@1=0.03331  NDCG@1=0.03331\n",
      "\tHR@5=0.08624  MRR@5=0.05160  NDCG@5=0.06016\n",
      "\tHR@10=0.12087  MRR@10=0.05613  NDCG@10=0.07127\n",
      "\tHR@20=0.16468  MRR@20=0.05916  NDCG@20=0.08233\n",
      "[2019-12-23 16:30:10] [3/50] 0 mean_batch_loss : 7.496780\n",
      "[2019-12-23 16:30:44] [3/50] 500 mean_batch_loss : 7.278668\n",
      "Start predicting 2019-12-23 16:31:06\n",
      "change best\n",
      "testing finish [2019-12-23 16:31:07] \n",
      "\tHR@1=0.06351  MRR@1=0.06351  NDCG@1=0.06351\n",
      "\tHR@5=0.15623  MRR@5=0.09582  NDCG@5=0.11076\n",
      "\tHR@10=0.20189  MRR@10=0.10197  NDCG@10=0.12558\n",
      "\tHR@20=0.26183  MRR@20=0.10611  NDCG@20=0.14071\n",
      "[2019-12-23 16:31:07] [4/50] 0 mean_batch_loss : 6.177405\n",
      "[2019-12-23 16:31:40] [4/50] 500 mean_batch_loss : 6.239591\n",
      "Start predicting 2019-12-23 16:32:02\n",
      "change best\n",
      "testing finish [2019-12-23 16:32:03] \n",
      "\tHR@1=0.08631  MRR@1=0.08631  NDCG@1=0.08631\n",
      "\tHR@5=0.20096  MRR@5=0.12680  NDCG@5=0.14518\n",
      "\tHR@10=0.25846  MRR@10=0.13443  NDCG@10=0.16373\n",
      "\tHR@20=0.32216  MRR@20=0.13888  NDCG@20=0.17986\n",
      "[2019-12-23 16:32:04] [5/50] 0 mean_batch_loss : 5.707857\n",
      "[2019-12-23 16:32:36] [5/50] 500 mean_batch_loss : 5.563344\n",
      "Start predicting 2019-12-23 16:32:58\n",
      "change best\n",
      "testing finish [2019-12-23 16:32:59] \n",
      "\tHR@1=0.10527  MRR@1=0.10527  NDCG@1=0.10527\n",
      "\tHR@5=0.23110  MRR@5=0.14995  NDCG@5=0.17005\n",
      "\tHR@10=0.29619  MRR@10=0.15859  NDCG@10=0.19105\n",
      "\tHR@20=0.36301  MRR@20=0.16322  NDCG@20=0.20794\n",
      "[2019-12-23 16:33:00] [6/50] 0 mean_batch_loss : 5.311653\n",
      "[2019-12-23 16:33:31] [6/50] 500 mean_batch_loss : 5.114434\n",
      "Start predicting 2019-12-23 16:33:52\n",
      "change best\n",
      "testing finish [2019-12-23 16:33:53] \n",
      "\tHR@1=0.11591  MRR@1=0.11591  NDCG@1=0.11591\n",
      "\tHR@5=0.24960  MRR@5=0.16360  NDCG@5=0.18493\n",
      "\tHR@10=0.31562  MRR@10=0.17243  NDCG@10=0.20629\n",
      "\tHR@20=0.38415  MRR@20=0.17719  NDCG@20=0.22362\n",
      "[2019-12-23 16:33:53] [7/50] 0 mean_batch_loss : 4.691569\n",
      "[2019-12-23 16:34:25] [7/50] 500 mean_batch_loss : 4.810021\n",
      "Start predicting 2019-12-23 16:34:47\n",
      "change best\n",
      "testing finish [2019-12-23 16:34:48] \n",
      "\tHR@1=0.11902  MRR@1=0.11902  NDCG@1=0.11902\n",
      "\tHR@5=0.26183  MRR@5=0.17037  NDCG@5=0.19307\n",
      "\tHR@10=0.33518  MRR@10=0.18012  NDCG@10=0.21675\n",
      "\tHR@20=0.40345  MRR@20=0.18488  NDCG@20=0.23404\n",
      "[2019-12-23 16:34:48] [8/50] 0 mean_batch_loss : 4.346916\n",
      "[2019-12-23 16:35:19] [8/50] 500 mean_batch_loss : 4.596840\n",
      "Start predicting 2019-12-23 16:35:41\n",
      "change best\n",
      "testing finish [2019-12-23 16:35:42] \n",
      "\tHR@1=0.12721  MRR@1=0.12721  NDCG@1=0.12721\n",
      "\tHR@5=0.27214  MRR@5=0.17874  NDCG@5=0.20189\n",
      "\tHR@10=0.34173  MRR@10=0.18814  NDCG@10=0.22451\n",
      "\tHR@20=0.41098  MRR@20=0.19295  NDCG@20=0.24203\n",
      "[2019-12-23 16:35:42] [9/50] 0 mean_batch_loss : 4.060470\n",
      "[2019-12-23 16:36:13] [9/50] 500 mean_batch_loss : 4.428497\n",
      "Start predicting 2019-12-23 16:36:34\n",
      "change best\n",
      "testing finish [2019-12-23 16:36:35] \n",
      "\tHR@1=0.13151  MRR@1=0.13151  NDCG@1=0.13151\n",
      "\tHR@5=0.27921  MRR@5=0.18415  NDCG@5=0.20772\n",
      "\tHR@10=0.35309  MRR@10=0.19393  NDCG@10=0.23153\n",
      "\tHR@20=0.42057  MRR@20=0.19862  NDCG@20=0.24859\n",
      "[2019-12-23 16:36:35] [10/50] 0 mean_batch_loss : 4.104029\n",
      "[2019-12-23 16:37:06] [10/50] 500 mean_batch_loss : 4.297165\n",
      "Start predicting 2019-12-23 16:37:27\n",
      "change best\n",
      "testing finish [2019-12-23 16:37:28] \n",
      "\tHR@1=0.13376  MRR@1=0.13376  NDCG@1=0.13376\n",
      "\tHR@5=0.28661  MRR@5=0.18871  NDCG@5=0.21301\n",
      "\tHR@10=0.35712  MRR@10=0.19806  NDCG@10=0.23576\n",
      "\tHR@20=0.42301  MRR@20=0.20264  NDCG@20=0.25243\n",
      "[2019-12-23 16:37:28] [11/50] 0 mean_batch_loss : 4.430163\n",
      "[2019-12-23 16:38:00] [11/50] 500 mean_batch_loss : 4.198438\n",
      "Start predicting 2019-12-23 16:38:21\n",
      "change best\n",
      "testing finish [2019-12-23 16:38:22] \n",
      "\tHR@1=0.13481  MRR@1=0.13481  NDCG@1=0.13481\n",
      "\tHR@5=0.29091  MRR@5=0.19097  NDCG@5=0.21578\n",
      "\tHR@10=0.36036  MRR@10=0.20023  NDCG@10=0.23822\n",
      "\tHR@20=0.42757  MRR@20=0.20484  NDCG@20=0.25516\n",
      "[2019-12-23 16:38:22] [12/50] 0 mean_batch_loss : 4.164462\n",
      "[2019-12-23 16:38:53] [12/50] 500 mean_batch_loss : 4.112625\n",
      "Start predicting 2019-12-23 16:39:14\n",
      "change best\n",
      "testing finish [2019-12-23 16:39:15] \n",
      "\tHR@1=0.14030  MRR@1=0.14030  NDCG@1=0.14030\n",
      "\tHR@5=0.29243  MRR@5=0.19483  NDCG@5=0.21905\n",
      "\tHR@10=0.36261  MRR@10=0.20432  NDCG@10=0.24187\n",
      "\tHR@20=0.43140  MRR@20=0.20912  NDCG@20=0.25929\n",
      "[2019-12-23 16:39:15] [13/50] 0 mean_batch_loss : 4.067936\n",
      "[2019-12-23 16:39:47] [13/50] 500 mean_batch_loss : 4.047954\n",
      "Start predicting 2019-12-23 16:40:09\n",
      "change best\n",
      "testing finish [2019-12-23 16:40:10] \n",
      "\tHR@1=0.13977  MRR@1=0.13977  NDCG@1=0.13977\n",
      "\tHR@5=0.29837  MRR@5=0.19716  NDCG@5=0.22230\n",
      "\tHR@10=0.36796  MRR@10=0.20651  NDCG@10=0.24487\n",
      "\tHR@20=0.43537  MRR@20=0.21120  NDCG@20=0.26193\n",
      "[2019-12-23 16:40:10] [14/50] 0 mean_batch_loss : 3.822829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-12-23 16:40:42] [14/50] 500 mean_batch_loss : 3.987491\n",
      "Start predicting 2019-12-23 16:41:03\n",
      "change best\n",
      "testing finish [2019-12-23 16:41:04] \n",
      "\tHR@1=0.14155  MRR@1=0.14155  NDCG@1=0.14155\n",
      "\tHR@5=0.29989  MRR@5=0.19862  NDCG@5=0.22375\n",
      "\tHR@10=0.36875  MRR@10=0.20788  NDCG@10=0.24610\n",
      "\tHR@20=0.43695  MRR@20=0.21261  NDCG@20=0.26333\n",
      "[2019-12-23 16:41:04] [15/50] 0 mean_batch_loss : 3.887766\n",
      "[2019-12-23 16:41:35] [15/50] 500 mean_batch_loss : 3.938730\n",
      "Start predicting 2019-12-23 16:41:52\n",
      "change best\n",
      "testing finish [2019-12-23 16:41:53] \n",
      "\tHR@1=0.14215  MRR@1=0.14215  NDCG@1=0.14215\n",
      "\tHR@5=0.29950  MRR@5=0.19897  NDCG@5=0.22394\n",
      "\tHR@10=0.36922  MRR@10=0.20832  NDCG@10=0.24653\n",
      "\tHR@20=0.43841  MRR@20=0.21316  NDCG@20=0.26407\n",
      "[2019-12-23 16:41:53] [16/50] 0 mean_batch_loss : 3.771483\n",
      "[2019-12-23 16:42:20] [16/50] 500 mean_batch_loss : 3.898684\n",
      "Start predicting 2019-12-23 16:42:35\n",
      "change best\n",
      "testing finish [2019-12-23 16:42:37] \n",
      "\tHR@1=0.14374  MRR@1=0.14374  NDCG@1=0.14374\n",
      "\tHR@5=0.30360  MRR@5=0.20141  NDCG@5=0.22678\n",
      "\tHR@10=0.37160  MRR@10=0.21054  NDCG@10=0.24882\n",
      "\tHR@20=0.44105  MRR@20=0.21536  NDCG@20=0.26639\n",
      "[2019-12-23 16:42:37] [17/50] 0 mean_batch_loss : 3.936148\n",
      "[2019-12-23 16:42:58] [17/50] 500 mean_batch_loss : 3.859452\n",
      "Start predicting 2019-12-23 16:43:15\n",
      "change best\n",
      "testing finish [2019-12-23 16:43:17] \n",
      "\tHR@1=0.14532  MRR@1=0.14532  NDCG@1=0.14532\n",
      "\tHR@5=0.30419  MRR@5=0.20286  NDCG@5=0.22804\n",
      "\tHR@10=0.37589  MRR@10=0.21238  NDCG@10=0.25118\n",
      "\tHR@20=0.44489  MRR@20=0.21709  NDCG@20=0.26852\n",
      "[2019-12-23 16:43:17] [18/50] 0 mean_batch_loss : 3.730655\n",
      "[2019-12-23 16:43:45] [18/50] 500 mean_batch_loss : 3.818672\n",
      "Start predicting 2019-12-23 16:44:03\n",
      "change best\n",
      "testing finish [2019-12-23 16:44:04] \n",
      "\tHR@1=0.14618  MRR@1=0.14618  NDCG@1=0.14618\n",
      "\tHR@5=0.30663  MRR@5=0.20415  NDCG@5=0.22960\n",
      "\tHR@10=0.37517  MRR@10=0.21330  NDCG@10=0.25177\n",
      "\tHR@20=0.44561  MRR@20=0.21818  NDCG@20=0.26956\n",
      "[2019-12-23 16:44:04] [19/50] 0 mean_batch_loss : 3.528987\n",
      "[2019-12-23 16:44:18] [19/50] 500 mean_batch_loss : 3.790301\n",
      "Start predicting 2019-12-23 16:44:23\n",
      "testing finish [2019-12-23 16:44:24] \n",
      "\tHR@1=0.14631  MRR@1=0.14631  NDCG@1=0.14631\n",
      "\tHR@5=0.30690  MRR@5=0.20444  NDCG@5=0.22989\n",
      "\tHR@10=0.37583  MRR@10=0.21362  NDCG@10=0.25216\n",
      "\tHR@20=0.44508  MRR@20=0.21844  NDCG@20=0.26969\n",
      "[2019-12-23 16:44:24] [20/50] 0 mean_batch_loss : 3.687279\n",
      "[2019-12-23 16:44:34] [20/50] 500 mean_batch_loss : 3.764247\n",
      "Start predicting 2019-12-23 16:44:44\n",
      "change best\n",
      "testing finish [2019-12-23 16:44:45] \n",
      "\tHR@1=0.14757  MRR@1=0.14757  NDCG@1=0.14757\n",
      "\tHR@5=0.30545  MRR@5=0.20489  NDCG@5=0.22989\n",
      "\tHR@10=0.37741  MRR@10=0.21450  NDCG@10=0.25316\n",
      "\tHR@20=0.44660  MRR@20=0.21934  NDCG@20=0.27071\n",
      "[2019-12-23 16:44:45] [21/50] 0 mean_batch_loss : 3.665647\n",
      "[2019-12-23 16:45:04] [21/50] 500 mean_batch_loss : 3.738386\n",
      "Start predicting 2019-12-23 16:45:16\n",
      "change best\n",
      "testing finish [2019-12-23 16:45:17] \n",
      "\tHR@1=0.14843  MRR@1=0.14843  NDCG@1=0.14843\n",
      "\tHR@5=0.30934  MRR@5=0.20666  NDCG@5=0.23217\n",
      "\tHR@10=0.37939  MRR@10=0.21613  NDCG@10=0.25494\n",
      "\tHR@20=0.44852  MRR@20=0.22096  NDCG@20=0.27245\n",
      "[2019-12-23 16:45:17] [22/50] 0 mean_batch_loss : 3.804597\n",
      "[2019-12-23 16:45:31] [22/50] 500 mean_batch_loss : 3.709814\n",
      "Start predicting 2019-12-23 16:45:42\n",
      "testing finish [2019-12-23 16:45:43] \n",
      "\tHR@1=0.14611  MRR@1=0.14611  NDCG@1=0.14611\n",
      "\tHR@5=0.30842  MRR@5=0.20522  NDCG@5=0.23088\n",
      "\tHR@10=0.37973  MRR@10=0.21477  NDCG@10=0.25397\n",
      "\tHR@20=0.44878  MRR@20=0.21951  NDCG@20=0.27137\n",
      "[2019-12-23 16:45:43] [23/50] 0 mean_batch_loss : 3.474977\n",
      "[2019-12-23 16:45:59] [23/50] 500 mean_batch_loss : 3.689809\n",
      "Start predicting 2019-12-23 16:46:09\n",
      "testing finish [2019-12-23 16:46:10] \n",
      "\tHR@1=0.14968  MRR@1=0.14968  NDCG@1=0.14968\n",
      "\tHR@5=0.30862  MRR@5=0.20644  NDCG@5=0.23179\n",
      "\tHR@10=0.38078  MRR@10=0.21615  NDCG@10=0.25520\n",
      "\tHR@20=0.44799  MRR@20=0.22077  NDCG@20=0.27215\n",
      "[2019-12-23 16:46:10] [24/50] 0 mean_batch_loss : 3.675269\n",
      "[2019-12-23 16:46:24] [24/50] 500 mean_batch_loss : 3.665966\n",
      "Start predicting 2019-12-23 16:46:35\n",
      "change best\n",
      "testing finish [2019-12-23 16:46:36] \n",
      "\tHR@1=0.15200  MRR@1=0.15200  NDCG@1=0.15200\n",
      "\tHR@5=0.31133  MRR@5=0.20924  NDCG@5=0.23457\n",
      "\tHR@10=0.38230  MRR@10=0.21877  NDCG@10=0.25759\n",
      "\tHR@20=0.45248  MRR@20=0.22363  NDCG@20=0.27532\n",
      "[2019-12-23 16:46:36] [25/50] 0 mean_batch_loss : 3.503703\n",
      "[2019-12-23 16:46:52] [25/50] 500 mean_batch_loss : 3.649494\n",
      "Start predicting 2019-12-23 16:47:03\n",
      "testing finish [2019-12-23 16:47:04] \n",
      "\tHR@1=0.15114  MRR@1=0.15114  NDCG@1=0.15114\n",
      "\tHR@5=0.31192  MRR@5=0.20899  NDCG@5=0.23454\n",
      "\tHR@10=0.38250  MRR@10=0.21847  NDCG@10=0.25743\n",
      "\tHR@20=0.45057  MRR@20=0.22319  NDCG@20=0.27464\n",
      "[2019-12-23 16:47:04] [26/50] 0 mean_batch_loss : 3.423850\n",
      "[2019-12-23 16:47:18] [26/50] 500 mean_batch_loss : 3.636680\n",
      "Start predicting 2019-12-23 16:47:28\n",
      "change best\n",
      "testing finish [2019-12-23 16:47:30] \n",
      "\tHR@1=0.14988  MRR@1=0.14988  NDCG@1=0.14988\n",
      "\tHR@5=0.31238  MRR@5=0.20915  NDCG@5=0.23482\n",
      "\tHR@10=0.38382  MRR@10=0.21881  NDCG@10=0.25805\n",
      "\tHR@20=0.45308  MRR@20=0.22367  NDCG@20=0.27564\n",
      "[2019-12-23 16:47:30] [27/50] 0 mean_batch_loss : 3.463526\n",
      "[2019-12-23 16:47:46] [27/50] 500 mean_batch_loss : 3.618629\n",
      "Start predicting 2019-12-23 16:47:56\n",
      "change best\n",
      "testing finish [2019-12-23 16:47:58] \n",
      "\tHR@1=0.15312  MRR@1=0.15312  NDCG@1=0.15312\n",
      "\tHR@5=0.31450  MRR@5=0.21163  NDCG@5=0.23719\n",
      "\tHR@10=0.38343  MRR@10=0.22074  NDCG@10=0.25939\n",
      "\tHR@20=0.45196  MRR@20=0.22550  NDCG@20=0.27672\n",
      "[2019-12-23 16:47:58] [28/50] 0 mean_batch_loss : 3.583414\n",
      "[2019-12-23 16:48:12] [28/50] 500 mean_batch_loss : 3.597645\n",
      "Start predicting 2019-12-23 16:48:22\n",
      "change best\n",
      "testing finish [2019-12-23 16:48:23] \n",
      "\tHR@1=0.15213  MRR@1=0.15213  NDCG@1=0.15213\n",
      "\tHR@5=0.31443  MRR@5=0.21108  NDCG@5=0.23676\n",
      "\tHR@10=0.38343  MRR@10=0.22035  NDCG@10=0.25913\n",
      "\tHR@20=0.45301  MRR@20=0.22520  NDCG@20=0.27675\n",
      "[2019-12-23 16:48:23] [29/50] 0 mean_batch_loss : 3.472574\n",
      "[2019-12-23 16:48:39] [29/50] 500 mean_batch_loss : 3.592894\n",
      "Start predicting 2019-12-23 16:48:49\n",
      "change best\n",
      "testing finish [2019-12-23 16:48:50] \n",
      "\tHR@1=0.15371  MRR@1=0.15371  NDCG@1=0.15371\n",
      "\tHR@5=0.31569  MRR@5=0.21228  NDCG@5=0.23796\n",
      "\tHR@10=0.38468  MRR@10=0.22155  NDCG@10=0.26034\n",
      "\tHR@20=0.45341  MRR@20=0.22635  NDCG@20=0.27775\n",
      "[2019-12-23 16:48:50] [30/50] 0 mean_batch_loss : 3.585166\n",
      "[2019-12-23 16:49:04] [30/50] 500 mean_batch_loss : 3.577083\n",
      "Start predicting 2019-12-23 16:49:14\n",
      "change best\n",
      "testing finish [2019-12-23 16:49:15] \n",
      "\tHR@1=0.15219  MRR@1=0.15219  NDCG@1=0.15219\n",
      "\tHR@5=0.31443  MRR@5=0.21124  NDCG@5=0.23689\n",
      "\tHR@10=0.38475  MRR@10=0.22069  NDCG@10=0.25969\n",
      "\tHR@20=0.45473  MRR@20=0.22554  NDCG@20=0.27738\n",
      "[2019-12-23 16:49:15] [31/50] 0 mean_batch_loss : 3.371338\n",
      "[2019-12-23 16:49:31] [31/50] 500 mean_batch_loss : 3.561285\n",
      "Start predicting 2019-12-23 16:49:41\n",
      "change best\n",
      "testing finish [2019-12-23 16:49:42] \n",
      "\tHR@1=0.15213  MRR@1=0.15213  NDCG@1=0.15213\n",
      "\tHR@5=0.31708  MRR@5=0.21179  NDCG@5=0.23794\n",
      "\tHR@10=0.38944  MRR@10=0.22152  NDCG@10=0.26142\n",
      "\tHR@20=0.45652  MRR@20=0.22617  NDCG@20=0.27837\n",
      "[2019-12-23 16:49:43] [32/50] 0 mean_batch_loss : 3.462496\n",
      "[2019-12-23 16:49:56] [32/50] 500 mean_batch_loss : 3.555796\n",
      "Start predicting 2019-12-23 16:50:06\n",
      "testing finish [2019-12-23 16:50:07] \n",
      "\tHR@1=0.15114  MRR@1=0.15114  NDCG@1=0.15114\n",
      "\tHR@5=0.31655  MRR@5=0.21092  NDCG@5=0.23716\n",
      "\tHR@10=0.38534  MRR@10=0.22019  NDCG@10=0.25948\n",
      "\tHR@20=0.45665  MRR@20=0.22514  NDCG@20=0.27752\n",
      "[2019-12-23 16:50:08] [33/50] 0 mean_batch_loss : 3.377160\n",
      "[2019-12-23 16:50:23] [33/50] 500 mean_batch_loss : 3.538253\n",
      "Start predicting 2019-12-23 16:50:33\n",
      "change best\n",
      "testing finish [2019-12-23 16:50:35] \n",
      "\tHR@1=0.15332  MRR@1=0.15332  NDCG@1=0.15332\n",
      "\tHR@5=0.31913  MRR@5=0.21287  NDCG@5=0.23925\n",
      "\tHR@10=0.38964  MRR@10=0.22232  NDCG@10=0.26209\n",
      "\tHR@20=0.45645  MRR@20=0.22691  NDCG@20=0.27893\n",
      "[2019-12-23 16:50:35] [34/50] 0 mean_batch_loss : 3.467086\n",
      "[2019-12-23 16:50:49] [34/50] 500 mean_batch_loss : 3.532212\n",
      "Start predicting 2019-12-23 16:50:59\n",
      "change best\n",
      "testing finish [2019-12-23 16:51:01] \n",
      "\tHR@1=0.15378  MRR@1=0.15378  NDCG@1=0.15378\n",
      "\tHR@5=0.31648  MRR@5=0.21331  NDCG@5=0.23898\n",
      "\tHR@10=0.38699  MRR@10=0.22282  NDCG@10=0.26188\n",
      "\tHR@20=0.45645  MRR@20=0.22762  NDCG@20=0.27941\n",
      "[2019-12-23 16:51:01] [35/50] 0 mean_batch_loss : 3.504606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-12-23 16:51:17] [35/50] 500 mean_batch_loss : 3.524060\n",
      "Start predicting 2019-12-23 16:51:27\n",
      "testing finish [2019-12-23 16:51:28] \n",
      "\tHR@1=0.15378  MRR@1=0.15378  NDCG@1=0.15378\n",
      "\tHR@5=0.31483  MRR@5=0.21183  NDCG@5=0.23741\n",
      "\tHR@10=0.38495  MRR@10=0.22131  NDCG@10=0.26020\n",
      "\tHR@20=0.45592  MRR@20=0.22624  NDCG@20=0.27815\n",
      "[2019-12-23 16:51:28] [36/50] 0 mean_batch_loss : 3.511302\n",
      "[2019-12-23 16:51:43] [36/50] 500 mean_batch_loss : 3.505296\n",
      "Start predicting 2019-12-23 16:51:53\n",
      "change best\n",
      "testing finish [2019-12-23 16:51:54] \n",
      "\tHR@1=0.15345  MRR@1=0.15345  NDCG@1=0.15345\n",
      "\tHR@5=0.31794  MRR@5=0.21284  NDCG@5=0.23894\n",
      "\tHR@10=0.38937  MRR@10=0.22240  NDCG@10=0.26207\n",
      "\tHR@20=0.45837  MRR@20=0.22720  NDCG@20=0.27952\n",
      "[2019-12-23 16:51:54] [37/50] 0 mean_batch_loss : 3.529239\n",
      "[2019-12-23 16:52:10] [37/50] 500 mean_batch_loss : 3.492500\n",
      "Start predicting 2019-12-23 16:52:20\n",
      "testing finish [2019-12-23 16:52:22] \n",
      "\tHR@1=0.15186  MRR@1=0.15186  NDCG@1=0.15186\n",
      "\tHR@5=0.31589  MRR@5=0.21121  NDCG@5=0.23722\n",
      "\tHR@10=0.38680  MRR@10=0.22077  NDCG@10=0.26025\n",
      "\tHR@20=0.45519  MRR@20=0.22552  NDCG@20=0.27755\n",
      "[2019-12-23 16:52:22] [38/50] 0 mean_batch_loss : 3.434034\n",
      "[2019-12-23 16:52:36] [38/50] 500 mean_batch_loss : 3.492852\n",
      "Start predicting 2019-12-23 16:52:46\n",
      "change best\n",
      "testing finish [2019-12-23 16:52:47] \n",
      "\tHR@1=0.15365  MRR@1=0.15365  NDCG@1=0.15365\n",
      "\tHR@5=0.31846  MRR@5=0.21285  NDCG@5=0.23906\n",
      "\tHR@10=0.38918  MRR@10=0.22228  NDCG@10=0.26192\n",
      "\tHR@20=0.45863  MRR@20=0.22712  NDCG@20=0.27951\n",
      "[2019-12-23 16:52:47] [39/50] 0 mean_batch_loss : 3.299959\n",
      "[2019-12-23 16:53:03] [39/50] 500 mean_batch_loss : 3.485594\n",
      "Start predicting 2019-12-23 16:53:13\n",
      "testing finish [2019-12-23 16:53:14] \n",
      "\tHR@1=0.15074  MRR@1=0.15074  NDCG@1=0.15074\n",
      "\tHR@5=0.31622  MRR@5=0.21070  NDCG@5=0.23691\n",
      "\tHR@10=0.38666  MRR@10=0.22022  NDCG@10=0.25982\n",
      "\tHR@20=0.45599  MRR@20=0.22505  NDCG@20=0.27736\n",
      "[2019-12-23 16:53:14] [40/50] 0 mean_batch_loss : 3.286850\n",
      "[2019-12-23 16:53:28] [40/50] 500 mean_batch_loss : 3.477019\n",
      "Start predicting 2019-12-23 16:53:39\n",
      "testing finish [2019-12-23 16:53:40] \n",
      "\tHR@1=0.15219  MRR@1=0.15219  NDCG@1=0.15219\n",
      "\tHR@5=0.31509  MRR@5=0.21112  NDCG@5=0.23696\n",
      "\tHR@10=0.38898  MRR@10=0.22101  NDCG@10=0.26087\n",
      "\tHR@20=0.45883  MRR@20=0.22585  NDCG@20=0.27853\n",
      "[2019-12-23 16:53:40] [41/50] 0 mean_batch_loss : 3.397948\n",
      "[2019-12-23 16:53:55] [41/50] 500 mean_batch_loss : 3.468247\n",
      "Start predicting 2019-12-23 16:54:05\n",
      "change best\n",
      "testing finish [2019-12-23 16:54:07] \n",
      "\tHR@1=0.15167  MRR@1=0.15167  NDCG@1=0.15167\n",
      "\tHR@5=0.31575  MRR@5=0.21168  NDCG@5=0.23757\n",
      "\tHR@10=0.38898  MRR@10=0.22149  NDCG@10=0.26128\n",
      "\tHR@20=0.45962  MRR@20=0.22633  NDCG@20=0.27907\n",
      "[2019-12-23 16:54:07] [42/50] 0 mean_batch_loss : 3.432768\n",
      "[2019-12-23 16:54:21] [42/50] 500 mean_batch_loss : 3.458561\n",
      "Start predicting 2019-12-23 16:54:32\n",
      "testing finish [2019-12-23 16:54:33] \n",
      "\tHR@1=0.15120  MRR@1=0.15120  NDCG@1=0.15120\n",
      "\tHR@5=0.31457  MRR@5=0.21052  NDCG@5=0.23637\n",
      "\tHR@10=0.38739  MRR@10=0.22023  NDCG@10=0.25992\n",
      "\tHR@20=0.45731  MRR@20=0.22506  NDCG@20=0.27756\n",
      "[2019-12-23 16:54:33] [43/50] 0 mean_batch_loss : 3.115852\n",
      "[2019-12-23 16:54:48] [43/50] 500 mean_batch_loss : 3.452767\n",
      "Start predicting 2019-12-23 16:54:58\n",
      "testing finish [2019-12-23 16:54:59] \n",
      "\tHR@1=0.15206  MRR@1=0.15206  NDCG@1=0.15206\n",
      "\tHR@5=0.31708  MRR@5=0.21151  NDCG@5=0.23771\n",
      "\tHR@10=0.38759  MRR@10=0.22096  NDCG@10=0.26056\n",
      "\tHR@20=0.45949  MRR@20=0.22595  NDCG@20=0.27873\n",
      "[2019-12-23 16:55:00] [44/50] 0 mean_batch_loss : 3.297748\n",
      "[2019-12-23 16:55:14] [44/50] 500 mean_batch_loss : 3.442822\n",
      "Start predicting 2019-12-23 16:55:25\n",
      "testing finish [2019-12-23 16:55:26] \n",
      "\tHR@1=0.15061  MRR@1=0.15061  NDCG@1=0.15061\n",
      "\tHR@5=0.31516  MRR@5=0.21066  NDCG@5=0.23665\n",
      "\tHR@10=0.38752  MRR@10=0.22030  NDCG@10=0.26003\n",
      "\tHR@20=0.45837  MRR@20=0.22523  NDCG@20=0.27796\n",
      "[2019-12-23 16:55:26] [45/50] 0 mean_batch_loss : 3.428631\n",
      "[2019-12-23 16:55:41] [45/50] 500 mean_batch_loss : 3.444571\n",
      "Start predicting 2019-12-23 16:55:52\n",
      "change best\n",
      "testing finish [2019-12-23 16:55:53] \n",
      "\tHR@1=0.15285  MRR@1=0.15285  NDCG@1=0.15285\n",
      "\tHR@5=0.31688  MRR@5=0.21241  NDCG@5=0.23837\n",
      "\tHR@10=0.39334  MRR@10=0.22265  NDCG@10=0.26313\n",
      "\tHR@20=0.46134  MRR@20=0.22735  NDCG@20=0.28030\n",
      "[2019-12-23 16:55:53] [46/50] 0 mean_batch_loss : 3.233219\n",
      "[2019-12-23 16:56:08] [46/50] 500 mean_batch_loss : 3.432626\n",
      "Start predicting 2019-12-23 16:56:18\n",
      "testing finish [2019-12-23 16:56:19] \n",
      "\tHR@1=0.15332  MRR@1=0.15332  NDCG@1=0.15332\n",
      "\tHR@5=0.31694  MRR@5=0.21243  NDCG@5=0.23838\n",
      "\tHR@10=0.39050  MRR@10=0.22230  NDCG@10=0.26222\n",
      "\tHR@20=0.45843  MRR@20=0.22704  NDCG@20=0.27944\n",
      "[2019-12-23 16:56:20] [47/50] 0 mean_batch_loss : 3.311286\n",
      "[2019-12-23 16:56:35] [47/50] 500 mean_batch_loss : 3.421983\n",
      "Start predicting 2019-12-23 16:56:46\n",
      "testing finish [2019-12-23 16:56:47] \n",
      "\tHR@1=0.14995  MRR@1=0.14995  NDCG@1=0.14995\n",
      "\tHR@5=0.31536  MRR@5=0.20989  NDCG@5=0.23608\n",
      "\tHR@10=0.39036  MRR@10=0.21993  NDCG@10=0.26037\n",
      "\tHR@20=0.45896  MRR@20=0.22469  NDCG@20=0.27772\n",
      "[2019-12-23 16:56:47] [48/50] 0 mean_batch_loss : 3.241736\n",
      "[2019-12-23 16:57:01] [48/50] 500 mean_batch_loss : 3.416793\n",
      "Start predicting 2019-12-23 16:57:12\n",
      "change best\n",
      "testing finish [2019-12-23 16:57:13] \n",
      "\tHR@1=0.15649  MRR@1=0.15649  NDCG@1=0.15649\n",
      "\tHR@5=0.32117  MRR@5=0.21510  NDCG@5=0.24139\n",
      "\tHR@10=0.39149  MRR@10=0.22452  NDCG@10=0.26416\n",
      "\tHR@20=0.46094  MRR@20=0.22934  NDCG@20=0.28172\n",
      "[2019-12-23 16:57:13] [49/50] 0 mean_batch_loss : 3.453419\n",
      "[2019-12-23 16:57:29] [49/50] 500 mean_batch_loss : 3.413729\n",
      "Start predicting 2019-12-23 16:57:39\n",
      "testing finish [2019-12-23 16:57:40] \n",
      "\tHR@1=0.15312  MRR@1=0.15312  NDCG@1=0.15312\n",
      "\tHR@5=0.31932  MRR@5=0.21329  NDCG@5=0.23962\n",
      "\tHR@10=0.39235  MRR@10=0.22313  NDCG@10=0.26334\n",
      "\tHR@20=0.46134  MRR@20=0.22784  NDCG@20=0.28068\n",
      "[2019-12-23 16:57:41] [50/50] 0 mean_batch_loss : 3.287333\n",
      "[2019-12-23 16:57:55] [50/50] 500 mean_batch_loss : 3.407932\n",
      "Start predicting 2019-12-23 16:58:05\n",
      "testing finish [2019-12-23 16:58:06] \n",
      "\tHR@1=0.15537  MRR@1=0.15537  NDCG@1=0.15537\n",
      "\tHR@5=0.32012  MRR@5=0.21464  NDCG@5=0.24083\n",
      "\tHR@10=0.39010  MRR@10=0.22403  NDCG@10=0.26351\n",
      "\tHR@20=0.45923  MRR@20=0.22880  NDCG@20=0.28096\n",
      "current model hyper-parameters: session_length=20, hidden_size=100, lr=0.0010, dropout=0.00\n",
      "\n",
      "current model Recall@20=0.49941  MRR@20=0.22934\n",
      "the best result so far. Recall@20=0.49941  MRR@20=0.24183，其参数session_length=20, hidden_size=100, lr=0.0030, dropout=0.00\n",
      " \n",
      "\n",
      "The best result HR@20=0.49941  MRR@20=0.24183, hyper-parameters: session_length=20, hidden_size=100, lr=0.0030, dropout=0.00\n",
      ". \n",
      "over.\n"
     ]
    }
   ],
   "source": [
    "hidden_sizes = [100]\n",
    "dropouts = [0]\n",
    "lrs = [3e-3,1e-3]\n",
    "session_lengths = [20]\n",
    "patience = 10\n",
    "best_params = \"\"\n",
    "best_all_model = 0.0\n",
    "best_all_hr = 0.0\n",
    "best_all_mrr = 0.0\n",
    "best_all_r1m = 0.0\n",
    "for session_length in session_lengths:\n",
    "    for hidden_size in hidden_sizes:\n",
    "        for dropout in dropouts:\n",
    "            for lr in lrs:\n",
    "                args = {}\n",
    "                print(\"current model hyper-parameters: session_length=%d, hidden_size=%d, lr=%.4f, dropout=%.2f\\n\" % (session_length,hidden_size,lr,dropout))\n",
    "                args[\"session_length\"] = session_length\n",
    "                args[\"hidden_size\"] = hidden_size\n",
    "                args[\"dropout\"] = dropout\n",
    "                args[\"patience\"] = patience\n",
    "                args[\"lr\"] = lr\n",
    "                best_model,best_model_hr,best_model_mrr = train(args)\n",
    "                if best_model_hr + best_model_mrr > best_all_r1m:\n",
    "                    print(\"best model change\")\n",
    "                    best_all_r1m = best_model_hr + best_model_mrr\n",
    "                    best_all_hr = best_model_hr\n",
    "                    best_all_mrr = best_model_mrr\n",
    "                    best_all_model = best_model\n",
    "                    best_params = \"session_length=%d, hidden_size=%d, lr=%.4f, dropout=%.2f\\n\" % (session_length,hidden_size,lr,dropout)\n",
    "                best_model = None\n",
    "                print(\"current model hyper-parameters: session_length=%d, hidden_size=%d, lr=%.4f, dropout=%.2f\\n\" % (session_length,hidden_size,lr,dropout))\n",
    "                print(\"current model Recall@20=%.5f  MRR@20=%.5f\"%(best_all_hr,best_model_mrr))\n",
    "                print(\"the best result so far. Recall@20=%.5f  MRR@20=%.5f,数%s \\n\"%(best_all_hr,best_all_mrr,best_params))\n",
    "print(\"The best result HR@20=%.5f  MRR@20=%.5f, hyper-parameters: %s. \"%(best_all_hr,best_all_mrr,best_params))\n",
    "print(\"over.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smy",
   "language": "python",
   "name": "smy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
