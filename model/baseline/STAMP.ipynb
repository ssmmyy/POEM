{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T09:17:54.038683Z",
     "start_time": "2020-03-02T09:17:53.816789Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T09:17:54.347526Z",
     "start_time": "2020-03-02T09:17:54.039864Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T09:17:54.351139Z",
     "start_time": "2020-03-02T09:17:54.348747Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda:0\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T09:17:54.359501Z",
     "start_time": "2020-03-02T09:17:54.351966Z"
    }
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T09:17:54.366210Z",
     "start_time": "2020-03-02T09:17:54.360389Z"
    }
   },
   "outputs": [],
   "source": [
    "session_length = 70\n",
    "batch_size = 512\n",
    "plot_num = 5000\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T09:17:54.420479Z",
     "start_time": "2020-03-02T09:17:54.367194Z"
    }
   },
   "outputs": [],
   "source": [
    "class SessionData(object):\n",
    "    def __init__(self,session_index,session_id,items_indexes):\n",
    "        self.session_index = session_index\n",
    "        self.session_id = session_id\n",
    "        self.item_list = items_indexes\n",
    "\n",
    "    def generate_seq_datas(self,session_length,padding_idx=0,predict_length=1):\n",
    "        sessions = []\n",
    "        if len(self.item_list)<2:\n",
    "            self.item_list.append[self.item_list[0]]\n",
    "        if predict_length==1:\n",
    "            # when session length>=3\n",
    "            for i in range(1,len(self.item_list)-1):\n",
    "#             # when session length >=2\n",
    "#             for i in range(len(self.item_list)-1):\n",
    "                if i <session_length:\n",
    "                    train_data = [0 for _ in range(session_length-i-1)]\n",
    "                    train_data.extend(self.item_list[:i+1])\n",
    "                    train_data.append(self.item_list[i+1])\n",
    "                else:\n",
    "                    train_data = self.item_list[i+1-session_length:i+1]\n",
    "                    train_data.append(self.item_list[i+1])\n",
    "                sessions.append(train_data)\n",
    "        else:\n",
    "\n",
    "            pass\n",
    "        return self.session_index,sessions\n",
    "    def __str__(self):\n",
    "        info = \" session index = {}\\n session id = {} \\n the length of item list= {} \\n the fisrt item index in item list is {}\".format(self.session_index,self.session_id,len(self.item_list),self.item_list[0])\n",
    "        return info\n",
    "class SessionDataSet(object):\n",
    "    def __init__(self,train_file,test_file,padding_idx=0):\n",
    "        super(SessionDataSet,self).__init__()\n",
    "        self.index_count = 0\n",
    "        self.session_count = 0\n",
    "        self.train_count = 0\n",
    "        self.test_count = 0\n",
    "        self.max_session_length = 0\n",
    "\n",
    "        self.padding_idx = padding_idx\n",
    "        self.item2index = dict()\n",
    "        self.index2item = dict()\n",
    "        self.session2index = dict()\n",
    "        self.index2session = dict()\n",
    "        self.item_total_num = dict()\n",
    "        self.item2index[\"<pad>\"] = padding_idx\n",
    "        self.index2item[padding_idx] = \"<pad>\"\n",
    "        self.train_data = self.load_data(train_file)\n",
    "        print(\"training set is loaded, # index: \",len(self.item2index.keys()))\n",
    "        self.train_count = self.session_count\n",
    "        print(\"train_session_num\",self.train_count)\n",
    "        self.test_data = self.load_data(test_file)\n",
    "        print(\"testing set is loaded, # index: \",len(self.index2item.keys()))\n",
    "        print(\"# item\",self.index_count)\n",
    "        self.test_count = self.session_count-self.train_count\n",
    "        print(\"# test session:\",self.test_count)\n",
    "        self.all_training_data = []\n",
    "        self.all_testing_data = []\n",
    "        self.all_meta_training_data = []\n",
    "        self.all_meta_testing_data = []\n",
    "        self.train_session_length = 0\n",
    "        self.test_session_length = 0\n",
    "    \n",
    "    def load_data(self,file_path):\n",
    "        data =  pickle.load(open(file_path, 'rb'))\n",
    "        session_ids = data[0]\n",
    "        session_data = data[1]\n",
    "        session_label = data[2]\n",
    "\n",
    "        result_data = []\n",
    "        lenth = len(session_ids)\n",
    "        print(\"# session\",lenth)\n",
    "\n",
    "        last_session_id = session_ids[0]\n",
    "        \n",
    "        session_item_indexes = []\n",
    "\n",
    "        for item_id in session_data[0]:\n",
    "            if item_id not in self.item2index.keys():\n",
    "                self.index_count+=1\n",
    "                self.item2index[item_id] = self.index_count\n",
    "                self.index2item[self.index_count] = item_id\n",
    "                self.item_total_num[self.index_count] = 0\n",
    "            session_item_indexes.append(self.item2index[item_id])\n",
    "            self.item_total_num[self.item2index[item_id]] += 1\n",
    "        target_item = session_label[0]\n",
    "        if target_item not in self.item2index.keys():\n",
    "            self.index_count+=1\n",
    "            self.item2index[target_item] = self.index_count\n",
    "            self.index2item[self.index_count] = target_item\n",
    "            self.item_total_num[self.index_count] = 0\n",
    "        session_item_indexes.append(self.item2index[target_item])\n",
    "        self.item_total_num[self.item2index[target_item]] += 1\n",
    "\n",
    "        for session_id,items,target_item in zip(session_ids,session_data,session_label):\n",
    "            if session_id!=last_session_id:\n",
    "\n",
    "                self.session_count+=1\n",
    "                self.session2index[last_session_id] = self.session_count\n",
    "                self.index2session[self.session_count] = last_session_id\n",
    "                last_session_id = session_id\n",
    "                if len(session_item_indexes)>self.max_session_length:\n",
    "                    self.max_session_length = len(session_item_indexes)\n",
    "                new_session = SessionData(self.session_count,last_session_id,session_item_indexes)\n",
    "                result_data.append(new_session)\n",
    "                session_item_indexes = []\n",
    "                for item_id in items:\n",
    "                    if item_id not in self.item2index.keys():\n",
    "                        self.index_count+=1\n",
    "                        self.item2index[item_id] = self.index_count\n",
    "                        self.index2item[self.index_count] = item_id\n",
    "                        self.item_total_num[self.index_count] = 0\n",
    "                    session_item_indexes.append(self.item2index[item_id])\n",
    "                    self.item_total_num[self.item2index[item_id]] += 1\n",
    "                if target_item not in self.item2index.keys():\n",
    "                    self.index_count+=1\n",
    "                    self.item2index[target_item] = self.index_count\n",
    "                    self.index2item[self.index_count] = target_item\n",
    "                    self.item_total_num[self.index_count] = 0\n",
    "                session_item_indexes.append(self.item2index[target_item])\n",
    "                self.item_total_num[self.item2index[target_item]] += 1\n",
    "            else:\n",
    "                # 每个id只处理一次\n",
    "                continue\n",
    "\n",
    "        self.session_count+=1\n",
    "        self.session2index[last_session_id] = self.session_count\n",
    "        new_session = SessionData(self.session_count,last_session_id,session_item_indexes)\n",
    "        result_data.append(new_session)\n",
    "        print(\"loaded\")\n",
    "        print(new_session)\n",
    "        \n",
    "        return result_data\n",
    "    \n",
    "\n",
    "    def get_batch(self,batch_size,session_length=10,predict_length=1,all_data=None,phase=\"train\",neg_num=1,sampling_mathod=\"random\"):\n",
    "\n",
    "        if phase == \"train\":\n",
    "            if all_data is None:\n",
    "                all_data = self.get_all_training_data(session_length)\n",
    "            indexes = np.random.permutation(all_data.shape[0])\n",
    "            all_data = all_data[indexes]\n",
    "        else:\n",
    "            if all_data is None:\n",
    "                all_data = self.get_all_testing_data(session_length)\n",
    "        \n",
    "        sindex = 0\n",
    "        eindex = batch_size\n",
    "        while eindex < all_data.shape[0]:\n",
    "            batch = all_data[sindex: eindex]\n",
    "\n",
    "            temp = eindex\n",
    "            eindex = eindex + batch_size\n",
    "            sindex = temp\n",
    "            if phase ==\"train\":\n",
    "                batch = self.divid_and_extend_negative_samples(batch,session_length=session_length,predict_length=predict_length,neg_num=neg_num,method=sampling_mathod)\n",
    "            else:\n",
    "                batch = [batch[:,:session_length],batch[:,session_length:]]\n",
    "            yield batch\n",
    "\n",
    "        if eindex >= all_data.shape[0]:\n",
    "            batch = all_data[sindex:]\n",
    "            if phase ==\"train\":\n",
    "                batch = self.divid_and_extend_negative_samples(batch,session_length=session_length,predict_length=predict_length,neg_num=neg_num,method=sampling_mathod)\n",
    "            else:\n",
    "                batch = [batch[:,:session_length],batch[:,session_length:]]\n",
    "            yield batch\n",
    "    \n",
    "    def get_batch_with_neg(self,batch_size,session_length=10,predict_length=1,all_data=None,phase=\"train\",neg_num=1,sampling_mathod=\"random\"):\n",
    "        if phase == \"train\":\n",
    "            all_data = self.get_all_training_data_with_neg(session_length,neg_num)\n",
    "            indexes = np.random.permutation(all_data.shape[0])\n",
    "            all_data = all_data[indexes]\n",
    "        else:\n",
    "            all_data = self.get_all_testing_data_with_neg(session_length,neg_num)\n",
    "        \n",
    "        sindex = 0\n",
    "        eindex = batch_size\n",
    "        while eindex < all_data.shape[0]:\n",
    "            batch = all_data[sindex: eindex]\n",
    "\n",
    "            temp = eindex\n",
    "            eindex = eindex + batch_size\n",
    "            sindex = temp\n",
    "            if phase ==\"train\":\n",
    "                batch = [batch[:,:session_length],batch[:,session_length:session_length+predict_length],batch[:,-neg_num:]]\n",
    "            else:\n",
    "                batch = [batch[:,:session_length],batch[:,session_length:]]\n",
    "            yield batch\n",
    "\n",
    "        if eindex >= all_data.shape[0]:\n",
    "            batch = all_data[sindex:]\n",
    "            if phase ==\"train\":\n",
    "                batch = [batch[:,:session_length],batch[:,session_length:session_length+predict_length],batch[:,-neg_num:]]\n",
    "            else:\n",
    "                batch = [batch[:,:session_length],batch[:,session_length:]]\n",
    "            yield batch\n",
    "    \n",
    "    def get_batch_tasks_with_neg(self,batch_size,session_length=10,predict_length=1,all_data=None,phase=\"train\",neg_num=1,sampling_mathod=\"random\"):\n",
    "        if phase == \"train\":\n",
    "            all_data = self.get_all_meta_training_data_with_neg(session_length,neg_num)\n",
    "            random.shuffle(all_data)\n",
    "        else:\n",
    "            all_data = self.get_all_meta_testing_data_with_neg(session_length,neg_num)\n",
    "        sindex = 0\n",
    "        eindex = batch_size\n",
    "        while eindex < len(all_data):\n",
    "            batch = all_data[sindex: eindex]\n",
    "\n",
    "            temp = eindex\n",
    "            eindex = eindex + batch_size\n",
    "            sindex = temp\n",
    "\n",
    "            session_items = [batch[i][:,:session_length] for i in range(len(batch))]\n",
    "\n",
    "            target_item = [batch[i][:,session_length:session_length+predict_length] for i in range(len(batch))]\n",
    "\n",
    "            neg_item = [batch[i][:,-neg_num:] for i in range(len(batch))]\n",
    "            batch = [session_items,target_item,neg_item]\n",
    "            yield batch\n",
    "\n",
    "        if eindex >= len(all_data):\n",
    "            batch = all_data[sindex:]\n",
    "            session_items = [batch[i][:,:session_length] for i in range(len(batch))]\n",
    "\n",
    "            target_item = [batch[i][:,session_length:session_length+predict_length] for i in range(len(batch))]\n",
    "\n",
    "            neg_item = [batch[i][:,-neg_num:] for i in range(len(batch))]\n",
    "            batch = [session_items,target_item,neg_item]\n",
    "            yield batch\n",
    "    \n",
    "    def divid_and_extend_negative_samples(self,batch_data,session_length,predict_length=1,neg_num=1,method=\"random\"):\n",
    "        \"\"\"\n",
    "        divid and extend negative samples\n",
    "        \"\"\"\n",
    "        neg_items = []\n",
    "        if method == \"random\":\n",
    "            for session_and_target in batch_data:\n",
    "                neg_item = []\n",
    "                for i in range(neg_num):\n",
    "                    rand_item = random.randint(1,self.index_count)\n",
    "                    while rand_item in session_and_target or rand_item in neg_item:\n",
    "                        rand_item = random.randint(1,self.index_count)\n",
    "                    neg_item.append(rand_item)\n",
    "                neg_items.append(neg_item)\n",
    "        else:\n",
    "\n",
    "            total_list = set()\n",
    "            for session in batch_data:\n",
    "                for i in session:\n",
    "                    total_list.add(i) \n",
    "            total_list = list(total_list)\n",
    "            total_list =  sorted(total_list, key=lambda item: self.item_total_num[item],reverse=True)\n",
    "            for i,session in enumerate(batch_data):\n",
    "                np.random.choice(total_list)\n",
    "        session_items = batch_data[:,:session_length]\n",
    "        target_item = batch_data[:,session_length:]\n",
    "        neg_items = np.array(neg_items)\n",
    "        return [session_items,target_item,neg_items]\n",
    "    \n",
    "    def get_all_training_data(self,session_length,predict_length=1):\n",
    "        if len(self.all_training_data)!=0 and self.train_session_length==session_length:\n",
    "#             print(\"The build is complete and there is no need to repeat the build\")\n",
    "            return self.all_training_data\n",
    "        print(\"Start building the all training dataset\")\n",
    "        all_sessions = []\n",
    "        for session_data in self.train_data:\n",
    "            session_index,sessions = session_data.generate_seq_datas(session_length,padding_idx=self.padding_idx)\n",
    "            if sessions is not None:\n",
    "                all_sessions.extend(sessions)\n",
    "        all_sessions = np.array(all_sessions)\n",
    "        self.all_training_data = all_sessions\n",
    "        self.train_session_length=session_length\n",
    "        print(\"The total number of training samples is：\",all_sessions.shape)\n",
    "        return all_sessions\n",
    "    \n",
    "    def get_all_testing_data(self,session_length,predict_length=1):\n",
    "        if len(self.all_testing_data)!=0 and self.test_session_length==session_length:\n",
    "            return self.all_testing_data\n",
    "        all_sessions = []\n",
    "        for session_data in self.test_data:\n",
    "            session_index,sessions = session_data.generate_seq_datas(session_length,padding_idx=self.padding_idx)\n",
    "            if sessions is not None:\n",
    "                all_sessions.extend(sessions)\n",
    "        all_sessions = np.array(all_sessions)\n",
    "        self.all_testing_data = all_sessions\n",
    "        self.test_session_length=session_length\n",
    "        print(\"The total number of testing samples is：\",all_sessions.shape)\n",
    "        return all_sessions\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T09:17:55.682599Z",
     "start_time": "2020-03-02T09:17:54.421683Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# session 526135\n",
      "loaded\n",
      " session index = 132501\n",
      " session id = 598664 \n",
      " the length of item list= 5 \n",
      " the fisrt item index in item list is 15612\n",
      "training set is loaded, # index:  40841\n",
      "train_session_num 132501\n",
      "# session 44279\n",
      "loaded\n",
      " session index = 143847\n",
      " session id = 600240 \n",
      " the length of item list= 4 \n",
      " the fisrt item index in item list is 2093\n",
      "testing set is loaded, # index:  40841\n",
      "# item 40840\n",
      "# test session: 11346\n"
     ]
    }
   ],
   "source": [
    "# dataset = SessionDataSet(train_file=\"../../data/retailrocket_gcsan_my/train.txt\",test_file=\"../../data/retailrocket_gcsan_my/test.txt\")\n",
    "dataset = SessionDataSet(train_file=\"../../data/diginetica_gcsan_my/train.txt\",test_file=\"../../data/diginetica_gcsan_my/test.txt\")\n",
    "# dataset = SessionDataSet(train_file=\"../../data/yoochoose1_4_gcsan_my/train.txt\",test_file=\"../../data/yoochoose1_4_gcsan_my/test.txt\")\n",
    "# dataset = SessionDataSet(train_file=\"../../data/yoochoose1_64_gcsan_my/train.txt\",test_file=\"../../data/yoochoose1_64_gcsan_my/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T09:17:55.691256Z",
     "start_time": "2020-03-02T09:17:55.683575Z"
    }
   },
   "outputs": [],
   "source": [
    "def bpr_loss(r):\n",
    "    return torch.sum(-torch.log(torch.sigmoid(r)))\n",
    "def get_hit_num(pred,y_truth):\n",
    "    \"\"\"\n",
    "        pred: numpy type(batch_size,k) \n",
    "        y_truth: list type (batch_size,groudtruth_num)\n",
    "    \"\"\"\n",
    "\n",
    "    hit_num = 0\n",
    "    for i in range(len(y_truth)):\n",
    "        for value in y_truth[i]:\n",
    "            hit_num += np.sum(pred[i]==value)\n",
    "    return hit_num\n",
    "\n",
    "def get_rr(pred,y_truth):\n",
    "    rr=0.\n",
    "    for i in range(len(y_truth)):\n",
    "        for value in y_truth[i]:\n",
    "            hit_indexes = np.where(pred[i]==value)[0]\n",
    "            for hit_index in hit_indexes:\n",
    "                rr += 1/(hit_index+1)\n",
    "    return rr\n",
    "\n",
    "def get_dcg(pred,y_truth):\n",
    "    y_pred_score = np.zeros_like(pred)\n",
    "\n",
    "    for i in range(len(y_truth)):\n",
    "\n",
    "        for j,y_pred in enumerate(pred[i]):\n",
    "            if y_pred == y_truth[i][0]:\n",
    "                y_pred_score[i][j]=1\n",
    "    gain = 2 ** y_pred_score - 1\n",
    "    discounts = np.tile(np.log2(np.arange(pred.shape[1]) + 2),(len(y_truth),1))\n",
    "    dcg = np.sum(gain / discounts,axis=1)\n",
    "    return dcg\n",
    "\n",
    "def get_ndcg(pred,y_truth):\n",
    "    dcg = get_dcg(pred, y_truth)\n",
    "    idcg = get_dcg(np.concatenate((y_truth,np.zeros_like(pred)[:,:-1]-1),axis=1), y_truth)\n",
    "    ndcg = np.sum(dcg / idcg)\n",
    "\n",
    "    return ndcg\n",
    "\n",
    "def dcg_score(y_pre, y_true, k):\n",
    "    y_pre_score = np.zeros(k)\n",
    "    if len(y_pre) > k:\n",
    "        y_pre = y_pre[:k]\n",
    "    for i in range(len(y_pre)):\n",
    "        pre_tag = y_pre[i]\n",
    "        if pre_tag in y_true:\n",
    "            y_pre_score[i] = 1\n",
    "    gain = 2 ** y_pre_score - 1\n",
    "    discounts = np.log2(np.arange(k) + 2)\n",
    "    return np.sum(gain / discounts)\n",
    "\n",
    "\n",
    "def ndcg_score(y_pre, y_true, k=5):\n",
    "    dcg = dcg_score(y_pre, y_true, k)\n",
    "    idcg = dcg_score(y_true, y_true, k)\n",
    "    return dcg / idcg\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T09:17:55.715451Z",
     "start_time": "2020-03-02T09:17:55.692209Z"
    }
   },
   "outputs": [],
   "source": [
    "# Attention Layer\n",
    "class Attention(torch.nn.Module):\n",
    "    def __init__(self, method=\"specific\", hidden_size=64):\n",
    "        super(Attention, self).__init__()\n",
    "        self.config = list()\n",
    "        self.method = method\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        if self.method == \"dot\":\n",
    "        \n",
    "            self.query = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "            self.key = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "\n",
    "        elif self.method == \"general\":\n",
    "            self.attention = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "\n",
    "        elif self.method == \"concat\":\n",
    "            self.attention = torch.nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "            self.v = torch.nn.Parameter(torch.FloatTensor(self.hidden_size))\n",
    "            \n",
    "        elif self.method == \"specific\":\n",
    "            self.W0 = torch.nn.Linear(self.hidden_size,1,bias=False)\n",
    "            torch.nn.init.normal_(self.W0.weight,0,0.05)\n",
    "            self.W1 = torch.nn.Linear(self.hidden_size,self.hidden_size,bias=False)\n",
    "            torch.nn.init.normal_(self.W1.weight,0,0.05)\n",
    "            self.W2 = torch.nn.Linear(self.hidden_size,self.hidden_size,bias=False)\n",
    "            torch.nn.init.normal_(self.W2.weight,0,0.05)\n",
    "            self.W3 = torch.nn.Linear(self.hidden_size,self.hidden_size,bias=False)\n",
    "            torch.nn.init.normal_(self.W3.weight,0,0.05)\n",
    "            self.b = torch.nn.Parameter(torch.FloatTensor(self.hidden_size))\n",
    "\n",
    "                               \n",
    "            \n",
    "\n",
    "    def dot_score(self, hidden, encoder_output,weights=None):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "\n",
    "    def general_score(self, hidden, encoder_output,weights=None):\n",
    "        energy = self.attention(encoder_output)\n",
    "        return torch.sum(hidden * energy, dim=2)\n",
    "\n",
    "\n",
    "    def concat_score(self, hidden, encoder_output,weights=None):\n",
    "        energy = self.attention(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
    "        return torch.sum(self.v * energy, dim=2)\n",
    "    \n",
    "\n",
    "    def specific_score(self,session,x_t,m_s,mask=None,weights=None):\n",
    "        if weights is None:\n",
    "            if mask is None:\n",
    "                W1Xi = self.W1(session)\n",
    "                W2Xt = self.W2(x_t).unsqueeze(1).repeat((1,session.shape[1],1))\n",
    "                W3Ms = self.W3(m_s).unsqueeze(1).repeat((1,session.shape[1],1))\n",
    "                energy = self.W0(torch.sigmoid(W1Xi+W2Xt+W3Ms+self.b)).repeat((1,1,session.shape[2]))\n",
    "    #         energy = energy*mask\n",
    "            else:\n",
    "    #         print(session.shape,x_t.shape,m_s.shape)\n",
    "                W1Xi = self.W1(session)*mask\n",
    "                W2Xt = self.W2(x_t).unsqueeze(1).repeat((1,session.shape[1],1))*mask\n",
    "                W3Ms = self.W3(m_s).unsqueeze(1).repeat((1,session.shape[1],1))*mask\n",
    "                energy = self.W0(torch.sigmoid(W1Xi+W2Xt+W3Ms+self.b)).repeat((1,1,session.shape[2]))*mask\n",
    "    #         energy = energy*mask\n",
    "        else:\n",
    "            key = 1\n",
    "            if mask is None:\n",
    "                W1Xi = torch.matmul(session,weights[key].t())\n",
    "                key +=1\n",
    "                W2Xt = (torch.matmul(x_t,weights[key].t())).unsqueeze(1).repeat((1,session.shape[1],1))\n",
    "                key +=1\n",
    "                W3Ms = (torch.matmul(x_t,weights[key].t())).unsqueeze(1).repeat((1,session.shape[1],1))\n",
    "                energy = torch.matmul(torch.sigmoid(W1Xi+W2Xt+W3Ms+weights[key+1]),weights[0].t()).repeat((1,1,session.shape[2]))\n",
    "    #         energy = energy*mask\n",
    "            else:\n",
    "    #         print(session.shape,x_t.shape,m_s.shape)\n",
    "                W1Xi = torch.matmul(session,weights[key].t())*mask\n",
    "                key +=1\n",
    "                W2Xt = (torch.matmul(x_t,weights[key].t())).unsqueeze(1).repeat((1,session.shape[1],1))*mask\n",
    "                key +=1\n",
    "                W3Ms = (torch.matmul(x_t,weights[key].t())).unsqueeze(1).repeat((1,session.shape[1],1))*mask\n",
    "                energy = torch.matmul(torch.sigmoid(W1Xi+W2Xt+W3Ms+weights[key+1]),weights[0].t()).repeat((1,1,session.shape[2]))*mask\n",
    "        return torch.sum(energy*session,dim=1)\n",
    "            \n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs=None,x_t=None,mask=None):\n",
    "\n",
    "        if self.method == \"general\":\n",
    "            attention_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == \"concat\":\n",
    "            attention_energies = self.concat_score(hidden, encoder_outputs)\n",
    "        elif self.method == \"dot\":\n",
    "            attention_energies = self.dot_score(hidden, encoder_outputs)\n",
    "        elif self.method == \"specific\":\n",
    "            session = hidden\n",
    "            m_s = encoder_outputs\n",
    "            return self.specific_score(session,x_t,m_s,mask)\n",
    "\n",
    "        attention_energies = attention_energies.t()\n",
    "\n",
    "        return F.softmax(attention_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T09:17:55.725958Z",
     "start_time": "2020-03-02T09:17:55.716415Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class STAMP(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=64, itemNum=0, posNum=0, padding_idx=0, dropout=0.5,\n",
    "                 activate=\"relu\"):\n",
    "        super(STAMP, self).__init__()\n",
    "        self.padding_idx = padding_idx\n",
    "        self.hidden_size = hidden_size\n",
    "        if activate == \"sigmoid\":\n",
    "            self.activate = torch.sigmoid\n",
    "        elif activate == \"tanh\":\n",
    "            self.activate = torch.tanh\n",
    "        else:\n",
    "            self.activate = torch.relu\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.item_embedding = torch.nn.Embedding(itemNum, hidden_size, padding_idx=self.padding_idx,max_norm=1.5)\n",
    "        torch.nn.init.normal_(self.item_embedding.weight,0,0.002)\n",
    "        torch.nn.init.constant_(self.item_embedding.weight[0],0)\n",
    "        self.attention = Attention(method=\"specific\",hidden_size=hidden_size)\n",
    "        self.left_mlp1 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        torch.nn.init.normal_(self.left_mlp1.weight,0,0.05)\n",
    "        torch.nn.init.constant_(self.left_mlp1.bias,0)\n",
    "        self.right_mlp1 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        torch.nn.init.normal_(self.left_mlp1.weight,0,0.05)\n",
    "        torch.nn.init.constant_(self.right_mlp1.bias,0)\n",
    "        torch.nn.utils.clip_grad_norm_(self.parameters(),max_norm=110)\n",
    "        \n",
    "    def forward(self, session):\n",
    " \n",
    "        mask = (session!=self.padding_idx).float()\n",
    "\n",
    "        length = torch.sum(mask,1).unsqueeze(1).repeat((1,self.hidden_size))\n",
    "\n",
    "        mask = mask.unsqueeze(2).repeat((1,1,self.hidden_size))\n",
    "        session_item_vecs = self.item_embedding(session) * mask\n",
    "        mean_session = torch.sum(session_item_vecs, dim=1)/length\n",
    "\n",
    "        compute_output = self.attention(session_item_vecs,mean_session,session_item_vecs[:,-1])\n",
    "        left_output = self.dropout(self.activate(self.left_mlp1(compute_output)))\n",
    "        right_output = self.dropout(self.activate(self.right_mlp1(session_item_vecs[:,-1])))\n",
    "\n",
    "        result = torch.matmul(left_output* right_output,self.item_embedding.weight[1:].t())\n",
    "\n",
    "        return result\n",
    "\n",
    "    def predict_top_k(self, session, k=20):\n",
    "\n",
    "        mask = (session!=0).float()\n",
    "\n",
    "        length = torch.sum(mask,1).unsqueeze(1).repeat((1,self.hidden_size))\n",
    "\n",
    "        mask = mask.unsqueeze(2).repeat((1,1,self.hidden_size))\n",
    "        session_item_vecs = self.item_embedding(session) * mask\n",
    "        mean_session = torch.sum(session_item_vecs, dim=1)/length\n",
    "        compute_output = self.attention(session_item_vecs,mean_session,session_item_vecs[:,-1])\n",
    "        left_output =self.activate(self.left_mlp1(compute_output))\n",
    "\n",
    "        right_output = self.activate(self.right_mlp1(session_item_vecs[:,-1]))\n",
    "\n",
    "        result = torch.matmul(left_output * right_output,self.item_embedding.weight[1:].t())\n",
    "\n",
    "        result = torch.topk(result,k,dim=1)[1]\n",
    "        return result\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIKM S >= 3   Total 462.8‬s Avg: 11.29s\n",
    "    HR@20=0.62560  MRR@20=0.28525, hyper-parameters: session_length=20, hidden_size=100, lr=0.0030, dropout=0.00\n",
    "        HR@1=0.17281  MRR@1=0.17281  NDCG@1=0.17281\n",
    "        HR@5=0.42029  MRR@5=0.26414  NDCG@5=0.30303\n",
    "        HR@10=0.52594  MRR@10=0.27827  NDCG@10=0.33722\n",
    "        HR@20=0.62560  MRR@20=0.28525  NDCG@20=0.36250\n",
    "# RR S >= 3   Total 107s Avg: 7.92s\n",
    "    HR@20=0.50324  MRR@20=0.25608,数session_length=20, hidden_size=100, lr=0.0030, dropout=0.00\n",
    "        HR@1=0.17638  MRR@1=0.17638  NDCG@1=0.17638\n",
    "        HR@5=0.35270  MRR@5=0.24038  NDCG@5=0.26829\n",
    "        HR@10=0.43180  MRR@10=0.25106  NDCG@10=0.29398\n",
    "        HR@20=0.50324  MRR@20=0.25608  NDCG@20=0.31212\n",
    "# RSC64 S >= 3   Total 102.1s Avg: 5.37s\n",
    "    HR@20=0.68391  MRR@20=0.27994, hyper-parameters: session_length=20, hidden_size=100, lr=0.0030, dropout=0.00\n",
    "        HR@1=0.15194  MRR@1=0.15194  NDCG@1=0.15194\n",
    "        HR@5=0.43741  MRR@5=0.25381  NDCG@5=0.29936\n",
    "        HR@10=0.57307  MRR@10=0.27212  NDCG@10=0.34343\n",
    "        HR@20=0.68391  MRR@20=0.27994  NDCG@20=0.37161\n",
    "# RSC4 S >= 3   Total 514.9s Avg: 57.21s\n",
    "    HR@20=0.70356  MRR@20=0.28365, hyper-parameters: session_length=20, hidden_size=100, lr=0.0010, dropout=0.00\n",
    "        HR@1=0.15298  MRR@1=0.15298  NDCG@1=0.15298\n",
    "        HR@5=0.44449  MRR@5=0.25628  NDCG@5=0.30293\n",
    "        HR@10=0.58688  MRR@10=0.27541  NDCG@10=0.34911\n",
    "        HR@20=0.70356  MRR@20=0.28365  NDCG@20=0.37878"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T09:17:55.738310Z",
     "start_time": "2020-03-02T09:17:55.726858Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs=50\n",
    "def train(args):\n",
    "    hidden_size = args[\"hidden_size\"] if \"hidden_size\" in args.keys() else 100\n",
    "    dropout = args[\"dropout\"] if \"dropout\" in args.keys()  else 0.5\n",
    "    lr = args[\"lr\"] if \"lr\" in args.keys()  else 3e-3\n",
    "    session_length = args[\"session_length\"] if \"session_length\" in args.keys() else 20\n",
    "    patience = args[\"patience\"] if \"patience\" in args.keys() else 50\n",
    "    model = STAMP(hidden_size=hidden_size, itemNum=dataset.index_count+1, posNum=11, padding_idx=0, dropout=dropout,\n",
    "                 activate=\"tanh\").to(device)\n",
    "    opti = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "\n",
    "    best_model_hr = 0.0\n",
    "    best_model_mrr = 0.0\n",
    "    best_r1m = 0.0\n",
    "    best_model = None\n",
    "    first_loss = 0.0\n",
    "    predict_nums = [1,5,10,20]\n",
    "    no_improvement_epoch = 0\n",
    "    start_train_time = datetime.datetime.now()\n",
    "    for epoch in range(epochs):\n",
    "        batch_losses = []\n",
    "        epoch_losses = []\n",
    "        for i,batch_data in enumerate(dataset.get_batch(batch_size,session_length,phase=\"train\")):\n",
    "            sessions = torch.tensor(batch_data[0]).to(device)\n",
    "            target_items = torch.tensor(batch_data[1]).squeeze().to(device)-1\n",
    "            result_pos = model(sessions)\n",
    "            loss = loss_function(result_pos,target_items)\n",
    "            opti.zero_grad()\n",
    "            loss.backward()\n",
    "            norm = torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm=110)\n",
    "#             print(norm)\n",
    "            opti.step()\n",
    "            batch_losses.append(loss.cpu().detach().numpy())\n",
    "            epoch_losses.append(loss.cpu().detach().numpy())\n",
    "            if i % plot_num == 0:\n",
    "                time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                print(\"[%s] [%d/%d] %d mean_batch_loss : %0.6f\" % (time, epoch+1, epochs, i, np.mean(batch_losses)))\n",
    "                batch_losses = []\n",
    "        with torch.no_grad():\n",
    "            start_test_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(\"Start predicting\",start_test_time)\n",
    "            rrs = [0 for _ in range(len(predict_nums))]\n",
    "            hit_nums = [0 for _ in range(len(predict_nums))]\n",
    "            ndcgs = [0 for _ in range(len(predict_nums))]\n",
    "            for i,batch_data in enumerate(dataset.get_batch(batch_size,session_length,phase=\"test\")):\n",
    "                sessions = torch.tensor(batch_data[0]).to(device)\n",
    "                target_items = np.array(batch_data[1])-1\n",
    "                y_pred = model.predict_top_k(sessions,20).cpu().numpy()\n",
    "#                 print(y_pred[:2],target_items[:2])\n",
    "                for j,predict_num in enumerate(predict_nums):\n",
    "                    hit_nums[j]+=get_hit_num(y_pred[:,:predict_num],target_items)\n",
    "                    rrs[j]+=get_rr(y_pred[:,:predict_num],target_items)\n",
    "                    ndcgs[j]+=get_ndcg(y_pred[:,:predict_num],target_items)\n",
    "                    \n",
    "            end_test_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            hrs = [hit_num/len(dataset.all_testing_data) for hit_num in hit_nums]\n",
    "            mrrs = [rr/len(dataset.all_testing_data) for rr in rrs]\n",
    "            mndcgs = [ndcg/len(dataset.all_testing_data) for ndcg in ndcgs]\n",
    "            if hrs[-1] + mrrs[-1] > best_r1m:\n",
    "                print(\"change best\")\n",
    "                best_model = deepcopy(model)\n",
    "                best_model_hr = hrs[-1]\n",
    "                best_model_mrr = mrrs[-1]\n",
    "                best_r1m = hrs[-1] + mrrs[-1]\n",
    "                no_improvement_epoch = 0\n",
    "            else:\n",
    "                no_improvement_epoch +=1\n",
    "            print(\"testing finish [%s] \"%end_test_time)\n",
    "            for k,predict_num in enumerate(predict_nums):\n",
    "                print(\"\\tHR@%d=%.5f  MRR@%d=%.5f  NDCG@%d=%.5f\"%(predict_num,hrs[k],predict_num,mrrs[k],predict_num,mndcgs[k]))\n",
    "        if no_improvement_epoch>=patience:\n",
    "            print(\"early stopping\")\n",
    "            break\n",
    "    end_train_time = datetime.datetime.now()\n",
    "    print(\"training and testting over, Total time\",end_train_time-start_train_time)\n",
    "    return best_model,best_model_hr,best_model_mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-02T09:36:15.478350Z",
     "start_time": "2020-03-02T09:17:55.739195Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current model hyper-parameters: session_length=20, hidden_size=100, lr=0.0010, dropout=0.00\n",
      "\n",
      "Start building the all training dataset\n",
      "The total number of training samples is： (526135, 21)\n",
      "[2020-03-02 17:18:01] [1/50] 0 mean_batch_loss : 10.617420\n",
      "Start predicting 2020-03-02 17:18:26\n",
      "The total number of testing samples is： (44279, 21)\n",
      "change best\n",
      "testing finish [2020-03-02 17:18:29] \n",
      "\tHR@1=0.00946  MRR@1=0.00946  NDCG@1=0.00946\n",
      "\tHR@5=0.03627  MRR@5=0.01833  NDCG@5=0.02274\n",
      "\tHR@10=0.05933  MRR@10=0.02137  NDCG@10=0.03016\n",
      "\tHR@20=0.08927  MRR@20=0.02342  NDCG@20=0.03770\n",
      "[2020-03-02 17:18:29] [2/50] 0 mean_batch_loss : 8.475081\n",
      "Start predicting 2020-03-02 17:18:56\n",
      "change best\n",
      "testing finish [2020-03-02 17:18:59] \n",
      "\tHR@1=0.05192  MRR@1=0.05192  NDCG@1=0.05192\n",
      "\tHR@5=0.15664  MRR@5=0.08842  NDCG@5=0.10527\n",
      "\tHR@10=0.22121  MRR@10=0.09699  NDCG@10=0.12611\n",
      "\tHR@20=0.29045  MRR@20=0.10177  NDCG@20=0.14359\n",
      "[2020-03-02 17:18:59] [3/50] 0 mean_batch_loss : 6.797451\n",
      "Start predicting 2020-03-02 17:19:27\n",
      "change best\n",
      "testing finish [2020-03-02 17:19:31] \n",
      "\tHR@1=0.08812  MRR@1=0.08812  NDCG@1=0.08812\n",
      "\tHR@5=0.24242  MRR@5=0.14245  NDCG@5=0.16720\n",
      "\tHR@10=0.32894  MRR@10=0.15400  NDCG@10=0.19518\n",
      "\tHR@20=0.42018  MRR@20=0.16036  NDCG@20=0.21829\n",
      "[2020-03-02 17:19:31] [4/50] 0 mean_batch_loss : 5.662708\n",
      "Start predicting 2020-03-02 17:19:56\n",
      "change best\n",
      "testing finish [2020-03-02 17:19:59] \n",
      "\tHR@1=0.11064  MRR@1=0.11064  NDCG@1=0.11064\n",
      "\tHR@5=0.29251  MRR@5=0.17570  NDCG@5=0.20467\n",
      "\tHR@10=0.38793  MRR@10=0.18834  NDCG@10=0.23544\n",
      "\tHR@20=0.48314  MRR@20=0.19498  NDCG@20=0.25955\n",
      "[2020-03-02 17:19:59] [5/50] 0 mean_batch_loss : 5.091112\n",
      "Start predicting 2020-03-02 17:20:24\n",
      "change best\n",
      "testing finish [2020-03-02 17:20:27] \n",
      "\tHR@1=0.12326  MRR@1=0.12326  NDCG@1=0.12326\n",
      "\tHR@5=0.32060  MRR@5=0.19412  NDCG@5=0.22551\n",
      "\tHR@10=0.42090  MRR@10=0.20750  NDCG@10=0.25794\n",
      "\tHR@20=0.52160  MRR@20=0.21450  NDCG@20=0.28342\n",
      "[2020-03-02 17:20:27] [6/50] 0 mean_batch_loss : 4.623771\n",
      "Start predicting 2020-03-02 17:20:52\n",
      "change best\n",
      "testing finish [2020-03-02 17:20:55] \n",
      "\tHR@1=0.13024  MRR@1=0.13024  NDCG@1=0.13024\n",
      "\tHR@5=0.33887  MRR@5=0.20568  NDCG@5=0.23876\n",
      "\tHR@10=0.44141  MRR@10=0.21937  NDCG@10=0.27192\n",
      "\tHR@20=0.54437  MRR@20=0.22652  NDCG@20=0.29796\n",
      "[2020-03-02 17:20:55] [7/50] 0 mean_batch_loss : 4.213615\n",
      "Start predicting 2020-03-02 17:21:24\n",
      "change best\n",
      "testing finish [2020-03-02 17:21:27] \n",
      "\tHR@1=0.13794  MRR@1=0.13794  NDCG@1=0.13794\n",
      "\tHR@5=0.35319  MRR@5=0.21558  NDCG@5=0.24975\n",
      "\tHR@10=0.45719  MRR@10=0.22949  NDCG@10=0.28342\n",
      "\tHR@20=0.55828  MRR@20=0.23655  NDCG@20=0.30903\n",
      "[2020-03-02 17:21:27] [8/50] 0 mean_batch_loss : 4.280944\n",
      "Start predicting 2020-03-02 17:21:52\n",
      "change best\n",
      "testing finish [2020-03-02 17:21:55] \n",
      "\tHR@1=0.14465  MRR@1=0.14465  NDCG@1=0.14465\n",
      "\tHR@5=0.36218  MRR@5=0.22344  NDCG@5=0.25791\n",
      "\tHR@10=0.46636  MRR@10=0.23744  NDCG@10=0.29169\n",
      "\tHR@20=0.56659  MRR@20=0.24443  NDCG@20=0.31708\n",
      "[2020-03-02 17:21:55] [9/50] 0 mean_batch_loss : 4.407105\n",
      "Start predicting 2020-03-02 17:22:20\n",
      "change best\n",
      "testing finish [2020-03-02 17:22:24] \n",
      "\tHR@1=0.14646  MRR@1=0.14646  NDCG@1=0.14646\n",
      "\tHR@5=0.36945  MRR@5=0.22736  NDCG@5=0.26266\n",
      "\tHR@10=0.47535  MRR@10=0.24145  NDCG@10=0.29687\n",
      "\tHR@20=0.57454  MRR@20=0.24836  NDCG@20=0.32198\n",
      "[2020-03-02 17:22:24] [10/50] 0 mean_batch_loss : 3.915933\n",
      "Start predicting 2020-03-02 17:22:47\n",
      "change best\n",
      "testing finish [2020-03-02 17:22:51] \n",
      "\tHR@1=0.14905  MRR@1=0.14905  NDCG@1=0.14905\n",
      "\tHR@5=0.37700  MRR@5=0.23199  NDCG@5=0.26804\n",
      "\tHR@10=0.48014  MRR@10=0.24585  NDCG@10=0.30149\n",
      "\tHR@20=0.58052  MRR@20=0.25284  NDCG@20=0.32691\n",
      "[2020-03-02 17:22:51] [11/50] 0 mean_batch_loss : 3.800966\n",
      "Start predicting 2020-03-02 17:23:19\n",
      "change best\n",
      "testing finish [2020-03-02 17:23:22] \n",
      "\tHR@1=0.15317  MRR@1=0.15317  NDCG@1=0.15317\n",
      "\tHR@5=0.37991  MRR@5=0.23564  NDCG@5=0.27151\n",
      "\tHR@10=0.48208  MRR@10=0.24933  NDCG@10=0.30460\n",
      "\tHR@20=0.58265  MRR@20=0.25633  NDCG@20=0.33005\n",
      "[2020-03-02 17:23:22] [12/50] 0 mean_batch_loss : 3.720266\n",
      "Start predicting 2020-03-02 17:23:51\n",
      "change best\n",
      "testing finish [2020-03-02 17:23:54] \n",
      "\tHR@1=0.15461  MRR@1=0.15461  NDCG@1=0.15461\n",
      "\tHR@5=0.38533  MRR@5=0.23858  NDCG@5=0.27506\n",
      "\tHR@10=0.48822  MRR@10=0.25235  NDCG@10=0.30838\n",
      "\tHR@20=0.58703  MRR@20=0.25923  NDCG@20=0.33338\n",
      "[2020-03-02 17:23:54] [13/50] 0 mean_batch_loss : 3.616432\n",
      "Start predicting 2020-03-02 17:24:19\n",
      "change best\n",
      "testing finish [2020-03-02 17:24:22] \n",
      "\tHR@1=0.15617  MRR@1=0.15617  NDCG@1=0.15617\n",
      "\tHR@5=0.39061  MRR@5=0.24164  NDCG@5=0.27868\n",
      "\tHR@10=0.49053  MRR@10=0.25501  NDCG@10=0.31103\n",
      "\tHR@20=0.59123  MRR@20=0.26206  NDCG@20=0.33656\n",
      "[2020-03-02 17:24:22] [14/50] 0 mean_batch_loss : 3.716746\n",
      "Start predicting 2020-03-02 17:24:47\n",
      "change best\n",
      "testing finish [2020-03-02 17:24:50] \n",
      "\tHR@1=0.15696  MRR@1=0.15696  NDCG@1=0.15696\n",
      "\tHR@5=0.39052  MRR@5=0.24250  NDCG@5=0.27934\n",
      "\tHR@10=0.49470  MRR@10=0.25641  NDCG@10=0.31302\n",
      "\tHR@20=0.59215  MRR@20=0.26321  NDCG@20=0.33771\n",
      "[2020-03-02 17:24:50] [15/50] 0 mean_batch_loss : 3.534269\n",
      "Start predicting 2020-03-02 17:25:16\n",
      "change best\n",
      "testing finish [2020-03-02 17:25:19] \n",
      "\tHR@1=0.16082  MRR@1=0.16082  NDCG@1=0.16082\n",
      "\tHR@5=0.39271  MRR@5=0.24560  NDCG@5=0.28220\n",
      "\tHR@10=0.49766  MRR@10=0.25969  NDCG@10=0.31622\n",
      "\tHR@20=0.59663  MRR@20=0.26660  NDCG@20=0.34129\n",
      "[2020-03-02 17:25:19] [16/50] 0 mean_batch_loss : 3.581925\n",
      "Start predicting 2020-03-02 17:25:47\n",
      "testing finish [2020-03-02 17:25:50] \n",
      "\tHR@1=0.15910  MRR@1=0.15910  NDCG@1=0.15910\n",
      "\tHR@5=0.39488  MRR@5=0.24520  NDCG@5=0.28243\n",
      "\tHR@10=0.49619  MRR@10=0.25874  NDCG@10=0.31522\n",
      "\tHR@20=0.59568  MRR@20=0.26568  NDCG@20=0.34041\n",
      "[2020-03-02 17:25:50] [17/50] 0 mean_batch_loss : 3.533472\n",
      "Start predicting 2020-03-02 17:26:17\n",
      "change best\n",
      "testing finish [2020-03-02 17:26:20] \n",
      "\tHR@1=0.16202  MRR@1=0.16202  NDCG@1=0.16202\n",
      "\tHR@5=0.39538  MRR@5=0.24736  NDCG@5=0.28419\n",
      "\tHR@10=0.49992  MRR@10=0.26126  NDCG@10=0.31795\n",
      "\tHR@20=0.59805  MRR@20=0.26811  NDCG@20=0.34281\n",
      "[2020-03-02 17:26:21] [18/50] 0 mean_batch_loss : 3.479535\n",
      "Start predicting 2020-03-02 17:26:46\n",
      "change best\n",
      "testing finish [2020-03-02 17:26:49] \n",
      "\tHR@1=0.16179  MRR@1=0.16179  NDCG@1=0.16179\n",
      "\tHR@5=0.39687  MRR@5=0.24801  NDCG@5=0.28505\n",
      "\tHR@10=0.50119  MRR@10=0.26201  NDCG@10=0.31887\n",
      "\tHR@20=0.59771  MRR@20=0.26875  NDCG@20=0.34332\n",
      "[2020-03-02 17:26:49] [19/50] 0 mean_batch_loss : 3.413366\n",
      "Start predicting 2020-03-02 17:27:14\n",
      "change best\n",
      "testing finish [2020-03-02 17:27:17] \n",
      "\tHR@1=0.16322  MRR@1=0.16322  NDCG@1=0.16322\n",
      "\tHR@5=0.39972  MRR@5=0.25007  NDCG@5=0.28731\n",
      "\tHR@10=0.50231  MRR@10=0.26389  NDCG@10=0.32062\n",
      "\tHR@20=0.59922  MRR@20=0.27067  NDCG@20=0.34518\n",
      "[2020-03-02 17:27:17] [20/50] 0 mean_batch_loss : 3.428917\n",
      "Start predicting 2020-03-02 17:27:44\n",
      "testing finish [2020-03-02 17:27:47] \n",
      "\tHR@1=0.16191  MRR@1=0.16191  NDCG@1=0.16191\n",
      "\tHR@5=0.39922  MRR@5=0.24879  NDCG@5=0.28623\n",
      "\tHR@10=0.50308  MRR@10=0.26269  NDCG@10=0.31985\n",
      "\tHR@20=0.60017  MRR@20=0.26947  NDCG@20=0.34445\n",
      "[2020-03-02 17:27:47] [21/50] 0 mean_batch_loss : 3.386730\n",
      "Start predicting 2020-03-02 17:28:15\n",
      "change best\n",
      "testing finish [2020-03-02 17:28:18] \n",
      "\tHR@1=0.16430  MRR@1=0.16430  NDCG@1=0.16430\n",
      "\tHR@5=0.40166  MRR@5=0.25138  NDCG@5=0.28879\n",
      "\tHR@10=0.50505  MRR@10=0.26519  NDCG@10=0.32223\n",
      "\tHR@20=0.60180  MRR@20=0.27195  NDCG@20=0.34676\n",
      "[2020-03-02 17:28:19] [22/50] 0 mean_batch_loss : 3.376489\n",
      "Start predicting 2020-03-02 17:28:43\n",
      "change best\n",
      "testing finish [2020-03-02 17:28:47] \n",
      "\tHR@1=0.16423  MRR@1=0.16423  NDCG@1=0.16423\n",
      "\tHR@5=0.40304  MRR@5=0.25165  NDCG@5=0.28932\n",
      "\tHR@10=0.50329  MRR@10=0.26505  NDCG@10=0.32175\n",
      "\tHR@20=0.60261  MRR@20=0.27199  NDCG@20=0.34692\n",
      "[2020-03-02 17:28:47] [23/50] 0 mean_batch_loss : 3.296918\n",
      "Start predicting 2020-03-02 17:29:12\n",
      "change best\n",
      "testing finish [2020-03-02 17:29:16] \n",
      "\tHR@1=0.16452  MRR@1=0.16452  NDCG@1=0.16452\n",
      "\tHR@5=0.40328  MRR@5=0.25164  NDCG@5=0.28935\n",
      "\tHR@10=0.50557  MRR@10=0.26536  NDCG@10=0.32250\n",
      "\tHR@20=0.60259  MRR@20=0.27213  NDCG@20=0.34708\n",
      "[2020-03-02 17:29:16] [24/50] 0 mean_batch_loss : 3.396438\n",
      "Start predicting 2020-03-02 17:29:40\n",
      "change best\n",
      "testing finish [2020-03-02 17:29:43] \n",
      "\tHR@1=0.16486  MRR@1=0.16486  NDCG@1=0.16486\n",
      "\tHR@5=0.40419  MRR@5=0.25253  NDCG@5=0.29026\n",
      "\tHR@10=0.50676  MRR@10=0.26630  NDCG@10=0.32352\n",
      "\tHR@20=0.60388  MRR@20=0.27310  NDCG@20=0.34814\n",
      "[2020-03-02 17:29:43] [25/50] 0 mean_batch_loss : 3.357355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting 2020-03-02 17:29:55\n",
      "change best\n",
      "testing finish [2020-03-02 17:29:58] \n",
      "\tHR@1=0.16500  MRR@1=0.16500  NDCG@1=0.16500\n",
      "\tHR@5=0.40374  MRR@5=0.25268  NDCG@5=0.29028\n",
      "\tHR@10=0.50703  MRR@10=0.26651  NDCG@10=0.32374\n",
      "\tHR@20=0.60392  MRR@20=0.27329  NDCG@20=0.34830\n",
      "[2020-03-02 17:29:58] [26/50] 0 mean_batch_loss : 3.396304\n",
      "Start predicting 2020-03-02 17:30:10\n",
      "change best\n",
      "testing finish [2020-03-02 17:30:13] \n",
      "\tHR@1=0.16602  MRR@1=0.16602  NDCG@1=0.16602\n",
      "\tHR@5=0.40502  MRR@5=0.25321  NDCG@5=0.29096\n",
      "\tHR@10=0.50697  MRR@10=0.26690  NDCG@10=0.32401\n",
      "\tHR@20=0.60489  MRR@20=0.27377  NDCG@20=0.34886\n",
      "[2020-03-02 17:30:13] [27/50] 0 mean_batch_loss : 3.286312\n",
      "Start predicting 2020-03-02 17:30:25\n",
      "change best\n",
      "testing finish [2020-03-02 17:30:28] \n",
      "\tHR@1=0.16513  MRR@1=0.16513  NDCG@1=0.16513\n",
      "\tHR@5=0.40439  MRR@5=0.25359  NDCG@5=0.29115\n",
      "\tHR@10=0.50877  MRR@10=0.26759  NDCG@10=0.32498\n",
      "\tHR@20=0.60525  MRR@20=0.27437  NDCG@20=0.34948\n",
      "[2020-03-02 17:30:28] [28/50] 0 mean_batch_loss : 3.254390\n",
      "Start predicting 2020-03-02 17:30:40\n",
      "change best\n",
      "testing finish [2020-03-02 17:30:43] \n",
      "\tHR@1=0.16552  MRR@1=0.16552  NDCG@1=0.16552\n",
      "\tHR@5=0.40566  MRR@5=0.25331  NDCG@5=0.29120\n",
      "\tHR@10=0.50837  MRR@10=0.26707  NDCG@10=0.32448\n",
      "\tHR@20=0.60580  MRR@20=0.27388  NDCG@20=0.34917\n",
      "[2020-03-02 17:30:43] [29/50] 0 mean_batch_loss : 3.341909\n",
      "Start predicting 2020-03-02 17:30:56\n",
      "change best\n",
      "testing finish [2020-03-02 17:30:59] \n",
      "\tHR@1=0.16425  MRR@1=0.16425  NDCG@1=0.16425\n",
      "\tHR@5=0.40778  MRR@5=0.25376  NDCG@5=0.29210\n",
      "\tHR@10=0.51085  MRR@10=0.26763  NDCG@10=0.32554\n",
      "\tHR@20=0.60715  MRR@20=0.27435  NDCG@20=0.34994\n",
      "[2020-03-02 17:30:59] [30/50] 0 mean_batch_loss : 3.420088\n",
      "Start predicting 2020-03-02 17:31:11\n",
      "change best\n",
      "testing finish [2020-03-02 17:31:14] \n",
      "\tHR@1=0.16665  MRR@1=0.16665  NDCG@1=0.16665\n",
      "\tHR@5=0.40667  MRR@5=0.25473  NDCG@5=0.29255\n",
      "\tHR@10=0.51189  MRR@10=0.26884  NDCG@10=0.32664\n",
      "\tHR@20=0.60681  MRR@20=0.27546  NDCG@20=0.35068\n",
      "[2020-03-02 17:31:14] [31/50] 0 mean_batch_loss : 3.317024\n",
      "Start predicting 2020-03-02 17:31:26\n",
      "testing finish [2020-03-02 17:31:29] \n",
      "\tHR@1=0.16441  MRR@1=0.16441  NDCG@1=0.16441\n",
      "\tHR@5=0.40827  MRR@5=0.25373  NDCG@5=0.29218\n",
      "\tHR@10=0.50981  MRR@10=0.26733  NDCG@10=0.32507\n",
      "\tHR@20=0.60692  MRR@20=0.27410  NDCG@20=0.34965\n",
      "[2020-03-02 17:31:29] [32/50] 0 mean_batch_loss : 3.293956\n",
      "Start predicting 2020-03-02 17:31:41\n",
      "testing finish [2020-03-02 17:31:44] \n",
      "\tHR@1=0.16545  MRR@1=0.16545  NDCG@1=0.16545\n",
      "\tHR@5=0.40861  MRR@5=0.25440  NDCG@5=0.29276\n",
      "\tHR@10=0.51142  MRR@10=0.26815  NDCG@10=0.32603\n",
      "\tHR@20=0.60726  MRR@20=0.27483  NDCG@20=0.35030\n",
      "[2020-03-02 17:31:44] [33/50] 0 mean_batch_loss : 3.203602\n",
      "Start predicting 2020-03-02 17:31:57\n",
      "testing finish [2020-03-02 17:32:00] \n",
      "\tHR@1=0.16477  MRR@1=0.16477  NDCG@1=0.16477\n",
      "\tHR@5=0.40608  MRR@5=0.25292  NDCG@5=0.29102\n",
      "\tHR@10=0.51074  MRR@10=0.26694  NDCG@10=0.32491\n",
      "\tHR@20=0.60747  MRR@20=0.27372  NDCG@20=0.34945\n",
      "[2020-03-02 17:32:00] [34/50] 0 mean_batch_loss : 3.298420\n",
      "Start predicting 2020-03-02 17:32:11\n",
      "testing finish [2020-03-02 17:32:14] \n",
      "\tHR@1=0.16676  MRR@1=0.16676  NDCG@1=0.16676\n",
      "\tHR@5=0.40561  MRR@5=0.25437  NDCG@5=0.29202\n",
      "\tHR@10=0.50868  MRR@10=0.26824  NDCG@10=0.32547\n",
      "\tHR@20=0.60584  MRR@20=0.27505  NDCG@20=0.35012\n",
      "[2020-03-02 17:32:14] [35/50] 0 mean_batch_loss : 3.126940\n",
      "Start predicting 2020-03-02 17:32:26\n",
      "change best\n",
      "testing finish [2020-03-02 17:32:29] \n",
      "\tHR@1=0.16581  MRR@1=0.16581  NDCG@1=0.16581\n",
      "\tHR@5=0.40970  MRR@5=0.25520  NDCG@5=0.29364\n",
      "\tHR@10=0.51185  MRR@10=0.26895  NDCG@10=0.32679\n",
      "\tHR@20=0.60785  MRR@20=0.27563  NDCG@20=0.35109\n",
      "[2020-03-02 17:32:29] [36/50] 0 mean_batch_loss : 3.306530\n",
      "Start predicting 2020-03-02 17:32:41\n",
      "change best\n",
      "testing finish [2020-03-02 17:32:44] \n",
      "\tHR@1=0.16724  MRR@1=0.16724  NDCG@1=0.16724\n",
      "\tHR@5=0.40918  MRR@5=0.25579  NDCG@5=0.29395\n",
      "\tHR@10=0.51185  MRR@10=0.26954  NDCG@10=0.32720\n",
      "\tHR@20=0.60774  MRR@20=0.27623  NDCG@20=0.35150\n",
      "[2020-03-02 17:32:44] [37/50] 0 mean_batch_loss : 3.174596\n",
      "Start predicting 2020-03-02 17:32:56\n",
      "change best\n",
      "testing finish [2020-03-02 17:32:59] \n",
      "\tHR@1=0.16769  MRR@1=0.16769  NDCG@1=0.16769\n",
      "\tHR@5=0.40832  MRR@5=0.25594  NDCG@5=0.29387\n",
      "\tHR@10=0.51083  MRR@10=0.26964  NDCG@10=0.32704\n",
      "\tHR@20=0.60853  MRR@20=0.27645  NDCG@20=0.35177\n",
      "[2020-03-02 17:32:59] [38/50] 0 mean_batch_loss : 3.135378\n",
      "Start predicting 2020-03-02 17:33:11\n",
      "change best\n",
      "testing finish [2020-03-02 17:33:14] \n",
      "\tHR@1=0.16649  MRR@1=0.16649  NDCG@1=0.16649\n",
      "\tHR@5=0.40873  MRR@5=0.25553  NDCG@5=0.29366\n",
      "\tHR@10=0.51248  MRR@10=0.26935  NDCG@10=0.32720\n",
      "\tHR@20=0.60930  MRR@20=0.27614  NDCG@20=0.35175\n",
      "[2020-03-02 17:33:14] [39/50] 0 mean_batch_loss : 3.241427\n",
      "Start predicting 2020-03-02 17:33:26\n",
      "testing finish [2020-03-02 17:33:29] \n",
      "\tHR@1=0.16550  MRR@1=0.16550  NDCG@1=0.16550\n",
      "\tHR@5=0.40852  MRR@5=0.25449  NDCG@5=0.29282\n",
      "\tHR@10=0.51313  MRR@10=0.26856  NDCG@10=0.32676\n",
      "\tHR@20=0.60948  MRR@20=0.27527  NDCG@20=0.35115\n",
      "[2020-03-02 17:33:29] [40/50] 0 mean_batch_loss : 3.235632\n",
      "Start predicting 2020-03-02 17:33:41\n",
      "testing finish [2020-03-02 17:33:44] \n",
      "\tHR@1=0.16523  MRR@1=0.16523  NDCG@1=0.16523\n",
      "\tHR@5=0.40785  MRR@5=0.25427  NDCG@5=0.29249\n",
      "\tHR@10=0.51185  MRR@10=0.26820  NDCG@10=0.32617\n",
      "\tHR@20=0.60837  MRR@20=0.27491  NDCG@20=0.35058\n",
      "[2020-03-02 17:33:44] [41/50] 0 mean_batch_loss : 3.133757\n",
      "Start predicting 2020-03-02 17:33:56\n",
      "change best\n",
      "testing finish [2020-03-02 17:33:59] \n",
      "\tHR@1=0.16701  MRR@1=0.16701  NDCG@1=0.16701\n",
      "\tHR@5=0.40968  MRR@5=0.25583  NDCG@5=0.29411\n",
      "\tHR@10=0.51191  MRR@10=0.26954  NDCG@10=0.32724\n",
      "\tHR@20=0.60927  MRR@20=0.27634  NDCG@20=0.35190\n",
      "[2020-03-02 17:33:59] [42/50] 0 mean_batch_loss : 3.163366\n",
      "Start predicting 2020-03-02 17:34:12\n",
      "testing finish [2020-03-02 17:34:15] \n",
      "\tHR@1=0.16604  MRR@1=0.16604  NDCG@1=0.16604\n",
      "\tHR@5=0.40825  MRR@5=0.25527  NDCG@5=0.29337\n",
      "\tHR@10=0.51225  MRR@10=0.26919  NDCG@10=0.32705\n",
      "\tHR@20=0.60871  MRR@20=0.27595  NDCG@20=0.35151\n",
      "[2020-03-02 17:34:15] [43/50] 0 mean_batch_loss : 3.042148\n",
      "Start predicting 2020-03-02 17:34:27\n",
      "testing finish [2020-03-02 17:34:30] \n",
      "\tHR@1=0.16529  MRR@1=0.16529  NDCG@1=0.16529\n",
      "\tHR@5=0.40925  MRR@5=0.25470  NDCG@5=0.29316\n",
      "\tHR@10=0.51270  MRR@10=0.26855  NDCG@10=0.32665\n",
      "\tHR@20=0.60855  MRR@20=0.27526  NDCG@20=0.35096\n",
      "[2020-03-02 17:34:30] [44/50] 0 mean_batch_loss : 3.387597\n",
      "Start predicting 2020-03-02 17:34:42\n",
      "change best\n",
      "testing finish [2020-03-02 17:34:45] \n",
      "\tHR@1=0.16692  MRR@1=0.16692  NDCG@1=0.16692\n",
      "\tHR@5=0.40818  MRR@5=0.25590  NDCG@5=0.29383\n",
      "\tHR@10=0.51426  MRR@10=0.27013  NDCG@10=0.32819\n",
      "\tHR@20=0.60916  MRR@20=0.27671  NDCG@20=0.35219\n",
      "[2020-03-02 17:34:45] [45/50] 0 mean_batch_loss : 3.130150\n",
      "Start predicting 2020-03-02 17:34:57\n",
      "testing finish [2020-03-02 17:35:00] \n",
      "\tHR@1=0.16608  MRR@1=0.16608  NDCG@1=0.16608\n",
      "\tHR@5=0.40798  MRR@5=0.25541  NDCG@5=0.29342\n",
      "\tHR@10=0.51214  MRR@10=0.26937  NDCG@10=0.32715\n",
      "\tHR@20=0.60963  MRR@20=0.27617  NDCG@20=0.35185\n",
      "[2020-03-02 17:35:00] [46/50] 0 mean_batch_loss : 3.123791\n",
      "Start predicting 2020-03-02 17:35:11\n",
      "testing finish [2020-03-02 17:35:14] \n",
      "\tHR@1=0.16739  MRR@1=0.16739  NDCG@1=0.16739\n",
      "\tHR@5=0.41040  MRR@5=0.25651  NDCG@5=0.29480\n",
      "\tHR@10=0.51334  MRR@10=0.27027  NDCG@10=0.32811\n",
      "\tHR@20=0.60851  MRR@20=0.27690  NDCG@20=0.35221\n",
      "[2020-03-02 17:35:14] [47/50] 0 mean_batch_loss : 3.172180\n",
      "Start predicting 2020-03-02 17:35:26\n",
      "change best\n",
      "testing finish [2020-03-02 17:35:29] \n",
      "\tHR@1=0.16644  MRR@1=0.16644  NDCG@1=0.16644\n",
      "\tHR@5=0.40834  MRR@5=0.25541  NDCG@5=0.29347\n",
      "\tHR@10=0.51334  MRR@10=0.26951  NDCG@10=0.32752\n",
      "\tHR@20=0.61099  MRR@20=0.27634  NDCG@20=0.35228\n",
      "[2020-03-02 17:35:29] [48/50] 0 mean_batch_loss : 3.136106\n",
      "Start predicting 2020-03-02 17:35:42\n",
      "testing finish [2020-03-02 17:35:45] \n",
      "\tHR@1=0.16606  MRR@1=0.16606  NDCG@1=0.16606\n",
      "\tHR@5=0.40706  MRR@5=0.25456  NDCG@5=0.29252\n",
      "\tHR@10=0.51311  MRR@10=0.26879  NDCG@10=0.32689\n",
      "\tHR@20=0.61108  MRR@20=0.27564  NDCG@20=0.35173\n",
      "[2020-03-02 17:35:45] [49/50] 0 mean_batch_loss : 3.180702\n",
      "Start predicting 2020-03-02 17:35:57\n",
      "testing finish [2020-03-02 17:36:00] \n",
      "\tHR@1=0.16769  MRR@1=0.16769  NDCG@1=0.16769\n",
      "\tHR@5=0.40952  MRR@5=0.25630  NDCG@5=0.29443\n",
      "\tHR@10=0.51279  MRR@10=0.27015  NDCG@10=0.32789\n",
      "\tHR@20=0.61018  MRR@20=0.27692  NDCG@20=0.35254\n",
      "[2020-03-02 17:36:00] [50/50] 0 mean_batch_loss : 3.108009\n",
      "Start predicting 2020-03-02 17:36:12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing finish [2020-03-02 17:36:15] \n",
      "\tHR@1=0.16477  MRR@1=0.16477  NDCG@1=0.16477\n",
      "\tHR@5=0.40818  MRR@5=0.25456  NDCG@5=0.29281\n",
      "\tHR@10=0.51309  MRR@10=0.26868  NDCG@10=0.32686\n",
      "\tHR@20=0.61047  MRR@20=0.27547  NDCG@20=0.35152\n",
      "training and testting over, Total time 0:18:15.172483\n",
      "best model change\n",
      "current model hyper-parameters: session_length=20, hidden_size=100, lr=0.0010, dropout=0.00\n",
      "\n",
      "current model HR@20=0.61099  MRR@20=0.27634\n",
      "the best result so far. HR@20=0.61099  MRR@20=0.27634,数session_length=20, hidden_size=100, lr=0.0010, dropout=0.00\n",
      " \n",
      "\n",
      "The best result HR@20=0.61099  MRR@20=0.27634, hyper-parameters: session_length=20, hidden_size=100, lr=0.0010, dropout=0.00\n",
      ". \n",
      "over.\n"
     ]
    }
   ],
   "source": [
    "hidden_sizes = [100]\n",
    "dropouts = [0]\n",
    "lrs = [1e-3]\n",
    "session_lengths = [20]\n",
    "patience = 5\n",
    "best_params = \"\"\n",
    "best_all_model = 0.0\n",
    "best_all_hr = 0.0\n",
    "best_all_mrr = 0.0\n",
    "best_all_r1m = 0.0\n",
    "for session_length in session_lengths:\n",
    "    for hidden_size in hidden_sizes:\n",
    "        for dropout in dropouts:\n",
    "            for lr in lrs:\n",
    "                args = {}\n",
    "                print(\"current model hyper-parameters: session_length=%d, hidden_size=%d, lr=%.4f, dropout=%.2f\\n\" % (session_length,hidden_size,lr,dropout))\n",
    "                args[\"session_length\"] = session_length\n",
    "                args[\"hidden_size\"] = hidden_size\n",
    "                args[\"dropout\"] = dropout\n",
    "                args[\"patience\"] = patience\n",
    "                args[\"lr\"] = lr\n",
    "                best_model,best_model_hr,best_model_mrr = train(args)\n",
    "                if best_model_hr + best_model_mrr > best_all_r1m:\n",
    "                    print(\"best model change\")\n",
    "                    best_all_r1m = best_model_hr + best_model_mrr\n",
    "                    best_all_hr = best_model_hr\n",
    "                    best_all_mrr = best_model_mrr\n",
    "                    best_all_model = best_model\n",
    "                    best_params = \"session_length=%d, hidden_size=%d, lr=%.4f, dropout=%.2f\\n\" % (session_length,hidden_size,lr,dropout)\n",
    "                best_model = None\n",
    "                print(\"current model hyper-parameters: session_length=%d, hidden_size=%d, lr=%.4f, dropout=%.2f\\n\" % (session_length,hidden_size,lr,dropout))\n",
    "                print(\"current model HR@20=%.5f  MRR@20=%.5f\"%(best_all_hr,best_model_mrr))\n",
    "                print(\"the best result so far. HR@20=%.5f  MRR@20=%.5f,数%s \\n\"%(best_all_hr,best_all_mrr,best_params))\n",
    "print(\"The best result HR@20=%.5f  MRR@20=%.5f, hyper-parameters: %s. \"%(best_all_hr,best_all_mrr,best_params))\n",
    "print(\"over.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smy",
   "language": "python",
   "name": "smy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
