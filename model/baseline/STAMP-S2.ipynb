{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-25T12:32:54.196969Z",
     "start_time": "2019-12-25T12:32:54.018148Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-25T12:32:54.483436Z",
     "start_time": "2019-12-25T12:32:54.198151Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-25T12:32:54.487815Z",
     "start_time": "2019-12-25T12:32:54.485542Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda:0\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-25T12:32:54.496998Z",
     "start_time": "2019-12-25T12:32:54.488681Z"
    }
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-25T12:32:54.503268Z",
     "start_time": "2019-12-25T12:32:54.497860Z"
    }
   },
   "outputs": [],
   "source": [
    "session_length = 20\n",
    "batch_size = 512\n",
    "plot_num = 30000\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-25T12:32:54.556643Z",
     "start_time": "2019-12-25T12:32:54.504662Z"
    }
   },
   "outputs": [],
   "source": [
    "# this part is different form POEM\n",
    "class SessionData(object):\n",
    "    def __init__(self,session_index,session_id,items_indexes):\n",
    "        self.session_index = session_index\n",
    "        self.session_id = session_id\n",
    "        self.item_list = items_indexes\n",
    "    def generate_seq_datas(self,session_length,padding_idx=0,predict_length=1):\n",
    "        sessions = []\n",
    "        if len(self.item_list)<2:\n",
    "            self.item_list.append[self.item_list[0]]\n",
    "        if predict_length==1:\n",
    "#             # when session length>=3\n",
    "#             for i in range(1,len(self.item_list)-1):\n",
    "            # when session length >=2\n",
    "            for i in range(len(self.item_list)-1):\n",
    "                if i <session_length:\n",
    "                    train_data = [0 for _ in range(session_length-i-1)]\n",
    "                    train_data.extend(self.item_list[:i+1])\n",
    "                    train_data.append(self.item_list[i+1])\n",
    "                else:\n",
    "                    train_data = self.item_list[i+1-session_length:i+1]\n",
    "                    train_data.append(self.item_list[i+1])\n",
    "                sessions.append(train_data)\n",
    "        else:\n",
    "            # To be continue if necessary\n",
    "            pass\n",
    "        return self.session_index,sessions\n",
    "    def __str__(self):\n",
    "        info = \" session index = {}\\n session id = {} \\n the length of item list= {} \\n the fisrt item index in item list is {}\".format(self.session_index,self.session_id,len(self.item_list),self.item_list[0])\n",
    "        return info\n",
    "class SessionDataSet(object):\n",
    "    def __init__(self,train_file,test_file,padding_idx=0):\n",
    "        super(SessionDataSet,self).__init__()\n",
    "        self.index_count = 0\n",
    "        self.session_count = 0\n",
    "        self.train_count = 0\n",
    "        self.test_count = 0\n",
    "        self.max_session_length = 0\n",
    "\n",
    "        self.padding_idx = padding_idx\n",
    "        self.item2index = dict()\n",
    "        self.index2item = dict()\n",
    "        self.session2index = dict()\n",
    "        self.index2session = dict()\n",
    "        self.item_total_num = dict()\n",
    "        self.item2index[\"<pad>\"] = padding_idx\n",
    "        self.index2item[padding_idx] = \"<pad>\"\n",
    "        self.train_data = self.load_data(train_file)\n",
    "        print(\"training set is loaded, # index: \",len(self.item2index.keys()))\n",
    "        self.train_count = self.session_count\n",
    "        print(\"train_session_num\",self.train_count)\n",
    "        self.test_data = self.load_data(test_file)\n",
    "        print(\"testing set is loaded, # index: \",len(self.index2item.keys()))\n",
    "        print(\"# item\",self.index_count)\n",
    "        self.test_count = self.session_count-self.train_count\n",
    "        print(\"# test session:\",self.test_count)\n",
    "        self.all_training_data = []\n",
    "        self.all_testing_data = []\n",
    "        self.all_meta_training_data = []\n",
    "        self.all_meta_testing_data = []\n",
    "        self.train_session_length = 0\n",
    "        self.test_session_length = 0\n",
    "    \n",
    "    def load_data(self,file_path):\n",
    "        data =  pickle.load(open(file_path, 'rb'))\n",
    "        session_ids = data[0]\n",
    "        session_data = data[1]\n",
    "        session_label = data[2]\n",
    "\n",
    "        result_data = []\n",
    "        lenth = len(session_ids)\n",
    "        print(\"# session\",lenth)\n",
    "\n",
    "        last_session_id = session_ids[0]\n",
    "        \n",
    "        session_item_indexes = []\n",
    "\n",
    "        for item_id in session_data[0]:\n",
    "            if item_id not in self.item2index.keys():\n",
    "                self.index_count+=1\n",
    "                self.item2index[item_id] = self.index_count\n",
    "                self.index2item[self.index_count] = item_id\n",
    "                self.item_total_num[self.index_count] = 0\n",
    "            session_item_indexes.append(self.item2index[item_id])\n",
    "            self.item_total_num[self.item2index[item_id]] += 1\n",
    "        target_item = session_label[0]\n",
    "        if target_item not in self.item2index.keys():\n",
    "            self.index_count+=1\n",
    "            self.item2index[target_item] = self.index_count\n",
    "            self.index2item[self.index_count] = target_item\n",
    "            self.item_total_num[self.index_count] = 0\n",
    "        session_item_indexes.append(self.item2index[target_item])\n",
    "        self.item_total_num[self.item2index[target_item]] += 1\n",
    "\n",
    "        for session_id,items,target_item in zip(session_ids,session_data,session_label):\n",
    "            if session_id!=last_session_id:\n",
    "\n",
    "                self.session_count+=1\n",
    "                self.session2index[last_session_id] = self.session_count\n",
    "                self.index2session[self.session_count] = last_session_id\n",
    "                if len(session_item_indexes)>self.max_session_length:\n",
    "                    self.max_session_length = len(session_item_indexes)\n",
    "                new_session = SessionData(self.session_count,last_session_id,session_item_indexes)\n",
    "                result_data.append(new_session)\n",
    "                last_session_id = session_id\n",
    "                session_item_indexes = []\n",
    "                for item_id in items:\n",
    "                    if item_id not in self.item2index.keys():\n",
    "                        self.index_count+=1\n",
    "                        self.item2index[item_id] = self.index_count\n",
    "                        self.index2item[self.index_count] = item_id\n",
    "                        self.item_total_num[self.index_count] = 0\n",
    "                    session_item_indexes.append(self.item2index[item_id])\n",
    "                    self.item_total_num[self.item2index[item_id]] += 1\n",
    "                if target_item not in self.item2index.keys():\n",
    "                    self.index_count+=1\n",
    "                    self.item2index[target_item] = self.index_count\n",
    "                    self.index2item[self.index_count] = target_item\n",
    "                    self.item_total_num[self.index_count] = 0\n",
    "                session_item_indexes.append(self.item2index[target_item])\n",
    "                self.item_total_num[self.item2index[target_item]] += 1\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        self.session_count+=1\n",
    "        self.session2index[last_session_id] = self.session_count\n",
    "        new_session = SessionData(self.session_count,last_session_id,session_item_indexes)\n",
    "        result_data.append(new_session)\n",
    "        print(\"loaded\")\n",
    "        print(new_session)\n",
    "        \n",
    "        return result_data\n",
    "    \n",
    "\n",
    "    def get_batch(self,batch_size,session_length=10,predict_length=1,all_data=None,phase=\"train\",neg_num=1,sampling_mathod=\"random\"):\n",
    "\n",
    "        if phase == \"train\":\n",
    "            if all_data is None:\n",
    "                all_data = self.get_all_training_data(session_length)\n",
    "            indexes = np.random.permutation(all_data.shape[0])\n",
    "            all_data = all_data[indexes]\n",
    "        else:\n",
    "            if all_data is None:\n",
    "                all_data = self.get_all_testing_data(session_length)\n",
    "        \n",
    "        sindex = 0\n",
    "        eindex = batch_size\n",
    "        while eindex < all_data.shape[0]:\n",
    "            batch = all_data[sindex: eindex]\n",
    "\n",
    "            temp = eindex\n",
    "            eindex = eindex + batch_size\n",
    "            sindex = temp\n",
    "            if phase ==\"train\":\n",
    "                batch = self.divid_and_extend_negative_samples(batch,session_length=session_length,predict_length=predict_length,neg_num=neg_num,method=sampling_mathod)\n",
    "            else:\n",
    "                batch = [batch[:,:session_length],batch[:,session_length:]]\n",
    "            yield batch\n",
    "\n",
    "        if eindex >= all_data.shape[0]:\n",
    "            batch = all_data[sindex:]\n",
    "            if phase ==\"train\":\n",
    "                batch = self.divid_and_extend_negative_samples(batch,session_length=session_length,predict_length=predict_length,neg_num=neg_num,method=sampling_mathod)\n",
    "            else:\n",
    "                batch = [batch[:,:session_length],batch[:,session_length:]]\n",
    "            yield batch\n",
    "    \n",
    "    def divid_and_extend_negative_samples(self,batch_data,session_length,predict_length=1,neg_num=1,method=\"random\"):\n",
    "        \"\"\"\n",
    "        divid and extend negative samples\n",
    "        \"\"\"\n",
    "        neg_items = []\n",
    "        if method == \"random\":\n",
    "            for session_and_target in batch_data:\n",
    "                neg_item = []\n",
    "                for i in range(neg_num):\n",
    "                    rand_item = random.randint(1,self.index_count)\n",
    "                    while rand_item in session_and_target or rand_item in neg_item:\n",
    "                        rand_item = random.randint(1,self.index_count)\n",
    "                    neg_item.append(rand_item)\n",
    "                neg_items.append(neg_item)\n",
    "        else:\n",
    "\n",
    "            total_list = set()\n",
    "            for session in batch_data:\n",
    "                for i in session:\n",
    "                    total_list.add(i) \n",
    "            total_list = list(total_list)\n",
    "            total_list =  sorted(total_list, key=lambda item: self.item_total_num[item],reverse=True)\n",
    "            for i,session in enumerate(batch_data):\n",
    "                np.random.choice(total_list)\n",
    "        session_items = batch_data[:,:session_length]\n",
    "        target_item = batch_data[:,session_length:]\n",
    "        neg_items = np.array(neg_items)\n",
    "        return [session_items,target_item,neg_items]\n",
    "    \n",
    "    def get_all_training_data(self,session_length,predict_length=1):\n",
    "        if len(self.all_training_data)!=0 and self.train_session_length==session_length:\n",
    "#             print(\"The build is complete and there is no need to repeat the build\")\n",
    "            return self.all_training_data\n",
    "        print(\"Start building the all training dataset\")\n",
    "        all_sessions = []\n",
    "        for session_data in self.train_data:\n",
    "            # 前session_length为session，后predict_length为target_item\n",
    "            session_index,sessions = session_data.generate_seq_datas(session_length,padding_idx=self.padding_idx)\n",
    "            if sessions is not None:\n",
    "                all_sessions.extend(sessions)\n",
    "        all_sessions = np.array(all_sessions)\n",
    "        self.all_training_data = all_sessions\n",
    "        self.train_session_length=session_length\n",
    "        print(\"The total number of training samples is：\",all_sessions.shape)\n",
    "        return all_sessions\n",
    "    \n",
    "    def get_all_testing_data(self,session_length,predict_length=1):\n",
    "        if len(self.all_testing_data)!=0 and self.test_session_length==session_length:\n",
    "            return self.all_testing_data\n",
    "        all_sessions = []\n",
    "        for session_data in self.test_data:\n",
    "            session_index,sessions = session_data.generate_seq_datas(session_length,padding_idx=self.padding_idx)\n",
    "            if sessions is not None:\n",
    "                all_sessions.extend(sessions)\n",
    "        all_sessions = np.array(all_sessions)\n",
    "        self.all_testing_data = all_sessions\n",
    "        self.test_session_length=session_length\n",
    "        print(\"The total number of testing samples is：\",all_sessions.shape)\n",
    "        return all_sessions\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-25T12:32:56.433626Z",
     "start_time": "2019-12-25T12:32:54.557792Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# session 749947\n",
      "loaded\n",
      " session index = 294620\n",
      " session id = 1131204 \n",
      " the length of item list= 2 \n",
      " the fisrt item index in item list is 23118\n",
      "training set is loaded, # index:  48990\n",
      "train_session_num 294620\n",
      "# session 28445\n",
      "loaded\n",
      " session index = 306825\n",
      " session id = 1582915 \n",
      " the length of item list= 6 \n",
      " the fisrt item index in item list is 4767\n",
      "testing set is loaded, # index:  48990\n",
      "# item 48989\n",
      "# test session: 12205\n"
     ]
    }
   ],
   "source": [
    "# dataset = SessionDataSet(train_file=\"../../data/retailrocket/train.txt\",test_file=\"../../data/srgnn/retailrocket/test.txt\")\n",
    "# dataset = SessionDataSet(train_file=\"../../data/diginetica/train.txt\",test_file=\"../../data/srgnn/diginetica/test.txt\")\n",
    "dataset = SessionDataSet(train_file=\"../../data/yoochoose1_4/train.txt\",test_file=\"../../data/srgnn/yoochoose1_4/test.txt\")\n",
    "# dataset = SessionDataSet(train_file=\"../../data/yoochoose1_64/train.txt\",test_file=\"../../data/srgnn/yoochoose1_64/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-25T12:32:56.442029Z",
     "start_time": "2019-12-25T12:32:56.434519Z"
    }
   },
   "outputs": [],
   "source": [
    "def bpr_loss(r):\n",
    "    return torch.sum(-torch.log(torch.sigmoid(r)))\n",
    "def get_hit_num(pred,y_truth):\n",
    "    \"\"\"\n",
    "        pred: numpy type(batch_size,k) \n",
    "        y_truth: list type (batch_size,groudtruth_num)\n",
    "    \"\"\"\n",
    "\n",
    "    hit_num = 0\n",
    "    for i in range(len(y_truth)):\n",
    "        for value in y_truth[i]:\n",
    "            hit_num += np.sum(pred[i]==value)\n",
    "    return hit_num\n",
    "\n",
    "def get_rr(pred,y_truth):\n",
    "    rr=0.\n",
    "    for i in range(len(y_truth)):\n",
    "        for value in y_truth[i]:\n",
    "            hit_indexes = np.where(pred[i]==value)[0]\n",
    "            for hit_index in hit_indexes:\n",
    "                rr += 1/(hit_index+1)\n",
    "    return rr\n",
    "\n",
    "def get_dcg(pred,y_truth):\n",
    "    y_pred_score = np.zeros_like(pred)\n",
    "\n",
    "    for i in range(len(y_truth)):\n",
    "\n",
    "        for j,y_pred in enumerate(pred[i]):\n",
    "            if y_pred == y_truth[i][0]:\n",
    "                y_pred_score[i][j]=1\n",
    "    gain = 2 ** y_pred_score - 1\n",
    "    discounts = np.tile(np.log2(np.arange(pred.shape[1]) + 2),(len(y_truth),1))\n",
    "    dcg = np.sum(gain / discounts,axis=1)\n",
    "    return dcg\n",
    "\n",
    "def get_ndcg(pred,y_truth):\n",
    "    dcg = get_dcg(pred, y_truth)\n",
    "    idcg = get_dcg(np.concatenate((y_truth,np.zeros_like(pred)[:,:-1]-1),axis=1), y_truth)\n",
    "    ndcg = np.sum(dcg / idcg)\n",
    "\n",
    "    return ndcg\n",
    "\n",
    "def dcg_score(y_pre, y_true, k):\n",
    "    y_pre_score = np.zeros(k)\n",
    "    if len(y_pre) > k:\n",
    "        y_pre = y_pre[:k]\n",
    "    for i in range(len(y_pre)):\n",
    "        pre_tag = y_pre[i]\n",
    "        if pre_tag in y_true:\n",
    "            y_pre_score[i] = 1\n",
    "    gain = 2 ** y_pre_score - 1\n",
    "    discounts = np.log2(np.arange(k) + 2)\n",
    "    return np.sum(gain / discounts)\n",
    "\n",
    "\n",
    "def ndcg_score(y_pre, y_true, k=5):\n",
    "    dcg = dcg_score(y_pre, y_true, k)\n",
    "    idcg = dcg_score(y_true, y_true, k)\n",
    "    return dcg / idcg\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-25T12:32:56.458414Z",
     "start_time": "2019-12-25T12:32:56.442950Z"
    }
   },
   "outputs": [],
   "source": [
    "# Attention Layer\n",
    "class Attention(torch.nn.Module):\n",
    "    def __init__(self, method=\"specific\", hidden_size=64):\n",
    "        super(Attention, self).__init__()\n",
    "        self.config = list()\n",
    "        self.method = method\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        if self.method == \"dot\":\n",
    "        \n",
    "            self.query = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "            self.key = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "\n",
    "        elif self.method == \"general\":\n",
    "            self.attention = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "\n",
    "        elif self.method == \"concat\":\n",
    "            self.attention = torch.nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "            self.v = torch.nn.Parameter(torch.FloatTensor(self.hidden_size))\n",
    "            \n",
    "        elif self.method == \"specific\":\n",
    "            self.W0 = torch.nn.Linear(self.hidden_size,1,bias=False)\n",
    "            torch.nn.init.normal_(self.W0.weight,0,0.05)\n",
    "            self.W1 = torch.nn.Linear(self.hidden_size,self.hidden_size,bias=False)\n",
    "            torch.nn.init.normal_(self.W1.weight,0,0.05)\n",
    "            self.W2 = torch.nn.Linear(self.hidden_size,self.hidden_size,bias=False)\n",
    "            torch.nn.init.normal_(self.W2.weight,0,0.05)\n",
    "            self.W3 = torch.nn.Linear(self.hidden_size,self.hidden_size,bias=False)\n",
    "            torch.nn.init.normal_(self.W3.weight,0,0.05)\n",
    "            self.b = torch.nn.Parameter(torch.FloatTensor(self.hidden_size))\n",
    "\n",
    "                               \n",
    "            \n",
    "\n",
    "    def dot_score(self, hidden, encoder_output,weights=None):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "\n",
    "    def general_score(self, hidden, encoder_output,weights=None):\n",
    "        energy = self.attention(encoder_output)\n",
    "        return torch.sum(hidden * energy, dim=2)\n",
    "\n",
    "\n",
    "    def concat_score(self, hidden, encoder_output,weights=None):\n",
    "        energy = self.attention(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
    "        return torch.sum(self.v * energy, dim=2)\n",
    "    \n",
    "\n",
    "    def specific_score(self,session,x_t,m_s,mask=None,weights=None):\n",
    "        if weights is None:\n",
    "            if mask is None:\n",
    "                W1Xi = self.W1(session)\n",
    "                W2Xt = self.W2(x_t).unsqueeze(1).repeat((1,session.shape[1],1))\n",
    "                W3Ms = self.W3(m_s).unsqueeze(1).repeat((1,session.shape[1],1))\n",
    "                energy = self.W0(torch.sigmoid(W1Xi+W2Xt+W3Ms+self.b)).repeat((1,1,session.shape[2]))\n",
    "    #         energy = energy*mask\n",
    "            else:\n",
    "    #         print(session.shape,x_t.shape,m_s.shape)\n",
    "                W1Xi = self.W1(session)*mask\n",
    "                W2Xt = self.W2(x_t).unsqueeze(1).repeat((1,session.shape[1],1))*mask\n",
    "                W3Ms = self.W3(m_s).unsqueeze(1).repeat((1,session.shape[1],1))*mask\n",
    "                energy = self.W0(torch.sigmoid(W1Xi+W2Xt+W3Ms+self.b)).repeat((1,1,session.shape[2]))*mask\n",
    "    #         energy = energy*mask\n",
    "        else:\n",
    "            key = 1\n",
    "            if mask is None:\n",
    "                W1Xi = torch.matmul(session,weights[key].t())\n",
    "                key +=1\n",
    "                W2Xt = (torch.matmul(x_t,weights[key].t())).unsqueeze(1).repeat((1,session.shape[1],1))\n",
    "                key +=1\n",
    "                W3Ms = (torch.matmul(x_t,weights[key].t())).unsqueeze(1).repeat((1,session.shape[1],1))\n",
    "                energy = torch.matmul(torch.sigmoid(W1Xi+W2Xt+W3Ms+weights[key+1]),weights[0].t()).repeat((1,1,session.shape[2]))\n",
    "    #         energy = energy*mask\n",
    "            else:\n",
    "    #         print(session.shape,x_t.shape,m_s.shape)\n",
    "                W1Xi = torch.matmul(session,weights[key].t())*mask\n",
    "                key +=1\n",
    "                W2Xt = (torch.matmul(x_t,weights[key].t())).unsqueeze(1).repeat((1,session.shape[1],1))*mask\n",
    "                key +=1\n",
    "                W3Ms = (torch.matmul(x_t,weights[key].t())).unsqueeze(1).repeat((1,session.shape[1],1))*mask\n",
    "                energy = torch.matmul(torch.sigmoid(W1Xi+W2Xt+W3Ms+weights[key+1]),weights[0].t()).repeat((1,1,session.shape[2]))*mask\n",
    "        return torch.sum(energy*session,dim=1)\n",
    "            \n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs=None,x_t=None,mask=None):\n",
    "\n",
    "        if self.method == \"general\":\n",
    "            attention_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == \"concat\":\n",
    "            attention_energies = self.concat_score(hidden, encoder_outputs)\n",
    "        elif self.method == \"dot\":\n",
    "            attention_energies = self.dot_score(hidden, encoder_outputs)\n",
    "        elif self.method == \"specific\":\n",
    "            session = hidden\n",
    "            m_s = encoder_outputs\n",
    "            return self.specific_score(session,x_t,m_s,mask)\n",
    "\n",
    "        attention_energies = attention_energies.t()\n",
    "\n",
    "        return F.softmax(attention_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-25T12:32:56.468936Z",
     "start_time": "2019-12-25T12:32:56.459389Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class STAMP(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=64, itemNum=0, posNum=0, padding_idx=0, dropout=0.5,\n",
    "                 activate=\"relu\"):\n",
    "        super(STAMP, self).__init__()\n",
    "        self.padding_idx = padding_idx\n",
    "        self.hidden_size = hidden_size\n",
    "        if activate == \"sigmoid\":\n",
    "            self.activate = torch.sigmoid\n",
    "        elif activate == \"tanh\":\n",
    "            self.activate = torch.tanh\n",
    "        else:\n",
    "            self.activate = torch.relu\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.item_embedding = torch.nn.Embedding(itemNum, hidden_size, padding_idx=self.padding_idx,max_norm=1.5)\n",
    "        torch.nn.init.normal_(self.item_embedding.weight,0,0.002)\n",
    "        torch.nn.init.constant_(self.item_embedding.weight[0],0)\n",
    "        self.attention = Attention(method=\"specific\",hidden_size=hidden_size)\n",
    "        self.left_mlp1 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        torch.nn.init.normal_(self.left_mlp1.weight,0,0.05)\n",
    "        torch.nn.init.constant_(self.left_mlp1.bias,0)\n",
    "        self.right_mlp1 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        torch.nn.init.normal_(self.left_mlp1.weight,0,0.05)\n",
    "        torch.nn.init.constant_(self.right_mlp1.bias,0)\n",
    "        torch.nn.utils.clip_grad_norm_(self.parameters(),max_norm=110)\n",
    "        \n",
    "    def forward(self, session):\n",
    " \n",
    "        mask = (session!=self.padding_idx).float()\n",
    "\n",
    "        length = torch.sum(mask,1).unsqueeze(1).repeat((1,self.hidden_size))\n",
    "\n",
    "        mask = mask.unsqueeze(2).repeat((1,1,self.hidden_size))\n",
    "        session_item_vecs = self.item_embedding(session) * mask\n",
    "        mean_session = torch.sum(session_item_vecs, dim=1)/length\n",
    "\n",
    "        compute_output = self.attention(session_item_vecs,mean_session,session_item_vecs[:,-1])\n",
    "        left_output = self.dropout(self.activate(self.left_mlp1(compute_output)))\n",
    "        right_output = self.dropout(self.activate(self.right_mlp1(session_item_vecs[:,-1])))\n",
    "\n",
    "        result = torch.matmul(left_output* right_output,self.item_embedding.weight[1:].t())\n",
    "\n",
    "        return result\n",
    "\n",
    "    def predict_top_k(self, session, k=20):\n",
    "\n",
    "        mask = (session!=0).float()\n",
    "\n",
    "        length = torch.sum(mask,1).unsqueeze(1).repeat((1,self.hidden_size))\n",
    "\n",
    "        mask = mask.unsqueeze(2).repeat((1,1,self.hidden_size))\n",
    "        session_item_vecs = self.item_embedding(session) * mask\n",
    "        mean_session = torch.sum(session_item_vecs, dim=1)/length\n",
    "        compute_output = self.attention(session_item_vecs,mean_session,session_item_vecs[:,-1])\n",
    "        left_output =self.activate(self.left_mlp1(compute_output))\n",
    "\n",
    "        right_output = self.activate(self.right_mlp1(session_item_vecs[:,-1]))\n",
    "\n",
    "        result = torch.matmul(left_output * right_output,self.item_embedding.weight[1:].t())\n",
    "\n",
    "        result = torch.topk(result,k,dim=1)[1]\n",
    "        return result\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIKM S >= 2  \n",
    "    HR@20=0.62227  MRR@20=0.28085, hyper-parameters: session_length=20, hidden_size=100, lr=0.0010, dropout=0.00\n",
    "        HR@1=0.16895  MRR@1=0.16895  NDCG@1=0.16895\n",
    "        HR@5=0.41572  MRR@5=0.25946  NDCG@5=0.29835\n",
    "        HR@10=0.52388  MRR@10=0.27400  NDCG@10=0.33342\n",
    "        HR@20=0.62227  MRR@20=0.28085  NDCG@20=0.35833\n",
    "# RR S >= 2  \n",
    "    HR@20=0.57483  MRR@20=0.31712, hyper-parameters: session_length=20, hidden_size=100, lr=0.0010, dropout=0.00\n",
    "        HR@1=0.22763  MRR@1=0.22763  NDCG@1=0.22763\n",
    "        HR@5=0.42844  MRR@5=0.30195  NDCG@5=0.33346\n",
    "        HR@10=0.50673  MRR@10=0.31240  NDCG@10=0.35878\n",
    "        HR@20=0.57483  MRR@20=0.31712  NDCG@20=0.37598\n",
    "# RSC64 S >= 2 \n",
    "    HR@20=0.69337  MRR@20=0.30292, hyper-parameters: session_length=20, hidden_size=100, lr=0.0010, dropout=0.00\n",
    "        HR@1=0.17360  MRR@1=0.17360  NDCG@1=0.17360\n",
    "        HR@5=0.46451  MRR@5=0.27888  NDCG@5=0.32501\n",
    "        HR@10=0.58791  MRR@10=0.29549  NDCG@10=0.36505\n",
    "        HR@20=0.69337  MRR@20=0.30292  NDCG@20=0.39185\n",
    "# RSC4 S >= 2   \n",
    "     HR@20=0.70938  MRR@20=0.31156, hyper-parameters: session_length=20, hidden_size=100, lr=0.0005, dropout=0.00\n",
    "        HR@1=0.18090  MRR@1=0.18090  NDCG@1=0.18090\n",
    "        HR@5=0.47340  MRR@5=0.28648  NDCG@5=0.33291\n",
    "        HR@10=0.60476  MRR@10=0.30418  NDCG@10=0.37556\n",
    "        HR@20=0.70938  MRR@20=0.31156  NDCG@20=0.40217"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-25T12:32:56.480923Z",
     "start_time": "2019-12-25T12:32:56.469992Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs=50\n",
    "def train(args):\n",
    "    hidden_size = args[\"hidden_size\"] if \"hidden_size\" in args.keys() else 100\n",
    "    dropout = args[\"dropout\"] if \"dropout\" in args.keys()  else 0.5\n",
    "    lr = args[\"lr\"] if \"lr\" in args.keys()  else 3e-3\n",
    "    session_length = args[\"session_length\"] if \"session_length\" in args.keys() else 20\n",
    "    model = STAMP(hidden_size=hidden_size, itemNum=dataset.index_count+1, posNum=11, padding_idx=0, dropout=0,\n",
    "                 activate=\"tanh\").to(device)\n",
    "    opti = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "    best_model_hr = 0.0\n",
    "    best_model_mrr = 0.0\n",
    "    best_r1m = 0.0\n",
    "    best_model = None\n",
    "    first_loss = 0.0\n",
    "    predict_nums = [1,5,10,20]\n",
    "    for epoch in range(epochs):\n",
    "        batch_losses = []\n",
    "        epoch_losses = []\n",
    "        for i,batch_data in enumerate(dataset.get_batch(batch_size,session_length,phase=\"train\")):\n",
    "            sessions = torch.tensor(batch_data[0]).to(device)\n",
    "            target_items = torch.tensor(batch_data[1]).squeeze().to(device)-1\n",
    "            result_pos = model(sessions)\n",
    "            loss = loss_function(result_pos,target_items)\n",
    "            opti.zero_grad()\n",
    "            loss.backward()\n",
    "            norm = torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm=110)\n",
    "#             print(norm)\n",
    "            opti.step()\n",
    "            batch_losses.append(loss.cpu().detach().numpy())\n",
    "            epoch_losses.append(loss.cpu().detach().numpy())\n",
    "            if i % plot_num == 0:\n",
    "                time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                print(\"[%s] [%d/%d] %d mean_batch_loss : %0.6f\" % (time, epoch+1, epochs, i, np.mean(batch_losses)))\n",
    "                batch_losses = []\n",
    "        with torch.no_grad():\n",
    "            start_test_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(\"Start predicting\",start_test_time)\n",
    "            rrs = [0 for _ in range(len(predict_nums))]\n",
    "            hit_nums = [0 for _ in range(len(predict_nums))]\n",
    "            ndcgs = [0 for _ in range(len(predict_nums))]\n",
    "            for i,batch_data in enumerate(dataset.get_batch(batch_size,session_length,phase=\"test\")):\n",
    "                sessions = torch.tensor(batch_data[0]).to(device)\n",
    "                target_items = np.array(batch_data[1])-1\n",
    "                y_pred = model.predict_top_k(sessions,20).cpu().numpy()\n",
    "#                 print(y_pred[:2],target_items[:2])\n",
    "                for j,predict_num in enumerate(predict_nums):\n",
    "                    hit_nums[j]+=get_hit_num(y_pred[:,:predict_num],target_items)\n",
    "                    rrs[j]+=get_rr(y_pred[:,:predict_num],target_items)\n",
    "                    ndcgs[j]+=get_ndcg(y_pred[:,:predict_num],target_items)\n",
    "                    \n",
    "            end_test_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            hrs = [hit_num/len(dataset.all_testing_data) for hit_num in hit_nums]\n",
    "            mrrs = [rr/len(dataset.all_testing_data) for rr in rrs]\n",
    "            mndcgs = [ndcg/len(dataset.all_testing_data) for ndcg in ndcgs]\n",
    "            if hrs[-1] + mrrs[-1] > best_r1m:\n",
    "                print(\"change best\")\n",
    "                best_model = deepcopy(model)\n",
    "                best_model_hr = hrs[-1]\n",
    "                best_model_mrr = mrrs[-1]\n",
    "                best_r1m = hrs[-1] + mrrs[-1]\n",
    "                no_improvement_epoch = 0\n",
    "            else:\n",
    "                no_improvement_epoch +=1\n",
    "            print(\"testing finish [%s] \"%end_test_time)\n",
    "            for k,predict_num in enumerate(predict_nums):\n",
    "                print(\"\\tHR@%d=%.5f  MRR@%d=%.5f  NDCG@%d=%.5f\"%(predict_num,hrs[k],predict_num,mrrs[k],predict_num,mndcgs[k]))\n",
    "        if no_improvement_epoch>=patience:\n",
    "            print(\"early stopping\")\n",
    "            break\n",
    "    return best_model,best_model_hr,best_model_mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-25T13:18:11.511586Z",
     "start_time": "2019-12-25T12:32:56.481922Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current model hyper-parameters: session_length=20, hidden_size=100, lr=0.0030, dropout=0.00\n",
      "\n",
      "Start building the all training dataset\n",
      "The total number of training samples is： (749947, 21)\n",
      "[2019-12-25 20:33:02] [1/50] 0 mean_batch_loss : 10.799348\n",
      "[2019-12-25 20:33:10] [1/50] 500 mean_batch_loss : 9.854733\n",
      "[2019-12-25 20:33:19] [1/50] 1000 mean_batch_loss : 7.909698\n",
      "Start predicting 2019-12-25 20:33:24\n",
      "The total number of testing samples is： (28445, 21)\n",
      "change best\n",
      "testing finish [2019-12-25 20:33:26] \n",
      "\tHR@1=0.14456  MRR@1=0.14456  NDCG@1=0.14456\n",
      "\tHR@5=0.29678  MRR@5=0.20019  NDCG@5=0.22422\n",
      "\tHR@10=0.36618  MRR@10=0.20954  NDCG@10=0.24675\n",
      "\tHR@20=0.43238  MRR@20=0.21418  NDCG@20=0.26354\n",
      "[2019-12-25 20:33:26] [2/50] 0 mean_batch_loss : 5.825375\n",
      "[2019-12-25 20:33:30] [2/50] 500 mean_batch_loss : 5.448526\n",
      "[2019-12-25 20:33:35] [2/50] 1000 mean_batch_loss : 5.224774\n",
      "Start predicting 2019-12-25 20:33:39\n",
      "change best\n",
      "testing finish [2019-12-25 20:33:41] \n",
      "\tHR@1=0.18618  MRR@1=0.18618  NDCG@1=0.18618\n",
      "\tHR@5=0.37557  MRR@5=0.25625  NDCG@5=0.28597\n",
      "\tHR@10=0.45414  MRR@10=0.26671  NDCG@10=0.31136\n",
      "\tHR@20=0.53131  MRR@20=0.27210  NDCG@20=0.33092\n",
      "[2019-12-25 20:33:41] [3/50] 0 mean_batch_loss : 4.450167\n",
      "[2019-12-25 20:33:51] [3/50] 500 mean_batch_loss : 4.571928\n",
      "[2019-12-25 20:34:01] [3/50] 1000 mean_batch_loss : 4.640850\n",
      "Start predicting 2019-12-25 20:34:10\n",
      "change best\n",
      "testing finish [2019-12-25 20:34:12] \n",
      "\tHR@1=0.20169  MRR@1=0.20169  NDCG@1=0.20169\n",
      "\tHR@5=0.39821  MRR@5=0.27409  NDCG@5=0.30500\n",
      "\tHR@10=0.48019  MRR@10=0.28513  NDCG@10=0.33161\n",
      "\tHR@20=0.55602  MRR@20=0.29046  NDCG@20=0.35086\n",
      "[2019-12-25 20:34:12] [4/50] 0 mean_batch_loss : 4.175799\n",
      "[2019-12-25 20:34:22] [4/50] 500 mean_batch_loss : 4.274512\n",
      "[2019-12-25 20:34:32] [4/50] 1000 mean_batch_loss : 4.418930\n",
      "Start predicting 2019-12-25 20:34:41\n",
      "change best\n",
      "testing finish [2019-12-25 20:34:43] \n",
      "\tHR@1=0.20735  MRR@1=0.20735  NDCG@1=0.20735\n",
      "\tHR@5=0.40172  MRR@5=0.28032  NDCG@5=0.31062\n",
      "\tHR@10=0.48560  MRR@10=0.29163  NDCG@10=0.33785\n",
      "\tHR@20=0.55894  MRR@20=0.29673  NDCG@20=0.35642\n",
      "[2019-12-25 20:34:43] [5/50] 0 mean_batch_loss : 3.935367\n",
      "[2019-12-25 20:34:52] [5/50] 500 mean_batch_loss : 4.138697\n",
      "[2019-12-25 20:35:02] [5/50] 1000 mean_batch_loss : 4.299354\n",
      "Start predicting 2019-12-25 20:35:12\n",
      "change best\n",
      "testing finish [2019-12-25 20:35:14] \n",
      "\tHR@1=0.21058  MRR@1=0.21058  NDCG@1=0.21058\n",
      "\tHR@5=0.40742  MRR@5=0.28311  NDCG@5=0.31406\n",
      "\tHR@10=0.48511  MRR@10=0.29356  NDCG@10=0.33926\n",
      "\tHR@20=0.56150  MRR@20=0.29892  NDCG@20=0.35865\n",
      "[2019-12-25 20:35:14] [6/50] 0 mean_batch_loss : 3.767555\n",
      "[2019-12-25 20:35:23] [6/50] 500 mean_batch_loss : 4.058609\n",
      "[2019-12-25 20:35:33] [6/50] 1000 mean_batch_loss : 4.216355\n",
      "Start predicting 2019-12-25 20:35:42\n",
      "change best\n",
      "testing finish [2019-12-25 20:35:44] \n",
      "\tHR@1=0.21375  MRR@1=0.21375  NDCG@1=0.21375\n",
      "\tHR@5=0.40984  MRR@5=0.28586  NDCG@5=0.31672\n",
      "\tHR@10=0.48638  MRR@10=0.29610  NDCG@10=0.34150\n",
      "\tHR@20=0.56333  MRR@20=0.30150  NDCG@20=0.36102\n",
      "[2019-12-25 20:35:44] [7/50] 0 mean_batch_loss : 3.771768\n",
      "[2019-12-25 20:35:53] [7/50] 500 mean_batch_loss : 3.990132\n",
      "[2019-12-25 20:36:03] [7/50] 1000 mean_batch_loss : 4.168249\n",
      "Start predicting 2019-12-25 20:36:12\n",
      "change best\n",
      "testing finish [2019-12-25 20:36:14] \n",
      "\tHR@1=0.21459  MRR@1=0.21459  NDCG@1=0.21459\n",
      "\tHR@5=0.41368  MRR@5=0.28778  NDCG@5=0.31911\n",
      "\tHR@10=0.49443  MRR@10=0.29854  NDCG@10=0.34520\n",
      "\tHR@20=0.56917  MRR@20=0.30373  NDCG@20=0.36410\n",
      "[2019-12-25 20:36:14] [8/50] 0 mean_batch_loss : 3.618650\n",
      "[2019-12-25 20:36:22] [8/50] 500 mean_batch_loss : 3.952572\n",
      "[2019-12-25 20:36:32] [8/50] 1000 mean_batch_loss : 4.127557\n",
      "Start predicting 2019-12-25 20:36:41\n",
      "testing finish [2019-12-25 20:36:43] \n",
      "\tHR@1=0.21280  MRR@1=0.21280  NDCG@1=0.21280\n",
      "\tHR@5=0.41434  MRR@5=0.28749  NDCG@5=0.31910\n",
      "\tHR@10=0.49228  MRR@10=0.29793  NDCG@10=0.34434\n",
      "\tHR@20=0.56780  MRR@20=0.30321  NDCG@20=0.36348\n",
      "[2019-12-25 20:36:43] [9/50] 0 mean_batch_loss : 3.785544\n",
      "[2019-12-25 20:36:51] [9/50] 500 mean_batch_loss : 3.912759\n",
      "[2019-12-25 20:37:01] [9/50] 1000 mean_batch_loss : 4.090574\n",
      "Start predicting 2019-12-25 20:37:10\n",
      "testing finish [2019-12-25 20:37:13] \n",
      "\tHR@1=0.21600  MRR@1=0.21600  NDCG@1=0.21600\n",
      "\tHR@5=0.41185  MRR@5=0.28826  NDCG@5=0.31904\n",
      "\tHR@10=0.49172  MRR@10=0.29895  NDCG@10=0.34490\n",
      "\tHR@20=0.56741  MRR@20=0.30420  NDCG@20=0.36404\n",
      "[2019-12-25 20:37:13] [10/50] 0 mean_batch_loss : 3.844662\n",
      "[2019-12-25 20:37:20] [10/50] 500 mean_batch_loss : 3.893344\n",
      "[2019-12-25 20:37:30] [10/50] 1000 mean_batch_loss : 4.061379\n",
      "Start predicting 2019-12-25 20:37:40\n",
      "testing finish [2019-12-25 20:37:42] \n",
      "\tHR@1=0.21406  MRR@1=0.21406  NDCG@1=0.21406\n",
      "\tHR@5=0.41290  MRR@5=0.28751  NDCG@5=0.31874\n",
      "\tHR@10=0.49123  MRR@10=0.29795  NDCG@10=0.34406\n",
      "\tHR@20=0.56520  MRR@20=0.30309  NDCG@20=0.36277\n",
      "[2019-12-25 20:37:42] [11/50] 0 mean_batch_loss : 3.544733\n",
      "[2019-12-25 20:37:50] [11/50] 500 mean_batch_loss : 3.864947\n",
      "[2019-12-25 20:38:00] [11/50] 1000 mean_batch_loss : 4.043698\n",
      "Start predicting 2019-12-25 20:38:09\n",
      "change best\n",
      "testing finish [2019-12-25 20:38:11] \n",
      "\tHR@1=0.21839  MRR@1=0.21839  NDCG@1=0.21839\n",
      "\tHR@5=0.41227  MRR@5=0.28992  NDCG@5=0.32040\n",
      "\tHR@10=0.48965  MRR@10=0.30036  NDCG@10=0.34554\n",
      "\tHR@20=0.56720  MRR@20=0.30573  NDCG@20=0.36512\n",
      "[2019-12-25 20:38:11] [12/50] 0 mean_batch_loss : 3.812901\n",
      "[2019-12-25 20:38:19] [12/50] 500 mean_batch_loss : 3.846998\n",
      "[2019-12-25 20:38:29] [12/50] 1000 mean_batch_loss : 4.025906\n",
      "Start predicting 2019-12-25 20:38:38\n",
      "change best\n",
      "testing finish [2019-12-25 20:38:40] \n",
      "\tHR@1=0.21452  MRR@1=0.21452  NDCG@1=0.21452\n",
      "\tHR@5=0.41459  MRR@5=0.28887  NDCG@5=0.32021\n",
      "\tHR@10=0.49429  MRR@10=0.29949  NDCG@10=0.34597\n",
      "\tHR@20=0.57121  MRR@20=0.30487  NDCG@20=0.36547\n",
      "[2019-12-25 20:38:40] [13/50] 0 mean_batch_loss : 3.627738\n",
      "[2019-12-25 20:38:49] [13/50] 500 mean_batch_loss : 3.831841\n",
      "[2019-12-25 20:38:58] [13/50] 1000 mean_batch_loss : 4.009800\n",
      "Start predicting 2019-12-25 20:39:07\n",
      "testing finish [2019-12-25 20:39:10] \n",
      "\tHR@1=0.21547  MRR@1=0.21547  NDCG@1=0.21547\n",
      "\tHR@5=0.41420  MRR@5=0.28909  NDCG@5=0.32026\n",
      "\tHR@10=0.49387  MRR@10=0.29980  NDCG@10=0.34610\n",
      "\tHR@20=0.56586  MRR@20=0.30481  NDCG@20=0.36432\n",
      "[2019-12-25 20:39:10] [14/50] 0 mean_batch_loss : 3.601027\n",
      "[2019-12-25 20:39:19] [14/50] 500 mean_batch_loss : 3.816555\n",
      "[2019-12-25 20:39:27] [14/50] 1000 mean_batch_loss : 3.999960\n",
      "Start predicting 2019-12-25 20:39:37\n",
      "testing finish [2019-12-25 20:39:39] \n",
      "\tHR@1=0.21719  MRR@1=0.21719  NDCG@1=0.21719\n",
      "\tHR@5=0.41455  MRR@5=0.29002  NDCG@5=0.32103\n",
      "\tHR@10=0.49404  MRR@10=0.30063  NDCG@10=0.34674\n",
      "\tHR@20=0.56748  MRR@20=0.30576  NDCG@20=0.36534\n",
      "[2019-12-25 20:39:39] [15/50] 0 mean_batch_loss : 3.826047\n",
      "[2019-12-25 20:39:49] [15/50] 500 mean_batch_loss : 3.811114\n",
      "[2019-12-25 20:39:57] [15/50] 1000 mean_batch_loss : 3.989275\n",
      "Start predicting 2019-12-25 20:40:06\n",
      "testing finish [2019-12-25 20:40:08] \n",
      "\tHR@1=0.21638  MRR@1=0.21638  NDCG@1=0.21638\n",
      "\tHR@5=0.41519  MRR@5=0.29020  NDCG@5=0.32134\n",
      "\tHR@10=0.49485  MRR@10=0.30092  NDCG@10=0.34718\n",
      "\tHR@20=0.56857  MRR@20=0.30605  NDCG@20=0.36585\n",
      "[2019-12-25 20:40:08] [16/50] 0 mean_batch_loss : 3.646904\n",
      "[2019-12-25 20:40:18] [16/50] 500 mean_batch_loss : 3.798675\n",
      "[2019-12-25 20:40:26] [16/50] 1000 mean_batch_loss : 3.984155\n",
      "Start predicting 2019-12-25 20:40:35\n",
      "testing finish [2019-12-25 20:40:37] \n",
      "\tHR@1=0.21666  MRR@1=0.21666  NDCG@1=0.21666\n",
      "\tHR@5=0.41339  MRR@5=0.28935  NDCG@5=0.32025\n",
      "\tHR@10=0.49538  MRR@10=0.30027  NDCG@10=0.34674\n",
      "\tHR@20=0.56910  MRR@20=0.30541  NDCG@20=0.36540\n",
      "[2019-12-25 20:40:37] [17/50] 0 mean_batch_loss : 3.798303\n",
      "[2019-12-25 20:40:47] [17/50] 500 mean_batch_loss : 3.792220\n",
      "[2019-12-25 20:40:55] [17/50] 1000 mean_batch_loss : 3.976059\n",
      "Start predicting 2019-12-25 20:41:05\n",
      "testing finish [2019-12-25 20:41:07] \n",
      "\tHR@1=0.21593  MRR@1=0.21593  NDCG@1=0.21593\n",
      "\tHR@5=0.41508  MRR@5=0.28923  NDCG@5=0.32056\n",
      "\tHR@10=0.49675  MRR@10=0.30025  NDCG@10=0.34708\n",
      "\tHR@20=0.57061  MRR@20=0.30540  NDCG@20=0.36578\n",
      "early stopping\n",
      "best model change\n",
      "current model hyper-parameters: session_length=20, hidden_size=100, lr=0.0030, dropout=0.00\n",
      "\n",
      "current model Recall@20=0.57121  MRR@20=0.30487\n",
      "the best result so far. Recall@20=0.57121  MRR@20=0.30487,数session_length=20, hidden_size=100, lr=0.0030, dropout=0.00\n",
      " \n",
      "\n",
      "current model hyper-parameters: session_length=20, hidden_size=100, lr=0.0010, dropout=0.00\n",
      "\n",
      "[2019-12-25 20:41:07] [1/50] 0 mean_batch_loss : 10.799348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-12-25 20:41:17] [1/50] 500 mean_batch_loss : 10.210074\n",
      "[2019-12-25 20:41:25] [1/50] 1000 mean_batch_loss : 9.546416\n",
      "Start predicting 2019-12-25 20:41:34\n",
      "change best\n",
      "testing finish [2019-12-25 20:41:36] \n",
      "\tHR@1=0.02348  MRR@1=0.02348  NDCG@1=0.02348\n",
      "\tHR@5=0.06092  MRR@5=0.03694  NDCG@5=0.04289\n",
      "\tHR@10=0.08592  MRR@10=0.04023  NDCG@10=0.05093\n",
      "\tHR@20=0.11320  MRR@20=0.04207  NDCG@20=0.05775\n",
      "[2019-12-25 20:41:36] [2/50] 0 mean_batch_loss : 8.336213\n",
      "[2019-12-25 20:41:46] [2/50] 500 mean_batch_loss : 8.062947\n",
      "[2019-12-25 20:41:54] [2/50] 1000 mean_batch_loss : 7.505450\n",
      "Start predicting 2019-12-25 20:42:03\n",
      "change best\n",
      "testing finish [2019-12-25 20:42:05] \n",
      "\tHR@1=0.08255  MRR@1=0.08255  NDCG@1=0.08255\n",
      "\tHR@5=0.19237  MRR@5=0.12176  NDCG@5=0.13927\n",
      "\tHR@10=0.24539  MRR@10=0.12886  NDCG@10=0.15643\n",
      "\tHR@20=0.30258  MRR@20=0.13283  NDCG@20=0.17090\n",
      "[2019-12-25 20:42:06] [3/50] 0 mean_batch_loss : 6.353549\n",
      "[2019-12-25 20:42:16] [3/50] 500 mean_batch_loss : 6.360643\n",
      "[2019-12-25 20:42:24] [3/50] 1000 mean_batch_loss : 6.137774\n",
      "Start predicting 2019-12-25 20:42:33\n",
      "change best\n",
      "testing finish [2019-12-25 20:42:35] \n",
      "\tHR@1=0.13507  MRR@1=0.13507  NDCG@1=0.13507\n",
      "\tHR@5=0.27826  MRR@5=0.18701  NDCG@5=0.20968\n",
      "\tHR@10=0.34505  MRR@10=0.19600  NDCG@10=0.23136\n",
      "\tHR@20=0.41107  MRR@20=0.20055  NDCG@20=0.24801\n",
      "[2019-12-25 20:42:35] [4/50] 0 mean_batch_loss : 5.666297\n",
      "[2019-12-25 20:42:45] [4/50] 500 mean_batch_loss : 5.457535\n",
      "[2019-12-25 20:42:54] [4/50] 1000 mean_batch_loss : 5.381198\n",
      "Start predicting 2019-12-25 20:43:02\n",
      "change best\n",
      "testing finish [2019-12-25 20:43:04] \n",
      "\tHR@1=0.16189  MRR@1=0.16189  NDCG@1=0.16189\n",
      "\tHR@5=0.32913  MRR@5=0.22292  NDCG@5=0.24933\n",
      "\tHR@10=0.40007  MRR@10=0.23243  NDCG@10=0.27231\n",
      "\tHR@20=0.46743  MRR@20=0.23713  NDCG@20=0.28937\n",
      "[2019-12-25 20:43:04] [5/50] 0 mean_batch_loss : 4.916933\n",
      "[2019-12-25 20:43:14] [5/50] 500 mean_batch_loss : 4.938057\n",
      "[2019-12-25 20:43:24] [5/50] 1000 mean_batch_loss : 4.930395\n",
      "Start predicting 2019-12-25 20:43:31\n",
      "change best\n",
      "testing finish [2019-12-25 20:43:33] \n",
      "\tHR@1=0.18295  MRR@1=0.18295  NDCG@1=0.18295\n",
      "\tHR@5=0.35606  MRR@5=0.24684  NDCG@5=0.27405\n",
      "\tHR@10=0.43269  MRR@10=0.25721  NDCG@10=0.29897\n",
      "\tHR@20=0.50065  MRR@20=0.26197  NDCG@20=0.31621\n",
      "[2019-12-25 20:43:33] [6/50] 0 mean_batch_loss : 4.635368\n",
      "[2019-12-25 20:43:43] [6/50] 500 mean_batch_loss : 4.605110\n",
      "[2019-12-25 20:43:53] [6/50] 1000 mean_batch_loss : 4.645928\n",
      "Start predicting 2019-12-25 20:44:00\n",
      "change best\n",
      "testing finish [2019-12-25 20:44:03] \n",
      "\tHR@1=0.19304  MRR@1=0.19304  NDCG@1=0.19304\n",
      "\tHR@5=0.37736  MRR@5=0.26103  NDCG@5=0.28999\n",
      "\tHR@10=0.44985  MRR@10=0.27072  NDCG@10=0.31344\n",
      "\tHR@20=0.51854  MRR@20=0.27550  NDCG@20=0.33083\n",
      "[2019-12-25 20:44:03] [7/50] 0 mean_batch_loss : 4.316590\n",
      "[2019-12-25 20:44:13] [7/50] 500 mean_batch_loss : 4.389044\n",
      "[2019-12-25 20:44:23] [7/50] 1000 mean_batch_loss : 4.436435\n",
      "Start predicting 2019-12-25 20:44:30\n",
      "change best\n",
      "testing finish [2019-12-25 20:44:32] \n",
      "\tHR@1=0.20056  MRR@1=0.20056  NDCG@1=0.20056\n",
      "\tHR@5=0.38741  MRR@5=0.26987  NDCG@5=0.29916\n",
      "\tHR@10=0.46430  MRR@10=0.28004  NDCG@10=0.32394\n",
      "\tHR@20=0.53162  MRR@20=0.28471  NDCG@20=0.34095\n",
      "[2019-12-25 20:44:32] [8/50] 0 mean_batch_loss : 4.098461\n",
      "[2019-12-25 20:44:42] [8/50] 500 mean_batch_loss : 4.233524\n",
      "[2019-12-25 20:44:52] [8/50] 1000 mean_batch_loss : 4.292403\n",
      "Start predicting 2019-12-25 20:44:59\n",
      "change best\n",
      "testing finish [2019-12-25 20:45:01] \n",
      "\tHR@1=0.20981  MRR@1=0.20981  NDCG@1=0.20981\n",
      "\tHR@5=0.39880  MRR@5=0.27982  NDCG@5=0.30946\n",
      "\tHR@10=0.47348  MRR@10=0.28984  NDCG@10=0.33367\n",
      "\tHR@20=0.54041  MRR@20=0.29453  NDCG@20=0.35064\n",
      "[2019-12-25 20:45:01] [9/50] 0 mean_batch_loss : 4.056054\n",
      "[2019-12-25 20:45:11] [9/50] 500 mean_batch_loss : 4.103880\n",
      "[2019-12-25 20:45:21] [9/50] 1000 mean_batch_loss : 4.178835\n",
      "Start predicting 2019-12-25 20:45:29\n",
      "change best\n",
      "testing finish [2019-12-25 20:45:31] \n",
      "\tHR@1=0.21329  MRR@1=0.21329  NDCG@1=0.21329\n",
      "\tHR@5=0.40299  MRR@5=0.28317  NDCG@5=0.31300\n",
      "\tHR@10=0.47748  MRR@10=0.29310  NDCG@10=0.33708\n",
      "\tHR@20=0.54470  MRR@20=0.29781  NDCG@20=0.35413\n",
      "[2019-12-25 20:45:31] [10/50] 0 mean_batch_loss : 4.022224\n",
      "[2019-12-25 20:45:41] [10/50] 500 mean_batch_loss : 4.020434\n",
      "[2019-12-25 20:45:51] [10/50] 1000 mean_batch_loss : 4.100347\n",
      "Start predicting 2019-12-25 20:45:59\n",
      "change best\n",
      "testing finish [2019-12-25 20:46:01] \n",
      "\tHR@1=0.21733  MRR@1=0.21733  NDCG@1=0.21733\n",
      "\tHR@5=0.40492  MRR@5=0.28675  NDCG@5=0.31618\n",
      "\tHR@10=0.48135  MRR@10=0.29702  NDCG@10=0.34097\n",
      "\tHR@20=0.55047  MRR@20=0.30188  NDCG@20=0.35852\n",
      "[2019-12-25 20:46:01] [11/50] 0 mean_batch_loss : 3.734867\n",
      "[2019-12-25 20:46:22] [11/50] 500 mean_batch_loss : 3.946895\n",
      "[2019-12-25 20:46:43] [11/50] 1000 mean_batch_loss : 4.031278\n",
      "Start predicting 2019-12-25 20:47:02\n",
      "change best\n",
      "testing finish [2019-12-25 20:47:05] \n",
      "\tHR@1=0.21839  MRR@1=0.21839  NDCG@1=0.21839\n",
      "\tHR@5=0.41090  MRR@5=0.28951  NDCG@5=0.31974\n",
      "\tHR@10=0.48508  MRR@10=0.29952  NDCG@10=0.34383\n",
      "\tHR@20=0.55556  MRR@20=0.30443  NDCG@20=0.36168\n",
      "[2019-12-25 20:47:05] [12/50] 0 mean_batch_loss : 3.747423\n",
      "[2019-12-25 20:47:28] [12/50] 500 mean_batch_loss : 3.888708\n",
      "[2019-12-25 20:47:47] [12/50] 1000 mean_batch_loss : 3.970328\n",
      "Start predicting 2019-12-25 20:48:09\n",
      "change best\n",
      "testing finish [2019-12-25 20:48:11] \n",
      "\tHR@1=0.21895  MRR@1=0.21895  NDCG@1=0.21895\n",
      "\tHR@5=0.41431  MRR@5=0.29099  NDCG@5=0.32170\n",
      "\tHR@10=0.49028  MRR@10=0.30119  NDCG@10=0.34632\n",
      "\tHR@20=0.55644  MRR@20=0.30583  NDCG@20=0.36310\n",
      "[2019-12-25 20:48:11] [13/50] 0 mean_batch_loss : 3.733158\n",
      "[2019-12-25 20:48:31] [13/50] 500 mean_batch_loss : 3.840779\n",
      "[2019-12-25 20:48:54] [13/50] 1000 mean_batch_loss : 3.931571\n",
      "Start predicting 2019-12-25 20:49:14\n",
      "change best\n",
      "testing finish [2019-12-25 20:49:16] \n",
      "\tHR@1=0.22183  MRR@1=0.22183  NDCG@1=0.22183\n",
      "\tHR@5=0.41473  MRR@5=0.29332  NDCG@5=0.32357\n",
      "\tHR@10=0.49225  MRR@10=0.30376  NDCG@10=0.34873\n",
      "\tHR@20=0.55961  MRR@20=0.30845  NDCG@20=0.36578\n",
      "[2019-12-25 20:49:16] [14/50] 0 mean_batch_loss : 3.769195\n",
      "[2019-12-25 20:49:40] [14/50] 500 mean_batch_loss : 3.793839\n",
      "[2019-12-25 20:50:01] [14/50] 1000 mean_batch_loss : 3.882448\n",
      "Start predicting 2019-12-25 20:50:21\n",
      "change best\n",
      "testing finish [2019-12-25 20:50:23] \n",
      "\tHR@1=0.22039  MRR@1=0.22039  NDCG@1=0.22039\n",
      "\tHR@5=0.41747  MRR@5=0.29333  NDCG@5=0.32425\n",
      "\tHR@10=0.49316  MRR@10=0.30348  NDCG@10=0.34878\n",
      "\tHR@20=0.56063  MRR@20=0.30818  NDCG@20=0.36586\n",
      "[2019-12-25 20:50:23] [15/50] 0 mean_batch_loss : 3.517942\n",
      "[2019-12-25 20:50:46] [15/50] 500 mean_batch_loss : 3.765508\n",
      "[2019-12-25 20:51:06] [15/50] 1000 mean_batch_loss : 3.854205\n",
      "Start predicting 2019-12-25 20:51:28\n",
      "change best\n",
      "testing finish [2019-12-25 20:51:31] \n",
      "\tHR@1=0.22538  MRR@1=0.22538  NDCG@1=0.22538\n",
      "\tHR@5=0.42116  MRR@5=0.29797  NDCG@5=0.32866\n",
      "\tHR@10=0.49587  MRR@10=0.30799  NDCG@10=0.35288\n",
      "\tHR@20=0.56323  MRR@20=0.31267  NDCG@20=0.36991\n",
      "[2019-12-25 20:51:31] [16/50] 0 mean_batch_loss : 3.714358\n",
      "[2019-12-25 20:51:50] [16/50] 500 mean_batch_loss : 3.726364\n",
      "[2019-12-25 20:52:14] [16/50] 1000 mean_batch_loss : 3.836343\n",
      "Start predicting 2019-12-25 20:52:33\n",
      "testing finish [2019-12-25 20:52:35] \n",
      "\tHR@1=0.22155  MRR@1=0.22155  NDCG@1=0.22155\n",
      "\tHR@5=0.42043  MRR@5=0.29574  NDCG@5=0.32684\n",
      "\tHR@10=0.49492  MRR@10=0.30571  NDCG@10=0.35096\n",
      "\tHR@20=0.56316  MRR@20=0.31044  NDCG@20=0.36820\n",
      "[2019-12-25 20:52:35] [17/50] 0 mean_batch_loss : 3.662856\n",
      "[2019-12-25 20:52:59] [17/50] 500 mean_batch_loss : 3.702992\n",
      "[2019-12-25 20:53:21] [17/50] 1000 mean_batch_loss : 3.804610\n",
      "Start predicting 2019-12-25 20:53:40\n",
      "testing finish [2019-12-25 20:53:43] \n",
      "\tHR@1=0.22197  MRR@1=0.22197  NDCG@1=0.22197\n",
      "\tHR@5=0.41965  MRR@5=0.29611  NDCG@5=0.32695\n",
      "\tHR@10=0.49485  MRR@10=0.30633  NDCG@10=0.35144\n",
      "\tHR@20=0.56354  MRR@20=0.31111  NDCG@20=0.36884\n",
      "[2019-12-25 20:53:43] [18/50] 0 mean_batch_loss : 3.586072\n",
      "[2019-12-25 20:54:05] [18/50] 500 mean_batch_loss : 3.681239\n",
      "[2019-12-25 20:54:26] [18/50] 1000 mean_batch_loss : 3.776921\n",
      "Start predicting 2019-12-25 20:54:48\n",
      "change best\n",
      "testing finish [2019-12-25 20:54:50] \n",
      "\tHR@1=0.22387  MRR@1=0.22387  NDCG@1=0.22387\n",
      "\tHR@5=0.42176  MRR@5=0.29795  NDCG@5=0.32884\n",
      "\tHR@10=0.49801  MRR@10=0.30819  NDCG@10=0.35356\n",
      "\tHR@20=0.56671  MRR@20=0.31298  NDCG@20=0.37096\n",
      "[2019-12-25 20:54:50] [19/50] 0 mean_batch_loss : 3.563989\n",
      "[2019-12-25 20:55:11] [19/50] 500 mean_batch_loss : 3.661939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-12-25 20:55:35] [19/50] 1000 mean_batch_loss : 3.761557\n",
      "Start predicting 2019-12-25 20:55:54\n",
      "testing finish [2019-12-25 20:55:56] \n",
      "\tHR@1=0.22292  MRR@1=0.22292  NDCG@1=0.22292\n",
      "\tHR@5=0.42355  MRR@5=0.29758  NDCG@5=0.32899\n",
      "\tHR@10=0.49780  MRR@10=0.30761  NDCG@10=0.35313\n",
      "\tHR@20=0.56397  MRR@20=0.31224  NDCG@20=0.36990\n",
      "[2019-12-25 20:55:56] [20/50] 0 mean_batch_loss : 3.474806\n",
      "[2019-12-25 20:56:20] [20/50] 500 mean_batch_loss : 3.646299\n",
      "[2019-12-25 20:56:41] [20/50] 1000 mean_batch_loss : 3.739263\n",
      "Start predicting 2019-12-25 20:57:01\n",
      "testing finish [2019-12-25 20:57:04] \n",
      "\tHR@1=0.22229  MRR@1=0.22229  NDCG@1=0.22229\n",
      "\tHR@5=0.42243  MRR@5=0.29678  NDCG@5=0.32812\n",
      "\tHR@10=0.49868  MRR@10=0.30702  NDCG@10=0.35284\n",
      "\tHR@20=0.56667  MRR@20=0.31178  NDCG@20=0.37008\n",
      "[2019-12-25 20:57:04] [21/50] 0 mean_batch_loss : 3.617377\n",
      "[2019-12-25 20:57:26] [21/50] 500 mean_batch_loss : 3.628491\n",
      "[2019-12-25 20:57:47] [21/50] 1000 mean_batch_loss : 3.720677\n",
      "Start predicting 2019-12-25 20:58:07\n",
      "change best\n",
      "testing finish [2019-12-25 20:58:10] \n",
      "\tHR@1=0.22471  MRR@1=0.22471  NDCG@1=0.22471\n",
      "\tHR@5=0.42313  MRR@5=0.29850  NDCG@5=0.32958\n",
      "\tHR@10=0.49851  MRR@10=0.30867  NDCG@10=0.35406\n",
      "\tHR@20=0.56731  MRR@20=0.31347  NDCG@20=0.37149\n",
      "[2019-12-25 20:58:10] [22/50] 0 mean_batch_loss : 3.597118\n",
      "[2019-12-25 20:58:31] [22/50] 500 mean_batch_loss : 3.614414\n",
      "[2019-12-25 20:58:53] [22/50] 1000 mean_batch_loss : 3.705945\n",
      "Start predicting 2019-12-25 20:59:13\n",
      "change best\n",
      "testing finish [2019-12-25 20:59:16] \n",
      "\tHR@1=0.22507  MRR@1=0.22507  NDCG@1=0.22507\n",
      "\tHR@5=0.42384  MRR@5=0.29907  NDCG@5=0.33017\n",
      "\tHR@10=0.50012  MRR@10=0.30936  NDCG@10=0.35496\n",
      "\tHR@20=0.56871  MRR@20=0.31416  NDCG@20=0.37235\n",
      "[2019-12-25 20:59:16] [23/50] 0 mean_batch_loss : 3.436473\n",
      "[2019-12-25 20:59:39] [23/50] 500 mean_batch_loss : 3.599082\n",
      "[2019-12-25 21:00:00] [23/50] 1000 mean_batch_loss : 3.692412\n",
      "Start predicting 2019-12-25 21:00:20\n",
      "testing finish [2019-12-25 21:00:22] \n",
      "\tHR@1=0.22362  MRR@1=0.22362  NDCG@1=0.22362\n",
      "\tHR@5=0.42587  MRR@5=0.29858  NDCG@5=0.33029\n",
      "\tHR@10=0.50248  MRR@10=0.30886  NDCG@10=0.35511\n",
      "\tHR@20=0.56857  MRR@20=0.31347  NDCG@20=0.37186\n",
      "[2019-12-25 21:00:22] [24/50] 0 mean_batch_loss : 3.430611\n",
      "[2019-12-25 21:00:44] [24/50] 500 mean_batch_loss : 3.582238\n",
      "[2019-12-25 21:01:04] [24/50] 1000 mean_batch_loss : 3.685668\n",
      "Start predicting 2019-12-25 21:01:24\n",
      "change best\n",
      "testing finish [2019-12-25 21:01:27] \n",
      "\tHR@1=0.22633  MRR@1=0.22633  NDCG@1=0.22633\n",
      "\tHR@5=0.42693  MRR@5=0.30105  NDCG@5=0.33243\n",
      "\tHR@10=0.50209  MRR@10=0.31112  NDCG@10=0.35678\n",
      "\tHR@20=0.57072  MRR@20=0.31588  NDCG@20=0.37413\n",
      "[2019-12-25 21:01:27] [25/50] 0 mean_batch_loss : 3.503855\n",
      "[2019-12-25 21:01:47] [25/50] 500 mean_batch_loss : 3.572598\n",
      "[2019-12-25 21:02:08] [25/50] 1000 mean_batch_loss : 3.663270\n",
      "Start predicting 2019-12-25 21:02:29\n",
      "change best\n",
      "testing finish [2019-12-25 21:02:32] \n",
      "\tHR@1=0.22739  MRR@1=0.22739  NDCG@1=0.22739\n",
      "\tHR@5=0.42798  MRR@5=0.30169  NDCG@5=0.33315\n",
      "\tHR@10=0.50171  MRR@10=0.31162  NDCG@10=0.35708\n",
      "\tHR@20=0.57065  MRR@20=0.31643  NDCG@20=0.37455\n",
      "[2019-12-25 21:02:32] [26/50] 0 mean_batch_loss : 3.548981\n",
      "[2019-12-25 21:02:52] [26/50] 500 mean_batch_loss : 3.566127\n",
      "[2019-12-25 21:03:14] [26/50] 1000 mean_batch_loss : 3.655620\n",
      "Start predicting 2019-12-25 21:03:32\n",
      "testing finish [2019-12-25 21:03:35] \n",
      "\tHR@1=0.22542  MRR@1=0.22542  NDCG@1=0.22542\n",
      "\tHR@5=0.42524  MRR@5=0.29950  NDCG@5=0.33083\n",
      "\tHR@10=0.50378  MRR@10=0.31014  NDCG@10=0.35639\n",
      "\tHR@20=0.57188  MRR@20=0.31486  NDCG@20=0.37360\n",
      "[2019-12-25 21:03:35] [27/50] 0 mean_batch_loss : 3.319159\n",
      "[2019-12-25 21:03:56] [27/50] 500 mean_batch_loss : 3.545778\n",
      "[2019-12-25 21:04:17] [27/50] 1000 mean_batch_loss : 3.649041\n",
      "Start predicting 2019-12-25 21:04:37\n",
      "change best\n",
      "testing finish [2019-12-25 21:04:39] \n",
      "\tHR@1=0.22774  MRR@1=0.22774  NDCG@1=0.22774\n",
      "\tHR@5=0.42609  MRR@5=0.30173  NDCG@5=0.33274\n",
      "\tHR@10=0.50234  MRR@10=0.31193  NDCG@10=0.35742\n",
      "\tHR@20=0.57205  MRR@20=0.31686  NDCG@20=0.37516\n",
      "[2019-12-25 21:04:39] [28/50] 0 mean_batch_loss : 3.446116\n",
      "[2019-12-25 21:05:02] [28/50] 500 mean_batch_loss : 3.533824\n",
      "[2019-12-25 21:05:22] [28/50] 1000 mean_batch_loss : 3.637085\n",
      "Start predicting 2019-12-25 21:05:42\n",
      "testing finish [2019-12-25 21:05:44] \n",
      "\tHR@1=0.22647  MRR@1=0.22647  NDCG@1=0.22647\n",
      "\tHR@5=0.42517  MRR@5=0.30063  NDCG@5=0.33169\n",
      "\tHR@10=0.50244  MRR@10=0.31113  NDCG@10=0.35687\n",
      "\tHR@20=0.57128  MRR@20=0.31598  NDCG@20=0.37436\n",
      "[2019-12-25 21:05:44] [29/50] 0 mean_batch_loss : 3.473543\n",
      "[2019-12-25 21:06:05] [29/50] 500 mean_batch_loss : 3.532964\n",
      "[2019-12-25 21:06:26] [29/50] 1000 mean_batch_loss : 3.619724\n",
      "Start predicting 2019-12-25 21:06:47\n",
      "change best\n",
      "testing finish [2019-12-25 21:06:50] \n",
      "\tHR@1=0.22609  MRR@1=0.22609  NDCG@1=0.22609\n",
      "\tHR@5=0.42700  MRR@5=0.30101  NDCG@5=0.33244\n",
      "\tHR@10=0.50483  MRR@10=0.31141  NDCG@10=0.35762\n",
      "\tHR@20=0.57311  MRR@20=0.31618  NDCG@20=0.37492\n",
      "[2019-12-25 21:06:50] [30/50] 0 mean_batch_loss : 3.362637\n",
      "[2019-12-25 21:07:10] [30/50] 500 mean_batch_loss : 3.516206\n",
      "[2019-12-25 21:07:33] [30/50] 1000 mean_batch_loss : 3.632688\n",
      "Start predicting 2019-12-25 21:07:52\n",
      "testing finish [2019-12-25 21:07:54] \n",
      "\tHR@1=0.22552  MRR@1=0.22552  NDCG@1=0.22552\n",
      "\tHR@5=0.42668  MRR@5=0.30044  NDCG@5=0.33192\n",
      "\tHR@10=0.50473  MRR@10=0.31094  NDCG@10=0.35725\n",
      "\tHR@20=0.57261  MRR@20=0.31563  NDCG@20=0.37439\n",
      "[2019-12-25 21:07:54] [31/50] 0 mean_batch_loss : 3.376543\n",
      "[2019-12-25 21:08:17] [31/50] 500 mean_batch_loss : 3.516638\n",
      "[2019-12-25 21:08:37] [31/50] 1000 mean_batch_loss : 3.610378\n",
      "Start predicting 2019-12-25 21:08:57\n",
      "testing finish [2019-12-25 21:09:00] \n",
      "\tHR@1=0.22545  MRR@1=0.22545  NDCG@1=0.22545\n",
      "\tHR@5=0.42693  MRR@5=0.30036  NDCG@5=0.33190\n",
      "\tHR@10=0.50617  MRR@10=0.31100  NDCG@10=0.35759\n",
      "\tHR@20=0.57216  MRR@20=0.31563  NDCG@20=0.37433\n",
      "[2019-12-25 21:09:00] [32/50] 0 mean_batch_loss : 3.581967\n",
      "[2019-12-25 21:09:22] [32/50] 500 mean_batch_loss : 3.510687\n",
      "[2019-12-25 21:09:41] [32/50] 1000 mean_batch_loss : 3.600517\n",
      "Start predicting 2019-12-25 21:10:02\n",
      "change best\n",
      "testing finish [2019-12-25 21:10:04] \n",
      "\tHR@1=0.22436  MRR@1=0.22436  NDCG@1=0.22436\n",
      "\tHR@5=0.42700  MRR@5=0.29975  NDCG@5=0.33147\n",
      "\tHR@10=0.50557  MRR@10=0.31028  NDCG@10=0.35693\n",
      "\tHR@20=0.57430  MRR@20=0.31508  NDCG@20=0.37433\n",
      "[2019-12-25 21:10:04] [33/50] 0 mean_batch_loss : 3.293730\n",
      "[2019-12-25 21:10:24] [33/50] 500 mean_batch_loss : 3.497594\n",
      "[2019-12-25 21:10:44] [33/50] 1000 mean_batch_loss : 3.597212\n",
      "Start predicting 2019-12-25 21:11:05\n",
      "change best\n",
      "testing finish [2019-12-25 21:11:07] \n",
      "\tHR@1=0.22763  MRR@1=0.22763  NDCG@1=0.22763\n",
      "\tHR@5=0.42844  MRR@5=0.30195  NDCG@5=0.33346\n",
      "\tHR@10=0.50673  MRR@10=0.31240  NDCG@10=0.35878\n",
      "\tHR@20=0.57483  MRR@20=0.31712  NDCG@20=0.37598\n",
      "[2019-12-25 21:11:07] [34/50] 0 mean_batch_loss : 3.416264\n",
      "[2019-12-25 21:11:20] [34/50] 500 mean_batch_loss : 3.495055\n",
      "[2019-12-25 21:11:36] [34/50] 1000 mean_batch_loss : 3.590490\n",
      "Start predicting 2019-12-25 21:11:52\n",
      "testing finish [2019-12-25 21:11:54] \n",
      "\tHR@1=0.22500  MRR@1=0.22500  NDCG@1=0.22500\n",
      "\tHR@5=0.42957  MRR@5=0.30071  NDCG@5=0.33280\n",
      "\tHR@10=0.50638  MRR@10=0.31099  NDCG@10=0.35767\n",
      "\tHR@20=0.57434  MRR@20=0.31573  NDCG@20=0.37488\n",
      "[2019-12-25 21:11:54] [35/50] 0 mean_batch_loss : 3.545543\n",
      "[2019-12-25 21:12:14] [35/50] 500 mean_batch_loss : 3.486229\n",
      "[2019-12-25 21:12:36] [35/50] 1000 mean_batch_loss : 3.582100\n",
      "Start predicting 2019-12-25 21:12:54\n",
      "change best\n",
      "testing finish [2019-12-25 21:12:56] \n",
      "\tHR@1=0.22971  MRR@1=0.22971  NDCG@1=0.22971\n",
      "\tHR@5=0.42960  MRR@5=0.30368  NDCG@5=0.33506\n",
      "\tHR@10=0.50568  MRR@10=0.31391  NDCG@10=0.35974\n",
      "\tHR@20=0.57381  MRR@20=0.31865  NDCG@20=0.37699\n",
      "[2019-12-25 21:12:56] [36/50] 0 mean_batch_loss : 3.563508\n",
      "[2019-12-25 21:13:17] [36/50] 500 mean_batch_loss : 3.485173\n",
      "[2019-12-25 21:13:39] [36/50] 1000 mean_batch_loss : 3.571052\n",
      "Start predicting 2019-12-25 21:13:57\n",
      "testing finish [2019-12-25 21:13:59] \n",
      "\tHR@1=0.22594  MRR@1=0.22594  NDCG@1=0.22594\n",
      "\tHR@5=0.42893  MRR@5=0.30195  NDCG@5=0.33365\n",
      "\tHR@10=0.50448  MRR@10=0.31209  NDCG@10=0.35814\n",
      "\tHR@20=0.57427  MRR@20=0.31697  NDCG@20=0.37581\n",
      "[2019-12-25 21:13:59] [37/50] 0 mean_batch_loss : 3.419868\n",
      "[2019-12-25 21:14:21] [37/50] 500 mean_batch_loss : 3.473730\n",
      "[2019-12-25 21:14:42] [37/50] 1000 mean_batch_loss : 3.566522\n",
      "Start predicting 2019-12-25 21:15:01\n",
      "testing finish [2019-12-25 21:15:03] \n",
      "\tHR@1=0.22770  MRR@1=0.22770  NDCG@1=0.22770\n",
      "\tHR@5=0.42858  MRR@5=0.30230  NDCG@5=0.33378\n",
      "\tHR@10=0.50343  MRR@10=0.31235  NDCG@10=0.35804\n",
      "\tHR@20=0.57212  MRR@20=0.31717  NDCG@20=0.37548\n",
      "[2019-12-25 21:15:03] [38/50] 0 mean_batch_loss : 3.218837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-12-25 21:15:24] [38/50] 500 mean_batch_loss : 3.471825\n",
      "[2019-12-25 21:15:44] [38/50] 1000 mean_batch_loss : 3.557729\n",
      "Start predicting 2019-12-25 21:16:04\n",
      "testing finish [2019-12-25 21:16:06] \n",
      "\tHR@1=0.22675  MRR@1=0.22675  NDCG@1=0.22675\n",
      "\tHR@5=0.42879  MRR@5=0.30205  NDCG@5=0.33366\n",
      "\tHR@10=0.50364  MRR@10=0.31211  NDCG@10=0.35793\n",
      "\tHR@20=0.57318  MRR@20=0.31699  NDCG@20=0.37558\n",
      "[2019-12-25 21:16:06] [39/50] 0 mean_batch_loss : 3.361989\n",
      "[2019-12-25 21:16:26] [39/50] 500 mean_batch_loss : 3.458809\n",
      "[2019-12-25 21:16:47] [39/50] 1000 mean_batch_loss : 3.560250\n",
      "Start predicting 2019-12-25 21:17:08\n",
      "testing finish [2019-12-25 21:17:10] \n",
      "\tHR@1=0.22798  MRR@1=0.22798  NDCG@1=0.22798\n",
      "\tHR@5=0.42823  MRR@5=0.30242  NDCG@5=0.33378\n",
      "\tHR@10=0.50642  MRR@10=0.31293  NDCG@10=0.35915\n",
      "\tHR@20=0.57321  MRR@20=0.31760  NDCG@20=0.37607\n",
      "[2019-12-25 21:17:10] [40/50] 0 mean_batch_loss : 3.318259\n",
      "[2019-12-25 21:17:28] [40/50] 500 mean_batch_loss : 3.460852\n",
      "[2019-12-25 21:17:50] [40/50] 1000 mean_batch_loss : 3.549670\n",
      "Start predicting 2019-12-25 21:18:09\n",
      "testing finish [2019-12-25 21:18:11] \n",
      "\tHR@1=0.22640  MRR@1=0.22640  NDCG@1=0.22640\n",
      "\tHR@5=0.42700  MRR@5=0.30115  NDCG@5=0.33253\n",
      "\tHR@10=0.50364  MRR@10=0.31146  NDCG@10=0.35740\n",
      "\tHR@20=0.57286  MRR@20=0.31629  NDCG@20=0.37492\n",
      "early stopping\n",
      "best model change\n",
      "current model hyper-parameters: session_length=20, hidden_size=100, lr=0.0010, dropout=0.00\n",
      "\n",
      "current model Recall@20=0.57381  MRR@20=0.31865\n",
      "the best result so far. Recall@20=0.57381  MRR@20=0.31865,数session_length=20, hidden_size=100, lr=0.0010, dropout=0.00\n",
      " \n",
      "\n",
      "The best result HR@20=0.57381  MRR@20=0.31865, hyper-parameters: session_length=20, hidden_size=100, lr=0.0010, dropout=0.00\n",
      ". \n",
      "over.\n"
     ]
    }
   ],
   "source": [
    "hidden_sizes = [100]\n",
    "dropouts = [0]\n",
    "lrs = [3e-3,1e-3]\n",
    "session_lengths = [20]\n",
    "patience = 5\n",
    "best_params = \"\"\n",
    "best_all_model = 0.0\n",
    "best_all_hr = 0.0\n",
    "best_all_mrr = 0.0\n",
    "best_all_r1m = 0.0\n",
    "for session_length in session_lengths:\n",
    "    for hidden_size in hidden_sizes:\n",
    "        for dropout in dropouts:\n",
    "            for lr in lrs:\n",
    "                args = {}\n",
    "                print(\"current model hyper-parameters: session_length=%d, hidden_size=%d, lr=%.4f, dropout=%.2f\\n\" % (session_length,hidden_size,lr,dropout))\n",
    "                args[\"session_length\"] = session_length\n",
    "                args[\"hidden_size\"] = hidden_size\n",
    "                args[\"dropout\"] = dropout\n",
    "                args[\"patience\"] = patience\n",
    "                args[\"lr\"] = lr\n",
    "                best_model,best_model_hr,best_model_mrr = train(args)\n",
    "                if best_model_hr + best_model_mrr > best_all_r1m:\n",
    "                    print(\"best model change\")\n",
    "                    best_all_r1m = best_model_hr + best_model_mrr\n",
    "                    best_all_hr = best_model_hr\n",
    "                    best_all_mrr = best_model_mrr\n",
    "                    best_all_model = best_model\n",
    "                    best_params = \"session_length=%d, hidden_size=%d, lr=%.4f, dropout=%.2f\\n\" % (session_length,hidden_size,lr,dropout)\n",
    "                best_model = None\n",
    "                print(\"current model hyper-parameters: session_length=%d, hidden_size=%d, lr=%.4f, dropout=%.2f\\n\" % (session_length,hidden_size,lr,dropout))\n",
    "                print(\"current model Recall@20=%.5f  MRR@20=%.5f\"%(best_all_hr,best_model_mrr))\n",
    "                print(\"the best result so far. Recall@20=%.5f  MRR@20=%.5f,数%s \\n\"%(best_all_hr,best_all_mrr,best_params))\n",
    "print(\"The best result HR@20=%.5f  MRR@20=%.5f, hyper-parameters: %s. \"%(best_all_hr,best_all_mrr,best_params))\n",
    "print(\"over.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smy",
   "language": "python",
   "name": "smy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
