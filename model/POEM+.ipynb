{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-26T12:34:10.365920Z",
     "start_time": "2019-12-26T12:34:09.900395Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "MhcOrxNibMjb"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "import copy\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-26T12:34:10.369447Z",
     "start_time": "2019-12-26T12:34:10.366990Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "EGxgHeR6bN0K"
   },
   "outputs": [],
   "source": [
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda:0\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-26T12:34:10.377899Z",
     "start_time": "2019-12-26T12:34:10.370549Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "tBeDjO8dbPzP"
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "plot_num = 50000\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-26T12:34:10.405534Z",
     "start_time": "2019-12-26T12:34:10.378788Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "zef81a0tbRC4"
   },
   "outputs": [],
   "source": [
    "class SessionData(object):\n",
    "    def __init__(self,session_index,session_id,items_indexes):\n",
    "        self.session_index = session_index\n",
    "        self.session_id = session_id\n",
    "        self.item_list = items_indexes\n",
    "    def generate_seq_datas(self,session_length,padding_idx=0,predict_length=1):\n",
    "        sessions = []\n",
    "        if len(self.item_list)<2:\n",
    "            self.item_list.append[self.item_list[0]]\n",
    "        if predict_length==1:\n",
    "            # when session length>=3\n",
    "            for i in range(1,len(self.item_list)-1):\n",
    "#             # when session length >=2\n",
    "#             for i in range(len(self.item_list)-1):\n",
    "                if i <session_length:\n",
    "                    train_data = [0 for _ in range(session_length-i-1)]\n",
    "                    train_data.extend(self.item_list[:i+1])\n",
    "                    train_data.append(self.item_list[i+1])\n",
    "                else:\n",
    "                    train_data = self.item_list[i+1-session_length:i+1]\n",
    "                    train_data.append(self.item_list[i+1])\n",
    "                sessions.append(train_data)\n",
    "        else:\n",
    "            pass\n",
    "        return self.session_index,sessions\n",
    "    def __str__(self):\n",
    "        info = \" session index = {}\\n session id = {} \\n the length of item list= {} \\n the fisrt item index in item list is {}\".format(self.session_index,self.session_id,len(self.item_list),self.item_list[0])\n",
    "        return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-26T12:34:10.435644Z",
     "start_time": "2019-12-26T12:34:10.407185Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "hl6NiyNJbTRg"
   },
   "outputs": [],
   "source": [
    "class SessionDataSet(object):\n",
    "    def __init__(self,train_file,test_file,padding_idx=0):\n",
    "        super(SessionDataSet,self).__init__()\n",
    "        self.index_count = 0\n",
    "        self.session_count = 0\n",
    "        self.train_count = 0\n",
    "        self.test_count = 0\n",
    "        self.max_session_length = 0\n",
    "\n",
    "        self.padding_idx = padding_idx\n",
    "        self.item2index = dict()\n",
    "        self.index2item = dict()\n",
    "        self.session2index = dict()\n",
    "        self.index2session = dict()\n",
    "        self.item_total_num = dict()\n",
    "        self.item2index[\"<pad>\"] = padding_idx\n",
    "        self.index2item[padding_idx] = \"<pad>\"\n",
    "        self.train_data = self.load_data(train_file)\n",
    "        print(\"training set is loaded, # index: \",len(self.item2index.keys()))\n",
    "        self.train_count = self.session_count\n",
    "        print(\"train_session_num\",self.train_count)\n",
    "        self.test_data = self.load_data(test_file)\n",
    "        print(\"testing set is loaded, # index: \",len(self.index2item.keys()))\n",
    "        print(\"# item\",self.index_count)\n",
    "        self.test_count = self.session_count-self.train_count\n",
    "        print(\"# test session:\",self.test_count)\n",
    "        self.all_training_data = []\n",
    "        self.all_testing_data = []\n",
    "        self.all_meta_training_data = []\n",
    "        self.all_meta_testing_data = []\n",
    "        self.train_session_length = 0\n",
    "        self.test_session_length = 0\n",
    "    \n",
    "    def load_data(self,file_path):\n",
    "        data =  pickle.load(open(file_path, 'rb'))\n",
    "        session_ids = data[0]\n",
    "        session_data = data[1]\n",
    "        session_label = data[2]\n",
    "\n",
    "        result_data = []\n",
    "        lenth = len(session_ids)\n",
    "        print(\"# session\",lenth)\n",
    "\n",
    "        last_session_id = session_ids[0]\n",
    "        \n",
    "        session_item_indexes = []\n",
    "\n",
    "        for item_id in session_data[0]:\n",
    "            if item_id not in self.item2index.keys():\n",
    "                self.index_count+=1\n",
    "                self.item2index[item_id] = self.index_count\n",
    "                self.index2item[self.index_count] = item_id\n",
    "                self.item_total_num[self.index_count] = 0\n",
    "            session_item_indexes.append(self.item2index[item_id])\n",
    "            self.item_total_num[self.item2index[item_id]] += 1\n",
    "        target_item = session_label[0]\n",
    "        if target_item not in self.item2index.keys():\n",
    "            self.index_count+=1\n",
    "            self.item2index[target_item] = self.index_count\n",
    "            self.index2item[self.index_count] = target_item\n",
    "            self.item_total_num[self.index_count] = 0\n",
    "        session_item_indexes.append(self.item2index[target_item])\n",
    "        self.item_total_num[self.item2index[target_item]] += 1\n",
    "\n",
    "        for session_id,items,target_item in zip(session_ids,session_data,session_label):\n",
    "            if session_id!=last_session_id:\n",
    "\n",
    "                self.session_count+=1\n",
    "                self.session2index[last_session_id] = self.session_count\n",
    "                self.index2session[self.session_count] = last_session_id\n",
    "                last_session_id = session_id\n",
    "                if len(session_item_indexes)>self.max_session_length:\n",
    "                    self.max_session_length = len(session_item_indexes)\n",
    "                new_session = SessionData(self.session_count,last_session_id,session_item_indexes)\n",
    "                result_data.append(new_session)\n",
    "                session_item_indexes = []\n",
    "                for item_id in items:\n",
    "                    if item_id not in self.item2index.keys():\n",
    "                        self.index_count+=1\n",
    "                        self.item2index[item_id] = self.index_count\n",
    "                        self.index2item[self.index_count] = item_id\n",
    "                        self.item_total_num[self.index_count] = 0\n",
    "                    session_item_indexes.append(self.item2index[item_id])\n",
    "                    self.item_total_num[self.item2index[item_id]] += 1\n",
    "                if target_item not in self.item2index.keys():\n",
    "                    self.index_count+=1\n",
    "                    self.item2index[target_item] = self.index_count\n",
    "                    self.index2item[self.index_count] = target_item\n",
    "                    self.item_total_num[self.index_count] = 0\n",
    "                session_item_indexes.append(self.item2index[target_item])\n",
    "                self.item_total_num[self.item2index[target_item]] += 1\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        self.session_count+=1\n",
    "        self.session2index[last_session_id] = self.session_count\n",
    "        new_session = SessionData(self.session_count,last_session_id,session_item_indexes)\n",
    "        result_data.append(new_session)\n",
    "        print(\"loaded\")\n",
    "        print(new_session)\n",
    "        \n",
    "        return result_data\n",
    "    \n",
    "\n",
    "    def get_batch(self,batch_size,session_length=10,predict_length=1,all_data=None,phase=\"train\",neg_num=1,sampling_mathod=\"random\"):\n",
    "\n",
    "        if phase == \"train\":\n",
    "            if all_data is None:\n",
    "                all_data = self.get_all_training_data(session_length)\n",
    "            indexes = np.random.permutation(all_data.shape[0])\n",
    "            all_data = all_data[indexes]\n",
    "        else:\n",
    "            if all_data is None:\n",
    "                all_data = self.get_all_testing_data(session_length)\n",
    "        \n",
    "        sindex = 0\n",
    "        eindex = batch_size\n",
    "        while eindex < all_data.shape[0]:\n",
    "            batch = all_data[sindex: eindex]\n",
    "\n",
    "            temp = eindex\n",
    "            eindex = eindex + batch_size\n",
    "            sindex = temp\n",
    "            if phase ==\"train\":\n",
    "                batch = self.divid_and_extend_negative_samples(batch,session_length=session_length,predict_length=predict_length,neg_num=neg_num,method=sampling_mathod)\n",
    "            else:\n",
    "                batch = [batch[:,:session_length],batch[:,session_length:]]\n",
    "            yield batch\n",
    "\n",
    "        if eindex >= all_data.shape[0]:\n",
    "            batch = all_data[sindex:]\n",
    "            if phase ==\"train\":\n",
    "                batch = self.divid_and_extend_negative_samples(batch,session_length=session_length,predict_length=predict_length,neg_num=neg_num,method=sampling_mathod)\n",
    "            else:\n",
    "                batch = [batch[:,:session_length],batch[:,session_length:]]\n",
    "            yield batch\n",
    "    \n",
    "    def divid_and_extend_negative_samples(self,batch_data,session_length,predict_length=1,neg_num=1,method=\"random\"):\n",
    "        \"\"\"\n",
    "        divid and extend negative samples\n",
    "        \"\"\"\n",
    "        neg_items = []\n",
    "        if method == \"random\":\n",
    "            for session_and_target in batch_data:\n",
    "                neg_item = []\n",
    "                for i in range(neg_num):\n",
    "                    rand_item = random.randint(1,self.index_count)\n",
    "                    while rand_item in session_and_target or rand_item in neg_item:\n",
    "                        rand_item = random.randint(1,self.index_count)\n",
    "                    neg_item.append(rand_item)\n",
    "                neg_items.append(neg_item)\n",
    "        else:\n",
    "\n",
    "            total_list = set()\n",
    "            for session in batch_data:\n",
    "                for i in session:\n",
    "                    total_list.add(i) \n",
    "            total_list = list(total_list)\n",
    "            total_list =  sorted(total_list, key=lambda item: self.item_total_num[item],reverse=True)\n",
    "            for i,session in enumerate(batch_data):\n",
    "                np.random.choice(total_list)\n",
    "        session_items = batch_data[:,:session_length]\n",
    "        target_item = batch_data[:,session_length:]\n",
    "        neg_items = np.array(neg_items)\n",
    "        return [session_items,target_item,neg_items]\n",
    "    \n",
    "    def get_all_training_data(self,session_length,predict_length=1):\n",
    "        if len(self.all_training_data)!=0 and self.train_session_length==session_length:\n",
    "#             print(\"The build is complete and there is no need to repeat the build\")\n",
    "            return self.all_training_data\n",
    "        print(\"Start building the all training dataset\")\n",
    "        all_sessions = []\n",
    "        for session_data in self.train_data:\n",
    "            # 前session_length为session，后predict_length为target_item\n",
    "            session_index,sessions = session_data.generate_seq_datas(session_length,padding_idx=self.padding_idx)\n",
    "            if sessions is not None:\n",
    "                all_sessions.extend(sessions)\n",
    "        all_sessions = np.array(all_sessions)\n",
    "        self.all_training_data = all_sessions\n",
    "        self.train_session_length=session_length\n",
    "        print(\"The total number of training samples is：\",all_sessions.shape)\n",
    "        return all_sessions\n",
    "    \n",
    "    def get_all_testing_data(self,session_length,predict_length=1):\n",
    "        if len(self.all_testing_data)!=0 and self.test_session_length==session_length:\n",
    "            return self.all_testing_data\n",
    "        all_sessions = []\n",
    "        for session_data in self.test_data:\n",
    "            session_index,sessions = session_data.generate_seq_datas(session_length,padding_idx=self.padding_idx)\n",
    "            if sessions is not None:\n",
    "                all_sessions.extend(sessions)\n",
    "        all_sessions = np.array(all_sessions)\n",
    "        self.all_testing_data = all_sessions\n",
    "        self.test_session_length=session_length\n",
    "        print(\"The total number of testing samples is：\",all_sessions.shape)\n",
    "        return all_sessions\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-26T12:34:11.519864Z",
     "start_time": "2019-12-26T12:34:10.436981Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "ctSu0HF8bUqh",
    "outputId": "7287ecc0-f73d-4883-c8f6-20c9b8aeddb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# session 433648\n",
      "loaded\n",
      " session index = 126168\n",
      " session id = 946108 \n",
      " the length of item list= 3 \n",
      " the fisrt item index in item list is 3314\n",
      "training set is loaded, # index:  36969\n",
      "train_session_num 126168\n",
      "# session 15132\n",
      "loaded\n",
      " session index = 130903\n",
      " session id = 1582915 \n",
      " the length of item list= 6 \n",
      " the fisrt item index in item list is 12498\n",
      "testing set is loaded, # index:  36969\n",
      "# item 36968\n",
      "# test session: 4735\n"
     ]
    }
   ],
   "source": [
    "# dataset = SessionDataSet(train_file=\"../data/retailrocket_gcsan_my/train.txt\",test_file=\"../data/srgnn/retailrocket_gcsan_my/test.txt\")\n",
    "dataset = SessionDataSet(train_file=\"../data/diginetica_gcsan_my/train.txt\",test_file=\"../data/srgnn/diginetica_gcsan_my/test.txt\")\n",
    "# dataset = SessionDataSet(train_file=\"../data/yoochoose1_4_gcsan_my/train.txt\",test_file=\"../data/srgnn/yoochoose1_4_gcsan_my/test.txt\")\n",
    "# dataset = SessionDataSet(train_file=\"../data/yoochoose1_64_gcsan_my/train.txt\",test_file=\"../data/srgnn/yoochoose1_64_gcsan_my/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-26T12:34:11.574860Z",
     "start_time": "2019-12-26T12:34:11.520746Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "cm7qU4B6bq2i"
   },
   "outputs": [],
   "source": [
    "def bpr_loss(r):\n",
    "    return torch.sum(-torch.log(torch.sigmoid(r)))\n",
    "def get_hit_num(pred,y_truth):\n",
    "    \"\"\"\n",
    "        pred: numpy type(batch_size,k) \n",
    "        y_truth: list type (batch_size,groudtruth_num)\n",
    "    \"\"\"\n",
    "\n",
    "    hit_num = 0\n",
    "    for i in range(len(y_truth)):\n",
    "        for value in y_truth[i]:\n",
    "            hit_num += np.sum(pred[i]==value)\n",
    "    return hit_num\n",
    "\n",
    "def get_rr(pred,y_truth):\n",
    "    rr=0.\n",
    "    for i in range(len(y_truth)):\n",
    "        for value in y_truth[i]:\n",
    "            hit_indexes = np.where(pred[i]==value)[0]\n",
    "            for hit_index in hit_indexes:\n",
    "                rr += 1/(hit_index+1)\n",
    "    return rr\n",
    "\n",
    "def get_dcg(pred,y_truth):\n",
    "    y_pred_score = np.zeros_like(pred)\n",
    "\n",
    "    for i in range(len(y_truth)):\n",
    "\n",
    "        for j,y_pred in enumerate(pred[i]):\n",
    "            if y_pred == y_truth[i][0]:\n",
    "                y_pred_score[i][j]=1\n",
    "    gain = 2 ** y_pred_score - 1\n",
    "    discounts = np.tile(np.log2(np.arange(pred.shape[1]) + 2),(len(y_truth),1))\n",
    "    dcg = np.sum(gain / discounts,axis=1)\n",
    "    return dcg\n",
    "\n",
    "def get_ndcg(pred,y_truth):\n",
    "    dcg = get_dcg(pred, y_truth)\n",
    "    idcg = get_dcg(np.concatenate((y_truth,np.zeros_like(pred)[:,:-1]-1),axis=1), y_truth)\n",
    "    ndcg = np.sum(dcg / idcg)\n",
    "\n",
    "    return ndcg\n",
    "\n",
    "def dcg_score(y_pre, y_true, k):\n",
    "    y_pre_score = np.zeros(k)\n",
    "    if len(y_pre) > k:\n",
    "        y_pre = y_pre[:k]\n",
    "    for i in range(len(y_pre)):\n",
    "        pre_tag = y_pre[i]\n",
    "        if pre_tag in y_true:\n",
    "            y_pre_score[i] = 1\n",
    "    gain = 2 ** y_pre_score - 1\n",
    "    discounts = np.log2(np.arange(k) + 2)\n",
    "    return np.sum(gain / discounts)\n",
    "\n",
    "\n",
    "def ndcg_score(y_pre, y_true, k=5):\n",
    "    dcg = dcg_score(y_pre, y_true, k)\n",
    "    idcg = dcg_score(y_true, y_true, k)\n",
    "    return dcg / idcg\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-26T12:34:11.597252Z",
     "start_time": "2019-12-26T12:34:11.575891Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "QdNvTTApbr_g"
   },
   "outputs": [],
   "source": [
    "# SelfAttention Layer\n",
    "class SelfAttention(torch.nn.Module):\n",
    "    def __init__(self, method, hidden_size,activate=\"selu\",dropout=0):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.config = list()\n",
    "        # 使用的Attention方法\n",
    "        self.method = method\n",
    "        # 隐藏层大小\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method not in ['dot', 'general']:\n",
    "            raise ValueError(self.method, \"Attention method do not exists.\")\n",
    "\n",
    "        if self.method == \"dot\":\n",
    "            self.query = torch.nn.Linear(self.hidden_size *2, self.hidden_size*2)\n",
    "            self.key = torch.nn.Linear(self.hidden_size*2, self.hidden_size*2)\n",
    "            torch.nn.init.constant_(self.query.bias,0)\n",
    "            torch.nn.init.constant_(self.key.bias,0)\n",
    "\n",
    "        if self.method == \"general\":\n",
    "            self.attention = torch.nn.Linear(self.hidden_size*2, self.hidden_size*2)\n",
    "            torch.nn.init.constant_(self.attention.bias,0)\n",
    "        \n",
    "        if activate == \"relu\":\n",
    "            self.activate = torch.relu\n",
    "        elif activate == \"tanh\":\n",
    "            self.activate = torch.tanh\n",
    "        elif activate == \"elu\":\n",
    "            self.activate = torch.nn.ELU()\n",
    "        elif activate == \"selu\":\n",
    "            self.activate = torch.selu\n",
    "        else:\n",
    "            self.activate = torch.sigmoid\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        torch.nn.utils.clip_grad_norm_(self.parameters(),max_norm=110)\n",
    "\n",
    "    def dot_score(self, encoder_output,is_train=True,weights=None):\n",
    "\n",
    "        if weights is None:\n",
    "            if is_train:\n",
    "                query = self.dropout(self.activate(self.query(encoder_output)))\n",
    "                key = self.dropout(self.activate(self.key(encoder_output)))\n",
    "            else:\n",
    "                query = self.activate(self.query(encoder_output))\n",
    "                key = self.activate(self.key(encoder_output))\n",
    "        else:\n",
    "            query = self.activate(torch.matmul(encoder_output,weights[0].t())+weights[1])\n",
    "            key = self.activate(torch.matmul(encoder_output,weights[2].t())+weights[3])\n",
    "        dot = query.bmm(key.permute(0, 2, 1))\n",
    "        return dot\n",
    "\n",
    "    def general_score(self, encoder_output,is_train=True,weights=None):\n",
    "        if weights is None:\n",
    "            if is_train:\n",
    "                energy = self.dropout(self.activate(self.attention(encoder_output)))\n",
    "            else:\n",
    "                energy = self.activate(self.attention(encoder_output))\n",
    "        else:\n",
    "            energy = self.activate(torch.matmul(encoder_output,weights[0].t())+weights[1])\n",
    "        return encoder_output.bmm(energy.permute(0, 2, 1))\n",
    "\n",
    "    def forward(self, encoder_outputs, mask=None,is_train=True):\n",
    "        # (batch_size,length,dim)\n",
    "        if self.method == \"general\":\n",
    "            attention_energies = self.general_score(encoder_outputs,is_train=is_train)\n",
    "        elif self.method == \"dot\":\n",
    "            attention_energies = self.dot_score(encoder_outputs,is_train=is_train)\n",
    "\n",
    "        #  (batch_size,length,length)\n",
    "        attention_energies.div_(torch.sqrt(torch.tensor(self.hidden_size, dtype=torch.float)))\n",
    "        if mask is not None:\n",
    "            new_mask = (1 - (1 - mask.float()).unsqueeze(1).permute(0, 2, 1).bmm(\n",
    "                (1 - mask.float()).unsqueeze(1)))\n",
    "\n",
    "            attention_energies = attention_energies - new_mask*1e12\n",
    "            weights = F.softmax(attention_energies, dim=2)\n",
    "            weights = weights*(1-new_mask)\n",
    "            # batch_size,length,length)*(batch_size,length,dim)->(batch_size,length,dim)->(batch_size,1,dim)->(batch_size,dim)\n",
    "            outputs = weights.bmm(encoder_outputs)\n",
    "            outputs.div_(mask.shape[1]-torch.sum(mask,dim=1).unsqueeze(1).unsqueeze(2).repeat((1,mask.shape[1],outputs.shape[2])).float())\n",
    "            outputs = outputs.sum(dim=1).squeeze(1)\n",
    "        else:\n",
    "            weights = F.softmax(attention_energies, dim=2)\n",
    "            # (batch_size,length,length)*(batch_size,length,dim)->(batch_size,length,dim)->(batch_size,1,dim)->(batch_size,dim)\n",
    "            outputs = (weights.bmm(encoder_outputs).sum(dim=1) / encoder_outputs.shape[1]).squeeze(1)\n",
    "        sa_weights = weights.sum(dim=1).squeeze(1)\n",
    "        return outputs, sa_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-26T12:34:11.613176Z",
     "start_time": "2019-12-26T12:34:11.598242Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "aVIrdlmKbtUq"
   },
   "outputs": [],
   "source": [
    "class POEM(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=64, itemNum=0, posNum=0, padding_idx=0, dropout=0.5,attention_method=\"dot\",head_num=4,\n",
    "                 activate=\"selu\",session_length=20,delta=16.0):\n",
    "        super(POEM, self).__init__()\n",
    "        self.padding_idx = padding_idx\n",
    "        self.hidden_size = hidden_size\n",
    "        self.head_num = head_num\n",
    "        self.delta = delta\n",
    "        self.session_length = session_length\n",
    "        if activate == \"sigmoid\":\n",
    "            self.activate = torch.sigmoid\n",
    "        elif activate == \"tanh\":\n",
    "            self.activate = torch.tanh\n",
    "        elif activate == \"relu\":\n",
    "            self.activate = torch.relu\n",
    "        elif activate == \"elu\":\n",
    "            self.activate = torch.nn.ELU()\n",
    "        else:\n",
    "            self.activate = torch.selu\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "        self.item_embedding = torch.nn.Embedding(itemNum, hidden_size, padding_idx=self.padding_idx,max_norm=1.5)\n",
    "        \n",
    "        self.position_embedding = torch.nn.Embedding(posNum,hidden_size,padding_idx=self.padding_idx,max_norm=1.5)\n",
    "    \n",
    "        self.position_weights = torch.nn.Embedding(posNum,1,padding_idx=self.padding_idx)\n",
    "        \n",
    "        self.self_attention = SelfAttention(attention_method, hidden_size,activate=activate,dropout=dropout).to(device)\n",
    "        torch.nn.init.constant_(self.item_embedding.weight[0],0)\n",
    "        torch.nn.init.constant_(self.position_embedding.weight[0],0)\n",
    "        torch.nn.init.constant_(self.position_weights.weight,1)\n",
    "        torch.nn.init.constant_(self.position_weights.weight[0],0)\n",
    "        \n",
    "        self.gen_mlp = torch.nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.cur_mlp = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.deep_mlp = torch.nn.Linear(hidden_size*3, hidden_size,bias=False)\n",
    "        \n",
    "    def forward(self, session,item=None,bpr_loss=False,neg_num=50):\n",
    "\n",
    "        mask = (session!=0).float()\n",
    "        length = torch.sum(mask,1).unsqueeze(1).expand((session.shape[0],self.hidden_size))\n",
    "        mask = mask.unsqueeze(2).expand((session.shape[0],session.shape[1],self.hidden_size))\n",
    "        session_item_embeddings = F.normalize(self.item_embedding(session),dim=-1)* mask\n",
    "        positions = session.shape[1] - torch.arange(0,session.shape[1]).unsqueeze(0).expand_as(session).to(device)\n",
    "        session_position_embeddings = self.dropout(self.position_embedding(positions))*mask\n",
    "        session_item_vecs = torch.cat((session_item_embeddings,session_position_embeddings), dim=2)\n",
    "        attention_mask = (session == self.padding_idx)\n",
    "        sa_output, sa_weights = self.self_attention(session_item_vecs, attention_mask)\n",
    "        session_position_weights = self.dropout(self.position_weights(positions))*mask\n",
    "        sa_weights = sa_weights.unsqueeze(2).expand_as(session_item_embeddings)\n",
    "        session_item_vecs2 = session_item_embeddings * session_position_weights * sa_weights\n",
    "        psa_output = torch.sum(session_item_vecs2, dim=1)/length\n",
    "        gen_output = self.dropout(self.activate(self.gen_mlp(sa_output)))\n",
    "        cur_output = self.dropout(self.activate(self.cur_mlp(session_item_embeddings[:,-1])))\n",
    "        deep_output = self.dropout(self.activate(self.deep_mlp(torch.cat((sa_output,session_item_embeddings[:,-1]),1))))\n",
    "        session_output =  F.normalize(gen_output * cur_output + deep_output + psa_output,dim=-1)\n",
    "        session_output = session_output*self.delta\n",
    "        item_embedding_weight = F.normalize(self.item_embedding.weight[1:],dim=-1)\n",
    "        result = torch.matmul(session_output,item_embedding_weight.t())\n",
    "        return result\n",
    "    \n",
    "    def predict_top_k(self, session, k=20):\n",
    "        mask = (session!=0).float()\n",
    "        length = torch.sum(mask,1).unsqueeze(1).expand((session.shape[0],self.hidden_size))\n",
    "        mask = mask.unsqueeze(2).expand((session.shape[0],session.shape[1],self.hidden_size))\n",
    "        session_item_embeddings = F.normalize(self.item_embedding(session),dim=-1)* mask\n",
    "        positions = session.shape[1] - torch.arange(0,session.shape[1]).unsqueeze(0).expand_as(session).to(device)\n",
    "        session_position_embeddings = self.position_embedding(positions)*mask\n",
    "        session_item_vecs = torch.cat((session_item_embeddings,session_position_embeddings), dim=2)\n",
    "        attention_mask = (session == self.padding_idx)\n",
    "        sa_output, sa_weights = self.self_attention(session_item_vecs, attention_mask,is_train=False)\n",
    "        session_position_weights = self.position_weights(positions)*mask\n",
    "        sa_weights = sa_weights.unsqueeze(2).expand_as(session_item_embeddings)\n",
    "        session_item_vecs2 = session_item_embeddings * session_position_weights * sa_weights\n",
    "        psa_output = torch.sum(session_item_vecs2, dim=1)/length\n",
    "        gen_output =self.activate(self.gen_mlp(sa_output))\n",
    "\n",
    "        cur_output = self.activate(self.cur_mlp(session_item_embeddings[:,-1]))\n",
    "        deep_output = self.activate(self.deep_mlp(torch.cat((sa_output,session_item_embeddings[:,-1]),1)))\n",
    "        session_output =  F.normalize(gen_output * cur_output + deep_output + psa_output,dim=-1)\n",
    "        session_output = session_output * self.delta\n",
    "        item_embedding_weight = F.normalize(self.item_embedding.weight[1:],dim=-1)\n",
    "        result = torch.matmul(session_output,item_embedding_weight.t())\n",
    "        result = torch.topk(result,k,dim=1)[1]\n",
    "        \n",
    "        return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-26T12:34:11.625956Z",
     "start_time": "2019-12-26T12:34:11.614137Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Jhrg56xebung"
   },
   "outputs": [],
   "source": [
    "epochs=50\n",
    "def train(args):\n",
    "    hidden_size = args[\"hidden_size\"] if \"hidden_size\" in args.keys() else 100\n",
    "    dropout = args[\"dropout\"] if \"dropout\" in args.keys()  else 0.5\n",
    "    attention_method = args[\"method\"] if \"method\" in args.keys()  else \"general\"\n",
    "    lr = args[\"lr\"] if \"lr\" in args.keys()  else 5e-4\n",
    "    weight_decay = args[\"weight_decay\"] if \"weight_decay\" in args.keys()  else 1e-5\n",
    "    amsgrad = args[\"amsgrad\"] if \"amsgrad\" in args.keys() else True\n",
    "    session_length = args[\"session_length\"] if \"session_length\" in args.keys() else 20\n",
    "    delta = args[\"delta\"] if \"delta\" in args.keys() else 20\n",
    "    model = POEM(hidden_size=hidden_size, itemNum=dataset.index_count+1, posNum=session_length+1, padding_idx=0, dropout=dropout,\n",
    "                 activate=\"selu\",attention_method=attention_method,delta=delta).to(device)\n",
    "    opti = torch.optim.Adam(model.parameters(),lr=lr,weight_decay=weight_decay,amsgrad=amsgrad)\n",
    "    patience = args[\"patience\"] if \"patience\" in args.keys() else 5\n",
    "    best_model_hr = 0.0\n",
    "    best_model_mrr = 0.0\n",
    "    best_r1m = 0.0\n",
    "    best_model = None\n",
    "    predict_nums = [1,5,10,20]\n",
    "    no_improvement_epoch = 0\n",
    "    for epoch in range(epochs):\n",
    "        batch_losses = []\n",
    "        epoch_losses = []\n",
    "        for i,batch_data in enumerate(dataset.get_batch(batch_size,session_length,phase=\"train\")):\n",
    "            sessions = torch.tensor(batch_data[0]).to(device)\n",
    "            target_items = torch.tensor(batch_data[1]).squeeze().to(device)-1\n",
    "            result_pos = model(sessions)\n",
    "            loss = loss_function(result_pos,target_items)\n",
    "            opti.zero_grad()\n",
    "            loss.backward()\n",
    "            opti.step()\n",
    "            batch_losses.append(loss.cpu().detach().numpy())\n",
    "            epoch_losses.append(loss.cpu().detach().numpy())\n",
    "            if i % plot_num == 0:\n",
    "                time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                print(\"[%s] [%d/%d] %d mean_batch_loss : %0.6f\" % (time, epoch+1, epochs, i, np.mean(batch_losses)))\n",
    "                batch_losses = []\n",
    "        with torch.no_grad():\n",
    "            start_test_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(\"Start predicting\",start_test_time)\n",
    "            rrs = [0 for _ in range(len(predict_nums))]\n",
    "            hit_nums = [0 for _ in range(len(predict_nums))]\n",
    "            ndcgs = [0 for _ in range(len(predict_nums))]\n",
    "            for i,batch_data in enumerate(dataset.get_batch(batch_size,session_length,phase=\"test\")):\n",
    "                \n",
    "                sessions = torch.tensor(batch_data[0]).to(device)\n",
    "                target_items = np.array(batch_data[1])-1\n",
    "                y_pred = model.predict_top_k(sessions,20).cpu().numpy()\n",
    "                \n",
    "                for j,predict_num in enumerate(predict_nums):\n",
    "                    hit_nums[j]+=get_hit_num(y_pred[:,:predict_num],target_items)\n",
    "                    rrs[j]+=get_rr(y_pred[:,:predict_num],target_items)\n",
    "                    ndcgs[j]+=get_ndcg(y_pred[:,:predict_num],target_items)\n",
    "                    \n",
    "            end_test_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            hrs = [hit_num/len(dataset.all_testing_data) for hit_num in hit_nums]\n",
    "            mrrs = [rr/len(dataset.all_testing_data) for rr in rrs]\n",
    "            mndcgs = [ndcg/len(dataset.all_testing_data) for ndcg in ndcgs]\n",
    "            if hrs[-1] + mrrs[-1] > best_r1m:\n",
    "                print(\"change best\")\n",
    "                best_model = deepcopy(model)\n",
    "                best_model_hr = hrs[-1]\n",
    "                best_model_mrr = mrrs[-1]\n",
    "                best_r1m = hrs[-1] + mrrs[-1]\n",
    "                no_improvement_epoch = 0\n",
    "            else:\n",
    "                no_improvement_epoch +=1\n",
    "            print(\"testing finish [%s] \"%end_test_time)\n",
    "            for k,predict_num in enumerate(predict_nums):\n",
    "                print(\"\\tHR@%d=%.5f  MRR@%d=%.5f  NDCG@%d=%.5f\"%(predict_num,hrs[k],predict_num,mrrs[k],predict_num,mndcgs[k]))\n",
    "        if no_improvement_epoch>=patience:\n",
    "            print(\"early stopping\")\n",
    "            break\n",
    "    return best_model,best_model_hr,best_model_mrr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2H2SFJ7F77Z0"
   },
   "source": [
    "# CIKM-Session>2\n",
    "    HR@20=0.67804  MRR@20=0.33200, hyper-parameters: session_length-20, hidden_size-100, lr-0.0010,delta=16.0, amsgrad-True, method-general, dropout-0.5, weight_decay-0.000000. \n",
    "        HR@1=0.21631  MRR@1=0.21631  NDCG@1=0.21631\n",
    "        HR@5=0.46975  MRR@5=0.31073  NDCG@5=0.35037\n",
    "        HR@10=0.57556  MRR@10=0.32485  NDCG@10=0.38459\n",
    "        HR@20=0.67804  MRR@20=0.33200  NDCG@20=0.41054\n",
    "# RR-Session>2\n",
    "    HR@20=0.57600  MRR@20=0.31254, hyper-parameters: session_length-20, hidden_size-100, lr-0.0010,delta=16.0, amsgrad-True, method-general, dropout-0.5, weight_decay-0.000000. \n",
    "        HR@1=0.22548  MRR@1=0.22548  NDCG@1=0.22548\n",
    "        HR@5=0.42030  MRR@5=0.29653  NDCG@5=0.32730\n",
    "        HR@10=0.49914  MRR@10=0.30720  NDCG@10=0.35294\n",
    "        HR@20=0.57600  MRR@20=0.31254  NDCG@20=0.37238\n",
    "# RSC64-Session>2\n",
    "    HR@20=0.71005  MRR@20=0.29621, hyper-parameters: session_length-20, hidden_size-100, lr-0.0005,delta=16.0, amsgrad-True, method-general, dropout-0.5, weight_decay-0.000000. \n",
    "        HR@1=0.16425  MRR@1=0.16425  NDCG@1=0.16425\n",
    "        HR@5=0.46047  MRR@5=0.26957  NDCG@5=0.31691\n",
    "        HR@10=0.60094  MRR@10=0.28852  NDCG@10=0.36253\n",
    "        HR@20=0.71005  MRR@20=0.29621  NDCG@20=0.39027\n",
    "# RSC4-Session>2\n",
    "    HR@20=0.72369  MRR@20=0.30340, hyper-parameters: session_length-20, hidden_size-100, lr-0.0005,delta=16.0, amsgrad-True, method-general, dropout-0.3, weight_decay-0.000000. \n",
    "        HR@1=0.16948  MRR@1=0.16948  NDCG@1=0.16948\n",
    "        HR@5=0.46912  MRR@5=0.27635  NDCG@5=0.32416\n",
    "        HR@10=0.61063  MRR@10=0.29543  NDCG@10=0.37012\n",
    "        HR@20=0.72369  MRR@20=0.30340  NDCG@20=0.39886"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-26T12:39:12.625089Z",
     "start_time": "2019-12-26T12:34:11.626861Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "S8pn0Z7bbv4t",
    "outputId": "c1aa9692-56a0-40d2-d99b-e75b4f1e537e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current model hyper-parameters: session_length=20, hidden_size=100, lr=0.0010,delta=16.0, amsgrad=True, method=general, dropout=0.5, weight_decay=0.000000. \n",
      "\n",
      "Start building the all training dataset\n",
      "The total number of training samples is： (433648, 21)\n",
      "[2019-12-26 20:34:16] [1/50] 0 mean_batch_loss : 9.858598\n",
      "Start predicting 2019-12-26 20:34:31\n",
      "The total number of testing samples is： (15132, 21)\n",
      "change best\n",
      "testing finish [2019-12-26 20:34:32] \n",
      "\tHR@1=0.20011  MRR@1=0.20011  NDCG@1=0.20011\n",
      "\tHR@5=0.35065  MRR@5=0.26428  NDCG@5=0.28624\n",
      "\tHR@10=0.37483  MRR@10=0.26753  NDCG@10=0.29408\n",
      "\tHR@20=0.39671  MRR@20=0.26905  NDCG@20=0.29961\n",
      "[2019-12-26 20:34:32] [2/50] 0 mean_batch_loss : 6.961129\n",
      "Start predicting 2019-12-26 20:34:44\n",
      "change best\n",
      "testing finish [2019-12-26 20:34:45] \n",
      "\tHR@1=0.21425  MRR@1=0.21425  NDCG@1=0.21425\n",
      "\tHR@5=0.38805  MRR@5=0.28305  NDCG@5=0.30944\n",
      "\tHR@10=0.43438  MRR@10=0.28933  NDCG@10=0.32452\n",
      "\tHR@20=0.47363  MRR@20=0.29209  NDCG@20=0.33449\n",
      "[2019-12-26 20:34:45] [3/50] 0 mean_batch_loss : 6.041860\n",
      "Start predicting 2019-12-26 20:35:00\n",
      "change best\n",
      "testing finish [2019-12-26 20:35:01] \n",
      "\tHR@1=0.21603  MRR@1=0.21603  NDCG@1=0.21603\n",
      "\tHR@5=0.40510  MRR@5=0.28850  NDCG@5=0.31767\n",
      "\tHR@10=0.46365  MRR@10=0.29647  NDCG@10=0.33676\n",
      "\tHR@20=0.51685  MRR@20=0.30013  NDCG@20=0.35017\n",
      "[2019-12-26 20:35:01] [4/50] 0 mean_batch_loss : 4.875797\n",
      "Start predicting 2019-12-26 20:35:15\n",
      "change best\n",
      "testing finish [2019-12-26 20:35:16] \n",
      "\tHR@1=0.21934  MRR@1=0.21934  NDCG@1=0.21934\n",
      "\tHR@5=0.41191  MRR@5=0.29191  NDCG@5=0.32186\n",
      "\tHR@10=0.47813  MRR@10=0.30086  NDCG@10=0.34339\n",
      "\tHR@20=0.54130  MRR@20=0.30525  NDCG@20=0.35937\n",
      "[2019-12-26 20:35:17] [5/50] 0 mean_batch_loss : 4.578319\n",
      "Start predicting 2019-12-26 20:35:31\n",
      "change best\n",
      "testing finish [2019-12-26 20:35:32] \n",
      "\tHR@1=0.22383  MRR@1=0.22383  NDCG@1=0.22383\n",
      "\tHR@5=0.41601  MRR@5=0.29528  NDCG@5=0.32536\n",
      "\tHR@10=0.48573  MRR@10=0.30473  NDCG@10=0.34805\n",
      "\tHR@20=0.55247  MRR@20=0.30937  NDCG@20=0.36494\n",
      "[2019-12-26 20:35:32] [6/50] 0 mean_batch_loss : 4.274351\n",
      "Start predicting 2019-12-26 20:35:47\n",
      "change best\n",
      "testing finish [2019-12-26 20:35:48] \n",
      "\tHR@1=0.22509  MRR@1=0.22509  NDCG@1=0.22509\n",
      "\tHR@5=0.41805  MRR@5=0.29635  NDCG@5=0.32664\n",
      "\tHR@10=0.49141  MRR@10=0.30621  NDCG@10=0.35044\n",
      "\tHR@20=0.55862  MRR@20=0.31087  NDCG@20=0.36743\n",
      "[2019-12-26 20:35:48] [7/50] 0 mean_batch_loss : 4.103286\n",
      "Start predicting 2019-12-26 20:36:03\n",
      "change best\n",
      "testing finish [2019-12-26 20:36:04] \n",
      "\tHR@1=0.22575  MRR@1=0.22575  NDCG@1=0.22575\n",
      "\tHR@5=0.41799  MRR@5=0.29661  NDCG@5=0.32682\n",
      "\tHR@10=0.49227  MRR@10=0.30672  NDCG@10=0.35103\n",
      "\tHR@20=0.56305  MRR@20=0.31167  NDCG@20=0.36898\n",
      "[2019-12-26 20:36:04] [8/50] 0 mean_batch_loss : 3.920578\n",
      "Start predicting 2019-12-26 20:36:19\n",
      "change best\n",
      "testing finish [2019-12-26 20:36:20] \n",
      "\tHR@1=0.22469  MRR@1=0.22469  NDCG@1=0.22469\n",
      "\tHR@5=0.41766  MRR@5=0.29634  NDCG@5=0.32656\n",
      "\tHR@10=0.49564  MRR@10=0.30695  NDCG@10=0.35199\n",
      "\tHR@20=0.56450  MRR@20=0.31176  NDCG@20=0.36943\n",
      "[2019-12-26 20:36:20] [9/50] 0 mean_batch_loss : 3.925926\n",
      "Start predicting 2019-12-26 20:36:35\n",
      "change best\n",
      "testing finish [2019-12-26 20:36:36] \n",
      "\tHR@1=0.22641  MRR@1=0.22641  NDCG@1=0.22641\n",
      "\tHR@5=0.42050  MRR@5=0.29806  NDCG@5=0.32854\n",
      "\tHR@10=0.49432  MRR@10=0.30806  NDCG@10=0.35256\n",
      "\tHR@20=0.56998  MRR@20=0.31332  NDCG@20=0.37171\n",
      "[2019-12-26 20:36:36] [10/50] 0 mean_batch_loss : 3.943671\n",
      "Start predicting 2019-12-26 20:36:51\n",
      "change best\n",
      "testing finish [2019-12-26 20:36:52] \n",
      "\tHR@1=0.22522  MRR@1=0.22522  NDCG@1=0.22522\n",
      "\tHR@5=0.42169  MRR@5=0.29728  NDCG@5=0.32823\n",
      "\tHR@10=0.49531  MRR@10=0.30726  NDCG@10=0.35219\n",
      "\tHR@20=0.57177  MRR@20=0.31260  NDCG@20=0.37156\n",
      "[2019-12-26 20:36:52] [11/50] 0 mean_batch_loss : 3.753649\n",
      "Start predicting 2019-12-26 20:37:07\n",
      "change best\n",
      "testing finish [2019-12-26 20:37:08] \n",
      "\tHR@1=0.22542  MRR@1=0.22542  NDCG@1=0.22542\n",
      "\tHR@5=0.42169  MRR@5=0.29746  NDCG@5=0.32836\n",
      "\tHR@10=0.49603  MRR@10=0.30753  NDCG@10=0.35255\n",
      "\tHR@20=0.57283  MRR@20=0.31290  NDCG@20=0.37203\n",
      "[2019-12-26 20:37:08] [12/50] 0 mean_batch_loss : 3.557485\n",
      "Start predicting 2019-12-26 20:37:23\n",
      "testing finish [2019-12-26 20:37:24] \n",
      "\tHR@1=0.22561  MRR@1=0.22561  NDCG@1=0.22561\n",
      "\tHR@5=0.42050  MRR@5=0.29645  NDCG@5=0.32727\n",
      "\tHR@10=0.49828  MRR@10=0.30692  NDCG@10=0.35251\n",
      "\tHR@20=0.57230  MRR@20=0.31205  NDCG@20=0.37122\n",
      "[2019-12-26 20:37:24] [13/50] 0 mean_batch_loss : 3.944383\n",
      "Start predicting 2019-12-26 20:37:38\n",
      "change best\n",
      "testing finish [2019-12-26 20:37:39] \n",
      "\tHR@1=0.22561  MRR@1=0.22561  NDCG@1=0.22561\n",
      "\tHR@5=0.41984  MRR@5=0.29646  NDCG@5=0.32714\n",
      "\tHR@10=0.49848  MRR@10=0.30713  NDCG@10=0.35274\n",
      "\tHR@20=0.57421  MRR@20=0.31242  NDCG@20=0.37193\n",
      "[2019-12-26 20:37:39] [14/50] 0 mean_batch_loss : 3.590226\n",
      "Start predicting 2019-12-26 20:37:53\n",
      "change best\n",
      "testing finish [2019-12-26 20:37:54] \n",
      "\tHR@1=0.22654  MRR@1=0.22654  NDCG@1=0.22654\n",
      "\tHR@5=0.42030  MRR@5=0.29696  NDCG@5=0.32761\n",
      "\tHR@10=0.49980  MRR@10=0.30771  NDCG@10=0.35346\n",
      "\tHR@20=0.57527  MRR@20=0.31295  NDCG@20=0.37254\n",
      "[2019-12-26 20:37:54] [15/50] 0 mean_batch_loss : 3.710104\n",
      "Start predicting 2019-12-26 20:38:08\n",
      "testing finish [2019-12-26 20:38:09] \n",
      "\tHR@1=0.22383  MRR@1=0.22383  NDCG@1=0.22383\n",
      "\tHR@5=0.42010  MRR@5=0.29541  NDCG@5=0.32641\n",
      "\tHR@10=0.49993  MRR@10=0.30625  NDCG@10=0.35241\n",
      "\tHR@20=0.57507  MRR@20=0.31147  NDCG@20=0.37141\n",
      "[2019-12-26 20:38:09] [16/50] 0 mean_batch_loss : 3.501507\n",
      "Start predicting 2019-12-26 20:38:24\n",
      "testing finish [2019-12-26 20:38:25] \n",
      "\tHR@1=0.22515  MRR@1=0.22515  NDCG@1=0.22515\n",
      "\tHR@5=0.41918  MRR@5=0.29617  NDCG@5=0.32677\n",
      "\tHR@10=0.49974  MRR@10=0.30706  NDCG@10=0.35296\n",
      "\tHR@20=0.57587  MRR@20=0.31234  NDCG@20=0.37220\n",
      "[2019-12-26 20:38:25] [17/50] 0 mean_batch_loss : 3.654710\n",
      "Start predicting 2019-12-26 20:38:39\n",
      "testing finish [2019-12-26 20:38:40] \n",
      "\tHR@1=0.22443  MRR@1=0.22443  NDCG@1=0.22443\n",
      "\tHR@5=0.41872  MRR@5=0.29559  NDCG@5=0.32622\n",
      "\tHR@10=0.50020  MRR@10=0.30658  NDCG@10=0.35269\n",
      "\tHR@20=0.57527  MRR@20=0.31178  NDCG@20=0.37165\n",
      "[2019-12-26 20:38:40] [18/50] 0 mean_batch_loss : 3.376356\n",
      "Start predicting 2019-12-26 20:38:55\n",
      "testing finish [2019-12-26 20:38:56] \n",
      "\tHR@1=0.22264  MRR@1=0.22264  NDCG@1=0.22264\n",
      "\tHR@5=0.41819  MRR@5=0.29412  NDCG@5=0.32498\n",
      "\tHR@10=0.50132  MRR@10=0.30540  NDCG@10=0.35205\n",
      "\tHR@20=0.57454  MRR@20=0.31045  NDCG@20=0.37052\n",
      "[2019-12-26 20:38:56] [19/50] 0 mean_batch_loss : 3.411345\n",
      "Start predicting 2019-12-26 20:39:11\n",
      "testing finish [2019-12-26 20:39:12] \n",
      "\tHR@1=0.22218  MRR@1=0.22218  NDCG@1=0.22218\n",
      "\tHR@5=0.41799  MRR@5=0.29365  NDCG@5=0.32457\n",
      "\tHR@10=0.49980  MRR@10=0.30480  NDCG@10=0.35126\n",
      "\tHR@20=0.57408  MRR@20=0.30998  NDCG@20=0.37006\n",
      "early stopping\n",
      "best model change\n",
      "current model hyper-parameters: session_length=20, hidden_size=100, lr=0.0010,delta=16.0, amsgrad=True, method=general, dropout=0.5, weight_decay=0.000000. \n",
      "\n",
      "current model HR@20=0.57527  MRR@20=0.31295.\n",
      "the best result so far. HR@20=0.57527  MRR@20=0.31295， hyper-parameters: session_length-20, hidden_size-100, lr-0.0010,delta=16.0, amsgrad-True, method-general, dropout-0.5, weight_decay-0.000000. \n",
      "\n",
      "The best result HR@20=0.57527  MRR@20=0.31295, hyper-parameters: session_length-20, hidden_size-100, lr-0.0010,delta=16.0, amsgrad-True, method-general, dropout-0.5, weight_decay-0.000000. \n",
      "over.\n"
     ]
    }
   ],
   "source": [
    "hidden_sizes = [100]\n",
    "dropouts = [0.5]\n",
    "attention_methods = [\"general\"]\n",
    "lrs = [1e-3]\n",
    "session_lengths = [20]\n",
    "weight_decays = [0]\n",
    "patience = 5\n",
    "deltas = [16.0]\n",
    "amsgrads = [True]\n",
    "best_params = \"\"\n",
    "best_all_model = 0.0\n",
    "best_all_hr = 0.0\n",
    "best_all_mrr = 0.0\n",
    "best_all_r1m = 0.0\n",
    "for session_length in session_lengths:\n",
    "    for hidden_size in hidden_sizes:\n",
    "        for amsgrad in amsgrads:\n",
    "            for attention_method in attention_methods:\n",
    "                for dropout in dropouts:\n",
    "                    for weight_decay in weight_decays:\n",
    "                        for lr in lrs:\n",
    "                            for delta in deltas:\n",
    "                                args = {}\n",
    "                                print(\"current model hyper-parameters: session_length=%d, hidden_size=%d, lr=%.4f,delta=%.1f, amsgrad=%s, method=%s, dropout=%.1f, weight_decay=%.6f. \\n\" % (session_length,hidden_size,lr,delta,str(amsgrad),attention_method,dropout,weight_decay))\n",
    "                                args[\"session_length\"] = session_length\n",
    "                                args[\"hidden_size\"] = hidden_size\n",
    "                                args[\"amsgrad\"] = amsgrad\n",
    "                                args[\"method\"] = attention_method\n",
    "                                args[\"dropout\"] = dropout\n",
    "                                args[\"weight_decay\"] = weight_decay\n",
    "                                args[\"lr\"] = lr\n",
    "                                args[\"delta\"] = delta\n",
    "                                args[\"patience\"] = patience\n",
    "                                best_model,best_model_hr,best_model_mrr = train(args)\n",
    "                                if best_model_hr + best_model_mrr > best_all_r1m:\n",
    "                                    print(\"best model change\")\n",
    "                                    best_all_r1m = best_model_hr + best_model_mrr\n",
    "                                    best_all_hr = best_model_hr\n",
    "                                    best_all_mrr = best_model_mrr\n",
    "                                    best_all_model = best_model\n",
    "                                    best_params = \"session_length-%d, hidden_size-%d, lr-%.4f,delta=%.1f, amsgrad-%s, method-%s, dropout-%.1f, weight_decay-%.6f\"%(session_length,hidden_size,lr,delta,str(amsgrad),attention_method,dropout,weight_decay)\n",
    "                                best_model = None\n",
    "                                print(\"current model hyper-parameters: session_length=%d, hidden_size=%d, lr=%.4f,delta=%.1f, amsgrad=%s, method=%s, dropout=%.1f, weight_decay=%.6f. \\n\" % (session_length,hidden_size,lr,delta,str(amsgrad),attention_method,dropout,weight_decay))\n",
    "                                print(\"current model HR@20=%.5f  MRR@20=%.5f.\"%(best_model_hr,best_model_mrr))\n",
    "                                print(\"the best result so far. HR@20=%.5f  MRR@20=%.5f， hyper-parameters: %s. \\n\"%(best_all_hr,best_all_mrr,best_params))\n",
    "print(\"The best result HR@20=%.5f  MRR@20=%.5f, hyper-parameters: %s. \"%(best_all_hr,best_all_mrr,best_params))\n",
    "print(\"over.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-26T12:39:12.633961Z",
     "start_time": "2019-12-26T12:39:12.626067Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "O8eiYy5UbyFR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.0000],\n",
       "        [0.8468],\n",
       "        [0.6264],\n",
       "        [1.2987],\n",
       "        [1.3130],\n",
       "        [1.9952],\n",
       "        [1.8754],\n",
       "        [2.1132],\n",
       "        [2.0780],\n",
       "        [1.9817],\n",
       "        [2.0239],\n",
       "        [1.9664],\n",
       "        [1.9574],\n",
       "        [1.9945],\n",
       "        [1.8578],\n",
       "        [1.8428],\n",
       "        [1.9018],\n",
       "        [1.7739],\n",
       "        [1.8916],\n",
       "        [1.9151],\n",
       "        [1.6250]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_all_model.position_weights.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-26T12:39:12.648444Z",
     "start_time": "2019-12-26T12:39:12.634933Z"
    }
   },
   "outputs": [],
   "source": [
    "# best_all_model.gate_weights.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "WDP-CE",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "smy",
   "language": "python",
   "name": "smy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
