{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T03:01:40.518980Z",
     "start_time": "2019-12-31T03:01:40.135745Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "MhcOrxNibMjb"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "import copy\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T03:01:40.522689Z",
     "start_time": "2019-12-31T03:01:40.520128Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "EGxgHeR6bN0K"
   },
   "outputs": [],
   "source": [
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda:0\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T03:01:40.530892Z",
     "start_time": "2019-12-31T03:01:40.523587Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "tBeDjO8dbPzP"
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "plot_num = 50000\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T03:01:40.556396Z",
     "start_time": "2019-12-31T03:01:40.531944Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "zef81a0tbRC4"
   },
   "outputs": [],
   "source": [
    "class SessionData(object):\n",
    "    def __init__(self,session_index,session_id,items_indexes):\n",
    "        self.session_index = session_index\n",
    "        self.session_id = session_id\n",
    "        self.item_list = items_indexes\n",
    "    def generate_seq_datas(self,session_length,padding_idx=0,predict_length=1):\n",
    "        sessions = []\n",
    "        if len(self.item_list)<2:\n",
    "            self.item_list.append[self.item_list[0]]\n",
    "        if predict_length==1:\n",
    "            # when session length>=3\n",
    "            for i in range(1,len(self.item_list)-1):\n",
    "#             # when session length >=2\n",
    "#             for i in range(len(self.item_list)-1):\n",
    "                if i <session_length:\n",
    "                    train_data = [0 for _ in range(session_length-i-1)]\n",
    "                    train_data.extend(self.item_list[:i+1])\n",
    "                    train_data.append(self.item_list[i+1])\n",
    "                else:\n",
    "                    train_data = self.item_list[i+1-session_length:i+1]\n",
    "                    train_data.append(self.item_list[i+1])\n",
    "                sessions.append(train_data)\n",
    "        else:\n",
    "            pass\n",
    "        return self.session_index,sessions\n",
    "    def __str__(self):\n",
    "        info = \" session index = {}\\n session id = {} \\n the length of item list= {} \\n the fisrt item index in item list is {}\".format(self.session_index,self.session_id,len(self.item_list),self.item_list[0])\n",
    "        return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T03:01:40.584971Z",
     "start_time": "2019-12-31T03:01:40.557970Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "hl6NiyNJbTRg"
   },
   "outputs": [],
   "source": [
    "class SessionDataSet(object):\n",
    "    def __init__(self,train_file,test_file,padding_idx=0):\n",
    "        super(SessionDataSet,self).__init__()\n",
    "        self.index_count = 0\n",
    "        self.session_count = 0\n",
    "        self.train_count = 0\n",
    "        self.test_count = 0\n",
    "        self.max_session_length = 0\n",
    "\n",
    "        self.padding_idx = padding_idx\n",
    "        self.item2index = dict()\n",
    "        self.index2item = dict()\n",
    "        self.session2index = dict()\n",
    "        self.index2session = dict()\n",
    "        self.item_total_num = dict()\n",
    "        self.item2index[\"<pad>\"] = padding_idx\n",
    "        self.index2item[padding_idx] = \"<pad>\"\n",
    "        self.train_data = self.load_data(train_file)\n",
    "        print(\"training set is loaded, # index: \",len(self.item2index.keys()))\n",
    "        self.train_count = self.session_count\n",
    "        print(\"train_session_num\",self.train_count)\n",
    "        self.test_data = self.load_data(test_file)\n",
    "        print(\"testing set is loaded, # index: \",len(self.index2item.keys()))\n",
    "        print(\"# item\",self.index_count)\n",
    "        self.test_count = self.session_count-self.train_count\n",
    "        print(\"# test session:\",self.test_count)\n",
    "        self.all_training_data = []\n",
    "        self.all_testing_data = []\n",
    "        self.all_meta_training_data = []\n",
    "        self.all_meta_testing_data = []\n",
    "        self.train_session_length = 0\n",
    "        self.test_session_length = 0\n",
    "    \n",
    "    def load_data(self,file_path):\n",
    "        data =  pickle.load(open(file_path, 'rb'))\n",
    "        session_ids = data[0]\n",
    "        session_data = data[1]\n",
    "        session_label = data[2]\n",
    "\n",
    "        result_data = []\n",
    "        lenth = len(session_ids)\n",
    "        print(\"# session\",lenth)\n",
    "\n",
    "        last_session_id = session_ids[0]\n",
    "        \n",
    "        session_item_indexes = []\n",
    "\n",
    "        for item_id in session_data[0]:\n",
    "            if item_id not in self.item2index.keys():\n",
    "                self.index_count+=1\n",
    "                self.item2index[item_id] = self.index_count\n",
    "                self.index2item[self.index_count] = item_id\n",
    "                self.item_total_num[self.index_count] = 0\n",
    "            session_item_indexes.append(self.item2index[item_id])\n",
    "            self.item_total_num[self.item2index[item_id]] += 1\n",
    "        target_item = session_label[0]\n",
    "        if target_item not in self.item2index.keys():\n",
    "            self.index_count+=1\n",
    "            self.item2index[target_item] = self.index_count\n",
    "            self.index2item[self.index_count] = target_item\n",
    "            self.item_total_num[self.index_count] = 0\n",
    "        session_item_indexes.append(self.item2index[target_item])\n",
    "        self.item_total_num[self.item2index[target_item]] += 1\n",
    "\n",
    "        for session_id,items,target_item in zip(session_ids,session_data,session_label):\n",
    "            if session_id!=last_session_id:\n",
    "\n",
    "                self.session_count+=1\n",
    "                self.session2index[last_session_id] = self.session_count\n",
    "                self.index2session[self.session_count] = last_session_id\n",
    "                if len(session_item_indexes)>self.max_session_length:\n",
    "                    self.max_session_length = len(session_item_indexes)\n",
    "                new_session = SessionData(self.session_count,last_session_id,session_item_indexes)\n",
    "                result_data.append(new_session)\n",
    "                last_session_id = session_id\n",
    "                session_item_indexes = []\n",
    "                for item_id in items:\n",
    "                    if item_id not in self.item2index.keys():\n",
    "                        self.index_count+=1\n",
    "                        self.item2index[item_id] = self.index_count\n",
    "                        self.index2item[self.index_count] = item_id\n",
    "                        self.item_total_num[self.index_count] = 0\n",
    "                    session_item_indexes.append(self.item2index[item_id])\n",
    "                    self.item_total_num[self.item2index[item_id]] += 1\n",
    "                if target_item not in self.item2index.keys():\n",
    "                    self.index_count+=1\n",
    "                    self.item2index[target_item] = self.index_count\n",
    "                    self.index2item[self.index_count] = target_item\n",
    "                    self.item_total_num[self.index_count] = 0\n",
    "                session_item_indexes.append(self.item2index[target_item])\n",
    "                self.item_total_num[self.item2index[target_item]] += 1\n",
    "            else:\n",
    "                # 每个id只处理一次\n",
    "                continue\n",
    "\n",
    "        self.session_count+=1\n",
    "        self.session2index[last_session_id] = self.session_count\n",
    "        new_session = SessionData(self.session_count,last_session_id,session_item_indexes)\n",
    "        result_data.append(new_session)\n",
    "        print(\"loaded\")\n",
    "        print(new_session)\n",
    "        \n",
    "        return result_data\n",
    "    \n",
    "\n",
    "    def get_batch(self,batch_size,session_length=10,predict_length=1,all_data=None,phase=\"train\",neg_num=1,sampling_mathod=\"random\"):\n",
    "\n",
    "        if phase == \"train\":\n",
    "            if all_data is None:\n",
    "                all_data = self.get_all_training_data(session_length)\n",
    "            indexes = np.random.permutation(all_data.shape[0])\n",
    "            all_data = all_data[indexes]\n",
    "        else:\n",
    "            if all_data is None:\n",
    "                all_data = self.get_all_testing_data(session_length)\n",
    "        \n",
    "        sindex = 0\n",
    "        eindex = batch_size\n",
    "        while eindex < all_data.shape[0]:\n",
    "            batch = all_data[sindex: eindex]\n",
    "\n",
    "            temp = eindex\n",
    "            eindex = eindex + batch_size\n",
    "            sindex = temp\n",
    "            if phase ==\"train\":\n",
    "                batch = self.divid_and_extend_negative_samples(batch,session_length=session_length,predict_length=predict_length,neg_num=neg_num,method=sampling_mathod)\n",
    "            else:\n",
    "                batch = [batch[:,:session_length],batch[:,session_length:]]\n",
    "            yield batch\n",
    "\n",
    "        if eindex >= all_data.shape[0]:\n",
    "            batch = all_data[sindex:]\n",
    "            if phase ==\"train\":\n",
    "                batch = self.divid_and_extend_negative_samples(batch,session_length=session_length,predict_length=predict_length,neg_num=neg_num,method=sampling_mathod)\n",
    "            else:\n",
    "                batch = [batch[:,:session_length],batch[:,session_length:]]\n",
    "            yield batch\n",
    "    \n",
    "    def get_batch_with_neg(self,batch_size,session_length=10,predict_length=1,all_data=None,phase=\"train\",neg_num=1,sampling_mathod=\"random\"):\n",
    "        if phase == \"train\":\n",
    "            all_data = self.get_all_training_data_with_neg(session_length,neg_num)\n",
    "            indexes = np.random.permutation(all_data.shape[0])\n",
    "            all_data = all_data[indexes]\n",
    "        else:\n",
    "            all_data = self.get_all_testing_data_with_neg(session_length,neg_num)\n",
    "        \n",
    "        sindex = 0\n",
    "        eindex = batch_size\n",
    "        while eindex < all_data.shape[0]:\n",
    "            batch = all_data[sindex: eindex]\n",
    "\n",
    "            temp = eindex\n",
    "            eindex = eindex + batch_size\n",
    "            sindex = temp\n",
    "            if phase ==\"train\":\n",
    "                batch = [batch[:,:session_length],batch[:,session_length:session_length+predict_length],batch[:,-neg_num:]]\n",
    "            else:\n",
    "                batch = [batch[:,:session_length],batch[:,session_length:]]\n",
    "            yield batch\n",
    "\n",
    "        if eindex >= all_data.shape[0]:\n",
    "            batch = all_data[sindex:]\n",
    "            if phase ==\"train\":\n",
    "                batch = [batch[:,:session_length],batch[:,session_length:session_length+predict_length],batch[:,-neg_num:]]\n",
    "            else:\n",
    "                batch = [batch[:,:session_length],batch[:,session_length:]]\n",
    "            yield batch\n",
    "    \n",
    "    def get_batch_tasks_with_neg(self,batch_size,session_length=10,predict_length=1,all_data=None,phase=\"train\",neg_num=1,sampling_mathod=\"random\"):\n",
    "        if phase == \"train\":\n",
    "            all_data = self.get_all_meta_training_data_with_neg(session_length,neg_num)\n",
    "            random.shuffle(all_data)\n",
    "        else:\n",
    "            all_data = self.get_all_meta_testing_data_with_neg(session_length,neg_num)\n",
    "        sindex = 0\n",
    "        eindex = batch_size\n",
    "        while eindex < len(all_data):\n",
    "            batch = all_data[sindex: eindex]\n",
    "\n",
    "            temp = eindex\n",
    "            eindex = eindex + batch_size\n",
    "            sindex = temp\n",
    "\n",
    "            session_items = [batch[i][:,:session_length] for i in range(len(batch))]\n",
    "\n",
    "            target_item = [batch[i][:,session_length:session_length+predict_length] for i in range(len(batch))]\n",
    "\n",
    "            neg_item = [batch[i][:,-neg_num:] for i in range(len(batch))]\n",
    "            batch = [session_items,target_item,neg_item]\n",
    "            yield batch\n",
    "\n",
    "        if eindex >= len(all_data):\n",
    "            batch = all_data[sindex:]\n",
    "            session_items = [batch[i][:,:session_length] for i in range(len(batch))]\n",
    "\n",
    "            target_item = [batch[i][:,session_length:session_length+predict_length] for i in range(len(batch))]\n",
    "\n",
    "            neg_item = [batch[i][:,-neg_num:] for i in range(len(batch))]\n",
    "            batch = [session_items,target_item,neg_item]\n",
    "            yield batch\n",
    "    \n",
    "    def divid_and_extend_negative_samples(self,batch_data,session_length,predict_length=1,neg_num=1,method=\"random\"):\n",
    "        \"\"\"\n",
    "        divid and extend negative samples\n",
    "        \"\"\"\n",
    "        neg_items = []\n",
    "        if method == \"random\":\n",
    "            for session_and_target in batch_data:\n",
    "                neg_item = []\n",
    "                for i in range(neg_num):\n",
    "                    rand_item = random.randint(1,self.index_count)\n",
    "                    while rand_item in session_and_target or rand_item in neg_item:\n",
    "                        rand_item = random.randint(1,self.index_count)\n",
    "                    neg_item.append(rand_item)\n",
    "                neg_items.append(neg_item)\n",
    "        else:\n",
    "\n",
    "            total_list = set()\n",
    "            for session in batch_data:\n",
    "                for i in session:\n",
    "                    total_list.add(i) \n",
    "            total_list = list(total_list)\n",
    "            total_list =  sorted(total_list, key=lambda item: self.item_total_num[item],reverse=True)\n",
    "            for i,session in enumerate(batch_data):\n",
    "                np.random.choice(total_list)\n",
    "        session_items = batch_data[:,:session_length]\n",
    "        target_item = batch_data[:,session_length:]\n",
    "        neg_items = np.array(neg_items)\n",
    "        return [session_items,target_item,neg_items]\n",
    "    \n",
    "    def get_all_training_data(self,session_length,predict_length=1):\n",
    "        if len(self.all_training_data)!=0 and self.train_session_length==session_length:\n",
    "#             print(\"The build is complete and there is no need to repeat the build\")\n",
    "            return self.all_training_data\n",
    "        print(\"Start building the all training dataset\")\n",
    "        all_sessions = []\n",
    "        for session_data in self.train_data:\n",
    "            # 前session_length为session，后predict_length为target_item\n",
    "            session_index,sessions = session_data.generate_seq_datas(session_length,padding_idx=self.padding_idx)\n",
    "            if sessions is not None:\n",
    "                all_sessions.extend(sessions)\n",
    "        all_sessions = np.array(all_sessions)\n",
    "        self.all_training_data = all_sessions\n",
    "        self.train_session_length=session_length\n",
    "        print(\"The total number of training samples is：\",all_sessions.shape)\n",
    "        return all_sessions\n",
    "    \n",
    "    def get_all_testing_data(self,session_length,predict_length=1):\n",
    "        if len(self.all_testing_data)!=0 and self.test_session_length==session_length:\n",
    "            return self.all_testing_data\n",
    "        all_sessions = []\n",
    "        for session_data in self.test_data:\n",
    "            session_index,sessions = session_data.generate_seq_datas(session_length,padding_idx=self.padding_idx)\n",
    "            if sessions is not None:\n",
    "                all_sessions.extend(sessions)\n",
    "        all_sessions = np.array(all_sessions)\n",
    "        self.all_testing_data = all_sessions\n",
    "        self.test_session_length=session_length\n",
    "        print(\"The total number of testing samples is：\",all_sessions.shape)\n",
    "        return all_sessions\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T03:01:41.601444Z",
     "start_time": "2019-12-31T03:01:40.586077Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "ctSu0HF8bUqh",
    "outputId": "7287ecc0-f73d-4883-c8f6-20c9b8aeddb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# session 433648\n",
      "loaded\n",
      " session index = 126168\n",
      " session id = 946108 \n",
      " the length of item list= 3 \n",
      " the fisrt item index in item list is 3314\n",
      "training set is loaded, # index:  36969\n",
      "train_session_num 126168\n",
      "# session 15132\n",
      "loaded\n",
      " session index = 130903\n",
      " session id = 1582915 \n",
      " the length of item list= 6 \n",
      " the fisrt item index in item list is 12498\n",
      "testing set is loaded, # index:  36969\n",
      "# item 36968\n",
      "# test session: 4735\n"
     ]
    }
   ],
   "source": [
    "# dataset = SessionDataSet(train_file=\"../data/retailrocket_gcsan_my/train.txt\",test_file=\"../data/srgnn/retailrocket_gcsan_my/test.txt\")\n",
    "dataset = SessionDataSet(train_file=\"../data/diginetica_gcsan_my/train.txt\",test_file=\"../data/srgnn/diginetica_gcsan_my/test.txt\")\n",
    "# dataset = SessionDataSet(train_file=\"../data/yoochoose1_4_gcsan_my/train.txt\",test_file=\"../data/srgnn/yoochoose1_4_gcsan_my/test.txt\")\n",
    "# dataset = SessionDataSet(train_file=\"../data/yoochoose1_64_gcsan_my/train.txt\",test_file=\"../data/srgnn/yoochoose1_64_gcsan_my/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T03:01:41.653246Z",
     "start_time": "2019-12-31T03:01:41.602302Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "cm7qU4B6bq2i"
   },
   "outputs": [],
   "source": [
    "def bpr_loss(r):\n",
    "    return torch.sum(-torch.log(torch.sigmoid(r)))\n",
    "def get_hit_num(pred,y_truth):\n",
    "    \"\"\"\n",
    "        pred: numpy type(batch_size,k) \n",
    "        y_truth: list type (batch_size,groudtruth_num)\n",
    "    \"\"\"\n",
    "\n",
    "    hit_num = 0\n",
    "    for i in range(len(y_truth)):\n",
    "        for value in y_truth[i]:\n",
    "            hit_num += np.sum(pred[i]==value)\n",
    "    return hit_num\n",
    "\n",
    "def get_rr(pred,y_truth):\n",
    "    rr=0.\n",
    "    for i in range(len(y_truth)):\n",
    "        for value in y_truth[i]:\n",
    "            hit_indexes = np.where(pred[i]==value)[0]\n",
    "            for hit_index in hit_indexes:\n",
    "                rr += 1/(hit_index+1)\n",
    "    return rr\n",
    "\n",
    "def get_dcg(pred,y_truth):\n",
    "    y_pred_score = np.zeros_like(pred)\n",
    "\n",
    "    for i in range(len(y_truth)):\n",
    "\n",
    "        for j,y_pred in enumerate(pred[i]):\n",
    "            if y_pred == y_truth[i][0]:\n",
    "                y_pred_score[i][j]=1\n",
    "    gain = 2 ** y_pred_score - 1\n",
    "    discounts = np.tile(np.log2(np.arange(pred.shape[1]) + 2),(len(y_truth),1))\n",
    "    dcg = np.sum(gain / discounts,axis=1)\n",
    "    return dcg\n",
    "\n",
    "def get_ndcg(pred,y_truth):\n",
    "    dcg = get_dcg(pred, y_truth)\n",
    "    idcg = get_dcg(np.concatenate((y_truth,np.zeros_like(pred)[:,:-1]-1),axis=1), y_truth)\n",
    "    ndcg = np.sum(dcg / idcg)\n",
    "\n",
    "    return ndcg\n",
    "\n",
    "def dcg_score(y_pre, y_true, k):\n",
    "    y_pre_score = np.zeros(k)\n",
    "    if len(y_pre) > k:\n",
    "        y_pre = y_pre[:k]\n",
    "    for i in range(len(y_pre)):\n",
    "        pre_tag = y_pre[i]\n",
    "        if pre_tag in y_true:\n",
    "            y_pre_score[i] = 1\n",
    "    gain = 2 ** y_pre_score - 1\n",
    "    discounts = np.log2(np.arange(k) + 2)\n",
    "    return np.sum(gain / discounts)\n",
    "\n",
    "\n",
    "def ndcg_score(y_pre, y_true, k=5):\n",
    "    dcg = dcg_score(y_pre, y_true, k)\n",
    "    idcg = dcg_score(y_true, y_true, k)\n",
    "    return dcg / idcg\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T03:01:41.676550Z",
     "start_time": "2019-12-31T03:01:41.654210Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "QdNvTTApbr_g"
   },
   "outputs": [],
   "source": [
    "# SelfAttention Layer\n",
    "class SelfAttention(torch.nn.Module):\n",
    "    def __init__(self, method, hidden_size,activate=\"selu\",dropout=0):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.config = list()\n",
    "        # 使用的Attention方法\n",
    "        self.method = method\n",
    "        # 隐藏层大小\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method not in ['dot', 'general']:\n",
    "            raise ValueError(self.method, \"Attention method do not exists.\")\n",
    "\n",
    "        if self.method == \"dot\":\n",
    "            self.query = torch.nn.Linear(self.hidden_size *2, self.hidden_size*2)\n",
    "            self.key = torch.nn.Linear(self.hidden_size*2, self.hidden_size*2)\n",
    "            torch.nn.init.constant_(self.query.bias,0)\n",
    "            torch.nn.init.constant_(self.key.bias,0)\n",
    "\n",
    "        if self.method == \"general\":\n",
    "            self.attention = torch.nn.Linear(self.hidden_size*2, self.hidden_size*2)\n",
    "            torch.nn.init.constant_(self.attention.bias,0)\n",
    "        \n",
    "        if activate == \"relu\":\n",
    "            self.activate = torch.relu\n",
    "        elif activate == \"tanh\":\n",
    "            self.activate = torch.tanh\n",
    "        elif activate == \"elu\":\n",
    "            self.activate = torch.nn.ELU()\n",
    "        elif activate == \"selu\":\n",
    "            self.activate = torch.selu\n",
    "        else:\n",
    "            self.activate = torch.sigmoid\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        torch.nn.utils.clip_grad_norm_(self.parameters(),max_norm=110)\n",
    "\n",
    "    def dot_score(self, encoder_output,is_train=True,weights=None):\n",
    "\n",
    "        if weights is None:\n",
    "            if is_train:\n",
    "                query = self.dropout(self.activate(self.query(encoder_output)))\n",
    "                key = self.dropout(self.activate(self.key(encoder_output)))\n",
    "            else:\n",
    "                query = self.activate(self.query(encoder_output))\n",
    "                key = self.activate(self.key(encoder_output))\n",
    "        else:\n",
    "            query = self.activate(torch.matmul(encoder_output,weights[0].t())+weights[1])\n",
    "            key = self.activate(torch.matmul(encoder_output,weights[2].t())+weights[3])\n",
    "        dot = query.bmm(key.permute(0, 2, 1))\n",
    "        return dot\n",
    "\n",
    "    def general_score(self, encoder_output,is_train=True,weights=None):\n",
    "        if weights is None:\n",
    "            if is_train:\n",
    "                energy = self.dropout(self.activate(self.attention(encoder_output)))\n",
    "            else:\n",
    "                energy = self.activate(self.attention(encoder_output))\n",
    "        else:\n",
    "            energy = self.activate(torch.matmul(encoder_output,weights[0].t())+weights[1])\n",
    "        return encoder_output.bmm(energy.permute(0, 2, 1))\n",
    "\n",
    "    def forward(self, encoder_outputs, mask=None,is_train=True):\n",
    "        # (batch_size,length,dim)\n",
    "        if self.method == \"general\":\n",
    "            attention_energies = self.general_score(encoder_outputs,is_train=is_train)\n",
    "        elif self.method == \"dot\":\n",
    "            attention_energies = self.dot_score(encoder_outputs,is_train=is_train)\n",
    "\n",
    "        #  (batch_size,length,length)\n",
    "        attention_energies.div_(torch.sqrt(torch.tensor(self.hidden_size, dtype=torch.float)))\n",
    "        if mask is not None:\n",
    "            new_mask = (1 - (1 - mask.float()).unsqueeze(1).permute(0, 2, 1).bmm(\n",
    "                (1 - mask.float()).unsqueeze(1)))\n",
    "\n",
    "            attention_energies = attention_energies - new_mask*1e12\n",
    "            weights = F.softmax(attention_energies, dim=2)\n",
    "            weights = weights*(1-new_mask)\n",
    "            # batch_size,length,length)*(batch_size,length,dim)->(batch_size,length,dim)->(batch_size,1,dim)->(batch_size,dim)\n",
    "            outputs = weights.bmm(encoder_outputs)\n",
    "            outputs.div_(mask.shape[1]-torch.sum(mask,dim=1).unsqueeze(1).unsqueeze(2).repeat((1,mask.shape[1],outputs.shape[2])).float())\n",
    "            outputs = outputs.sum(dim=1).squeeze(1)\n",
    "        else:\n",
    "            weights = F.softmax(attention_energies, dim=2)\n",
    "            # (batch_size,length,length)*(batch_size,length,dim)->(batch_size,length,dim)->(batch_size,1,dim)->(batch_size,dim)\n",
    "            outputs = (weights.bmm(encoder_outputs).sum(dim=1) / encoder_outputs.shape[1]).squeeze(1)\n",
    "        sa_weights = weights.sum(dim=1).squeeze(1)\n",
    "        return outputs, sa_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T03:01:41.691738Z",
     "start_time": "2019-12-31T03:01:41.677546Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "aVIrdlmKbtUq"
   },
   "outputs": [],
   "source": [
    "class POEM(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=64, itemNum=0, posNum=0, padding_idx=0, dropout=0.5,attention_method=\"dot\",head_num=4,\n",
    "                 activate=\"selu\",session_length=20,delta=16.0):\n",
    "        super(POEM, self).__init__()\n",
    "        self.padding_idx = padding_idx\n",
    "        self.hidden_size = hidden_size\n",
    "        self.head_num = head_num\n",
    "        self.delta = delta\n",
    "        self.session_length = session_length\n",
    "        if activate == \"sigmoid\":\n",
    "            self.activate = torch.sigmoid\n",
    "        elif activate == \"tanh\":\n",
    "            self.activate = torch.tanh\n",
    "        elif activate == \"relu\":\n",
    "            self.activate = torch.relu\n",
    "        elif activate == \"elu\":\n",
    "            self.activate = torch.nn.ELU()\n",
    "        else:\n",
    "            self.activate = torch.selu\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.item_embedding = torch.nn.Embedding(itemNum, hidden_size, padding_idx=self.padding_idx,max_norm=1.5)\n",
    "        \n",
    "        self.position_embedding = torch.nn.Embedding(posNum,hidden_size,padding_idx=self.padding_idx,max_norm=1.5)\n",
    "    \n",
    "        self.position_weights = torch.nn.Embedding(posNum,1,padding_idx=self.padding_idx)\n",
    "        \n",
    "        self.self_attention = SelfAttention(attention_method, hidden_size,activate=activate,dropout=dropout).to(device)\n",
    "        torch.nn.init.constant_(self.item_embedding.weight[0],0)\n",
    "        torch.nn.init.constant_(self.position_embedding.weight[0],0)\n",
    "        torch.nn.init.constant_(self.position_weights.weight,1)\n",
    "        torch.nn.init.constant_(self.position_weights.weight[0],0)\n",
    "        \n",
    "        self.gen_mlp = torch.nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.cur_mlp = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.deep_mlp = torch.nn.Linear(hidden_size*3, hidden_size,bias=False)\n",
    "        \n",
    "    def forward(self, session,item=None,bpr_loss=False,neg_num=50):\n",
    "\n",
    "        mask = (session!=0).float()\n",
    "        length = torch.sum(mask,1).unsqueeze(1).expand((session.shape[0],self.hidden_size))\n",
    "        mask = mask.unsqueeze(2).expand((session.shape[0],session.shape[1],self.hidden_size))\n",
    "        session_item_embeddings = self.item_embedding(session) * mask\n",
    "        positions = session.shape[1] - torch.arange(0,session.shape[1]).unsqueeze(0).expand_as(session).to(device)\n",
    "        session_position_embeddings = self.dropout(self.position_embedding(positions))*mask\n",
    "        session_item_vecs = torch.cat((session_item_embeddings,session_position_embeddings), dim=2)\n",
    "        attention_mask = (session == self.padding_idx)\n",
    "        sa_output, sa_weights = self.self_attention(session_item_vecs, attention_mask)\n",
    "        session_position_weights = self.dropout(self.position_weights(positions))*mask\n",
    "        sa_weights = sa_weights.unsqueeze(2).expand_as(session_item_embeddings)\n",
    "        session_item_vecs2 = session_item_embeddings * session_position_weights * sa_weights\n",
    "        psa_output = torch.sum(session_item_vecs2, dim=1)/length\n",
    "        gen_output = self.dropout(self.activate(self.gen_mlp(sa_output)))\n",
    "        cur_output = self.dropout(self.activate(self.cur_mlp(session_item_embeddings[:,-1])))\n",
    "        deep_output = self.dropout(self.activate(self.deep_mlp(torch.cat((sa_output,session_item_embeddings[:,-1]),1))))\n",
    "        session_output =  F.normalize(gen_output * cur_output + deep_output + psa_output,dim=-1)\n",
    "        session_output = session_output*self.delta\n",
    "        item_embedding_weight = F.normalize(self.item_embedding.weight[1:],dim=-1)\n",
    "        result = torch.matmul(session_output,item_embedding_weight.t())\n",
    "        return result\n",
    "    \n",
    "    def predict_top_k(self, session, k=20):\n",
    "        mask = (session!=0).float()\n",
    "        length = torch.sum(mask,1).unsqueeze(1).expand((session.shape[0],self.hidden_size))\n",
    "        mask = mask.unsqueeze(2).expand((session.shape[0],session.shape[1],self.hidden_size))\n",
    "        session_item_embeddings = self.item_embedding(session) * mask\n",
    "        positions = session.shape[1] - torch.arange(0,session.shape[1]).unsqueeze(0).expand_as(session).to(device)\n",
    "        session_position_embeddings = self.position_embedding(positions)*mask\n",
    "        session_item_vecs = torch.cat((session_item_embeddings,session_position_embeddings), dim=2)\n",
    "        attention_mask = (session == self.padding_idx)\n",
    "        sa_output, sa_weights = self.self_attention(session_item_vecs, attention_mask,is_train=False)\n",
    "        session_position_weights = self.position_weights(positions)*mask\n",
    "        sa_weights = sa_weights.unsqueeze(2).expand_as(session_item_embeddings)\n",
    "        session_item_vecs2 = session_item_embeddings * session_position_weights * sa_weights\n",
    "        psa_output = torch.sum(session_item_vecs2, dim=1)/length\n",
    "        gen_output =self.activate(self.gen_mlp(sa_output))\n",
    "\n",
    "        cur_output = self.activate(self.cur_mlp(session_item_embeddings[:,-1]))\n",
    "        deep_output = self.activate(self.deep_mlp(torch.cat((sa_output,session_item_embeddings[:,-1]),1)))\n",
    "        session_output =  F.normalize(gen_output * cur_output + deep_output + psa_output,dim=-1)\n",
    "        session_output = session_output * self.delta\n",
    "        item_embedding_weight = F.normalize(self.item_embedding.weight[1:],dim=-1)\n",
    "        result = torch.matmul(session_output,item_embedding_weight.t())\n",
    "        result = torch.topk(result,k,dim=1)[1]\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T03:01:41.703836Z",
     "start_time": "2019-12-31T03:01:41.692605Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Jhrg56xebung"
   },
   "outputs": [],
   "source": [
    "epochs=50\n",
    "def train(args):\n",
    "    hidden_size = args[\"hidden_size\"] if \"hidden_size\" in args.keys() else 100\n",
    "    dropout = args[\"dropout\"] if \"dropout\" in args.keys()  else 0.5\n",
    "    attention_method = args[\"method\"] if \"method\" in args.keys()  else \"general\"\n",
    "    lr = args[\"lr\"] if \"lr\" in args.keys()  else 5e-4\n",
    "    weight_decay = args[\"weight_decay\"] if \"weight_decay\" in args.keys()  else 1e-5\n",
    "    amsgrad = args[\"amsgrad\"] if \"amsgrad\" in args.keys() else True\n",
    "    session_length = args[\"session_length\"] if \"session_length\" in args.keys() else 20\n",
    "    delta = args[\"delta\"] if \"delta\" in args.keys() else 20\n",
    "    model = POEM(hidden_size=hidden_size, itemNum=dataset.index_count+1, posNum=session_length+1, padding_idx=0, dropout=dropout,\n",
    "                 activate=\"selu\",attention_method=attention_method,delta=delta).to(device)\n",
    "    opti = torch.optim.Adam(model.parameters(),lr=lr,weight_decay=weight_decay,amsgrad=amsgrad)\n",
    "    patience = args[\"patience\"] if \"patience\" in args.keys() else 5\n",
    "    best_model_hr = 0.0\n",
    "    best_model_mrr = 0.0\n",
    "    best_r1m = 0.0\n",
    "    best_model = None\n",
    "    predict_nums = [1,5,10,20]\n",
    "    no_improvement_epoch = 0\n",
    "    for epoch in range(epochs):\n",
    "        batch_losses = []\n",
    "        epoch_losses = []\n",
    "        for i,batch_data in enumerate(dataset.get_batch(batch_size,session_length,phase=\"train\")):\n",
    "            sessions = torch.tensor(batch_data[0]).to(device)\n",
    "            target_items = torch.tensor(batch_data[1]).squeeze().to(device)-1\n",
    "            result_pos = model(sessions)\n",
    "            loss = loss_function(result_pos,target_items)\n",
    "            opti.zero_grad()\n",
    "            loss.backward()\n",
    "            opti.step()\n",
    "            batch_losses.append(loss.cpu().detach().numpy())\n",
    "            epoch_losses.append(loss.cpu().detach().numpy())\n",
    "            if i % plot_num == 0:\n",
    "                time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                print(\"[%s] [%d/%d] %d mean_batch_loss : %0.6f\" % (time, epoch+1, epochs, i, np.mean(batch_losses)))\n",
    "                batch_losses = []\n",
    "        with torch.no_grad():\n",
    "            start_test_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(\"Start predicting\",start_test_time)\n",
    "            rrs = [0 for _ in range(len(predict_nums))]\n",
    "            hit_nums = [0 for _ in range(len(predict_nums))]\n",
    "            ndcgs = [0 for _ in range(len(predict_nums))]\n",
    "            for i,batch_data in enumerate(dataset.get_batch(batch_size,session_length,phase=\"test\")):\n",
    "                \n",
    "                sessions = torch.tensor(batch_data[0]).to(device)\n",
    "                target_items = np.array(batch_data[1])-1\n",
    "                y_pred = model.predict_top_k(sessions,20).cpu().numpy()\n",
    "                \n",
    "                for j,predict_num in enumerate(predict_nums):\n",
    "                    hit_nums[j]+=get_hit_num(y_pred[:,:predict_num],target_items)\n",
    "                    rrs[j]+=get_rr(y_pred[:,:predict_num],target_items)\n",
    "                    ndcgs[j]+=get_ndcg(y_pred[:,:predict_num],target_items)\n",
    "                    \n",
    "            end_test_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            hrs = [hit_num/len(dataset.all_testing_data) for hit_num in hit_nums]\n",
    "            mrrs = [rr/len(dataset.all_testing_data) for rr in rrs]\n",
    "            mndcgs = [ndcg/len(dataset.all_testing_data) for ndcg in ndcgs]\n",
    "            if hrs[-1] + mrrs[-1] > best_r1m:\n",
    "                print(\"change best\")\n",
    "                best_model = deepcopy(model)\n",
    "                best_model_hr = hrs[-1]\n",
    "                best_model_mrr = mrrs[-1]\n",
    "                best_r1m = hrs[-1] + mrrs[-1]\n",
    "                no_improvement_epoch = 0\n",
    "            else:\n",
    "                no_improvement_epoch +=1\n",
    "            print(\"testing finish [%s] \"%end_test_time)\n",
    "            for k,predict_num in enumerate(predict_nums):\n",
    "                print(\"\\tHR@%d=%.5f  MRR@%d=%.5f  NDCG@%d=%.5f\"%(predict_num,hrs[k],predict_num,mrrs[k],predict_num,mndcgs[k]))\n",
    "        if no_improvement_epoch>=patience:\n",
    "            print(\"early stopping\")\n",
    "            break\n",
    "    return best_model,best_model_hr,best_model_mrr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2H2SFJ7F77Z0"
   },
   "source": [
    "# CIKM-Session >= 3 13.24s\n",
    "    HR@20=0.67856  MRR@20=0.33175, hyper-parameters: session_length-20, hidden_size-100, lr-0.0010,delta=16.0, amsgrad-True, method-general, dropout-0.5, weight_decay-0.000000. \n",
    "        HR@1=0.21627  MRR@1=0.21627  NDCG@1=0.21627\n",
    "        HR@5=0.46828  MRR@5=0.31020  NDCG@5=0.34961\n",
    "        HR@10=0.57668  MRR@10=0.32467  NDCG@10=0.38467\n",
    "        HR@20=0.67856  MRR@20=0.33175  NDCG@20=0.41044\n",
    "# RR-Session >= 3 9.33s\n",
    "    HR@20=0.57534  MRR@20=0.31288， hyper-parameters: session_length-20, hidden_size-100, lr-0.0010,delta=16.0, amsgrad-True, method-general, dropout-0.5, weight_decay-0.000000. \n",
    "        HR@1=0.22588  MRR@1=0.22588  NDCG@1=0.22588\n",
    "        HR@5=0.42083  MRR@5=0.29699  NDCG@5=0.32779\n",
    "        HR@10=0.50007  MRR@10=0.30766  NDCG@10=0.35350\n",
    "        HR@20=0.57534  MRR@20=0.31288  NDCG@20=0.37252\n",
    "# RSC64-Session >= 3 6.33s\n",
    "    HR@20=0.71098  MRR@20=0.29453， hyper-parameters: session_length-20, hidden_size-100, lr-0.0010,delta=16.0, amsgrad-True, method-general, dropout-0.5, weight_decay-0.000000. \n",
    "        HR@1=0.16213  MRR@1=0.16213  NDCG@1=0.16213\n",
    "        HR@5=0.45847  MRR@5=0.26757  NDCG@5=0.31491\n",
    "        HR@10=0.60146  MRR@10=0.28679  NDCG@10=0.36129\n",
    "        HR@20=0.71098  MRR@20=0.29453  NDCG@20=0.38915\n",
    "    HR@20=0.71160  MRR@20=0.29509， hyper-parameters: session_length-20, hidden_size-100, lr-0.0005,delta=16.0, amsgrad-True, method-general, dropout-0.5, weight_decay-0.000000. \n",
    "        HR@1=0.16262  MRR@1=0.16262  NDCG@1=0.16262\n",
    "        HR@5=0.45716  MRR@5=0.26802  NDCG@5=0.31496\n",
    "        HR@10=0.59909  MRR@10=0.28712  NDCG@10=0.36101\n",
    "        HR@20=0.71160  MRR@20=0.29509  NDCG@20=0.38967\n",
    "# RSC4-Session >= 3 67.46s\n",
    "    HR@20=0.72319  MRR@20=0.30785, hyper-parameters: session_length-20, hidden_size-100, lr-0.0003,delta=16.0, amsgrad-True, method-general, dropout-0.0, weight_decay-0.000000. \n",
    "        HR@1=0.17419  MRR@1=0.17419  NDCG@1=0.17419\n",
    "        HR@5=0.47216  MRR@5=0.28121  NDCG@5=0.32861\n",
    "        HR@10=0.61253  MRR@10=0.30004  NDCG@10=0.37410\n",
    "        HR@20=0.72319  MRR@20=0.30785  NDCG@20=0.40224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T03:04:20.966861Z",
     "start_time": "2019-12-31T03:01:41.704675Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "S8pn0Z7bbv4t",
    "outputId": "c1aa9692-56a0-40d2-d99b-e75b4f1e537e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current model hyper-parameters: session_length=20, hidden_size=100, lr=0.0010,delta=16.0, amsgrad=True, method=general, dropout=0.5, weight_decay=0.000000. \n",
      "\n",
      "Start building the all training dataset\n",
      "The total number of training samples is： (433648, 21)\n",
      "[2019-12-31 11:01:46] [1/50] 0 mean_batch_loss : 9.824633\n",
      "Start predicting 2019-12-31 11:01:54\n",
      "The total number of testing samples is： (15132, 21)\n",
      "change best\n",
      "testing finish [2019-12-31 11:01:55] \n",
      "\tHR@1=0.20797  MRR@1=0.20797  NDCG@1=0.20797\n",
      "\tHR@5=0.35098  MRR@5=0.26867  NDCG@5=0.28957\n",
      "\tHR@10=0.37398  MRR@10=0.27175  NDCG@10=0.29703\n",
      "\tHR@20=0.39651  MRR@20=0.27334  NDCG@20=0.30275\n",
      "[2019-12-31 11:01:55] [2/50] 0 mean_batch_loss : 6.965831\n",
      "Start predicting 2019-12-31 11:02:03\n",
      "change best\n",
      "testing finish [2019-12-31 11:02:04] \n",
      "\tHR@1=0.21749  MRR@1=0.21749  NDCG@1=0.21749\n",
      "\tHR@5=0.38884  MRR@5=0.28500  NDCG@5=0.31108\n",
      "\tHR@10=0.43411  MRR@10=0.29111  NDCG@10=0.32579\n",
      "\tHR@20=0.47383  MRR@20=0.29391  NDCG@20=0.33589\n",
      "[2019-12-31 11:02:04] [3/50] 0 mean_batch_loss : 6.042022\n",
      "Start predicting 2019-12-31 11:02:12\n",
      "change best\n",
      "testing finish [2019-12-31 11:02:13] \n",
      "\tHR@1=0.21907  MRR@1=0.21907  NDCG@1=0.21907\n",
      "\tHR@5=0.40543  MRR@5=0.29044  NDCG@5=0.31921\n",
      "\tHR@10=0.46412  MRR@10=0.29843  NDCG@10=0.33835\n",
      "\tHR@20=0.51639  MRR@20=0.30207  NDCG@20=0.35159\n",
      "[2019-12-31 11:02:13] [4/50] 0 mean_batch_loss : 4.867581\n",
      "Start predicting 2019-12-31 11:02:21\n",
      "change best\n",
      "testing finish [2019-12-31 11:02:22] \n",
      "\tHR@1=0.22079  MRR@1=0.22079  NDCG@1=0.22079\n",
      "\tHR@5=0.41131  MRR@5=0.29262  NDCG@5=0.32226\n",
      "\tHR@10=0.47859  MRR@10=0.30175  NDCG@10=0.34416\n",
      "\tHR@20=0.54157  MRR@20=0.30614  NDCG@20=0.36011\n",
      "[2019-12-31 11:02:22] [5/50] 0 mean_batch_loss : 4.559913\n",
      "Start predicting 2019-12-31 11:02:30\n",
      "change best\n",
      "testing finish [2019-12-31 11:02:31] \n",
      "\tHR@1=0.22363  MRR@1=0.22363  NDCG@1=0.22363\n",
      "\tHR@5=0.41442  MRR@5=0.29483  NDCG@5=0.32465\n",
      "\tHR@10=0.48553  MRR@10=0.30454  NDCG@10=0.34786\n",
      "\tHR@20=0.55168  MRR@20=0.30916  NDCG@20=0.36463\n",
      "[2019-12-31 11:02:31] [6/50] 0 mean_batch_loss : 4.272336\n",
      "Start predicting 2019-12-31 11:02:39\n",
      "change best\n",
      "testing finish [2019-12-31 11:02:40] \n",
      "\tHR@1=0.22449  MRR@1=0.22449  NDCG@1=0.22449\n",
      "\tHR@5=0.41852  MRR@5=0.29640  NDCG@5=0.32681\n",
      "\tHR@10=0.49042  MRR@10=0.30612  NDCG@10=0.35020\n",
      "\tHR@20=0.55862  MRR@20=0.31087  NDCG@20=0.36746\n",
      "[2019-12-31 11:02:40] [7/50] 0 mean_batch_loss : 4.087121\n",
      "Start predicting 2019-12-31 11:02:48\n",
      "change best\n",
      "testing finish [2019-12-31 11:02:49] \n",
      "\tHR@1=0.22469  MRR@1=0.22469  NDCG@1=0.22469\n",
      "\tHR@5=0.41964  MRR@5=0.29647  NDCG@5=0.32712\n",
      "\tHR@10=0.49306  MRR@10=0.30637  NDCG@10=0.35096\n",
      "\tHR@20=0.56305  MRR@20=0.31121  NDCG@20=0.36864\n",
      "[2019-12-31 11:02:49] [8/50] 0 mean_batch_loss : 3.910193\n",
      "Start predicting 2019-12-31 11:02:57\n",
      "change best\n",
      "testing finish [2019-12-31 11:02:58] \n",
      "\tHR@1=0.22515  MRR@1=0.22515  NDCG@1=0.22515\n",
      "\tHR@5=0.41858  MRR@5=0.29685  NDCG@5=0.32718\n",
      "\tHR@10=0.49405  MRR@10=0.30713  NDCG@10=0.35179\n",
      "\tHR@20=0.56529  MRR@20=0.31210  NDCG@20=0.36984\n",
      "[2019-12-31 11:02:58] [9/50] 0 mean_batch_loss : 3.910091\n",
      "Start predicting 2019-12-31 11:03:06\n",
      "change best\n",
      "testing finish [2019-12-31 11:03:07] \n",
      "\tHR@1=0.22601  MRR@1=0.22601  NDCG@1=0.22601\n",
      "\tHR@5=0.41938  MRR@5=0.29771  NDCG@5=0.32802\n",
      "\tHR@10=0.49485  MRR@10=0.30798  NDCG@10=0.35263\n",
      "\tHR@20=0.56886  MRR@20=0.31313  NDCG@20=0.37135\n",
      "[2019-12-31 11:03:08] [10/50] 0 mean_batch_loss : 3.927575\n",
      "Start predicting 2019-12-31 11:03:16\n",
      "change best\n",
      "testing finish [2019-12-31 11:03:17] \n",
      "\tHR@1=0.22581  MRR@1=0.22581  NDCG@1=0.22581\n",
      "\tHR@5=0.42275  MRR@5=0.29801  NDCG@5=0.32903\n",
      "\tHR@10=0.49643  MRR@10=0.30790  NDCG@10=0.35291\n",
      "\tHR@20=0.57064  MRR@20=0.31307  NDCG@20=0.37170\n",
      "[2019-12-31 11:03:17] [11/50] 0 mean_batch_loss : 3.740057\n",
      "Start predicting 2019-12-31 11:03:25\n",
      "change best\n",
      "testing finish [2019-12-31 11:03:26] \n",
      "\tHR@1=0.22581  MRR@1=0.22581  NDCG@1=0.22581\n",
      "\tHR@5=0.42123  MRR@5=0.29751  NDCG@5=0.32829\n",
      "\tHR@10=0.49590  MRR@10=0.30761  NDCG@10=0.35257\n",
      "\tHR@20=0.57322  MRR@20=0.31298  NDCG@20=0.37212\n",
      "[2019-12-31 11:03:26] [12/50] 0 mean_batch_loss : 3.534871\n",
      "Start predicting 2019-12-31 11:03:34\n",
      "testing finish [2019-12-31 11:03:35] \n",
      "\tHR@1=0.22588  MRR@1=0.22588  NDCG@1=0.22588\n",
      "\tHR@5=0.42024  MRR@5=0.29683  NDCG@5=0.32751\n",
      "\tHR@10=0.49802  MRR@10=0.30728  NDCG@10=0.35273\n",
      "\tHR@20=0.57263  MRR@20=0.31243  NDCG@20=0.37157\n",
      "[2019-12-31 11:03:35] [13/50] 0 mean_batch_loss : 3.930592\n",
      "Start predicting 2019-12-31 11:03:43\n",
      "testing finish [2019-12-31 11:03:44] \n",
      "\tHR@1=0.22357  MRR@1=0.22357  NDCG@1=0.22357\n",
      "\tHR@5=0.41951  MRR@5=0.29539  NDCG@5=0.32627\n",
      "\tHR@10=0.49841  MRR@10=0.30611  NDCG@10=0.35197\n",
      "\tHR@20=0.57342  MRR@20=0.31132  NDCG@20=0.37095\n",
      "[2019-12-31 11:03:44] [14/50] 0 mean_batch_loss : 3.597035\n",
      "Start predicting 2019-12-31 11:03:52\n",
      "change best\n",
      "testing finish [2019-12-31 11:03:53] \n",
      "\tHR@1=0.22548  MRR@1=0.22548  NDCG@1=0.22548\n",
      "\tHR@5=0.42010  MRR@5=0.29664  NDCG@5=0.32735\n",
      "\tHR@10=0.49901  MRR@10=0.30734  NDCG@10=0.35304\n",
      "\tHR@20=0.57474  MRR@20=0.31259  NDCG@20=0.37218\n",
      "[2019-12-31 11:03:53] [15/50] 0 mean_batch_loss : 3.689915\n",
      "Start predicting 2019-12-31 11:04:01\n",
      "testing finish [2019-12-31 11:04:02] \n",
      "\tHR@1=0.22343  MRR@1=0.22343  NDCG@1=0.22343\n",
      "\tHR@5=0.41964  MRR@5=0.29524  NDCG@5=0.32618\n",
      "\tHR@10=0.49914  MRR@10=0.30609  NDCG@10=0.35213\n",
      "\tHR@20=0.57534  MRR@20=0.31140  NDCG@20=0.37143\n",
      "[2019-12-31 11:04:02] [16/50] 0 mean_batch_loss : 3.497535\n",
      "Start predicting 2019-12-31 11:04:10\n",
      "testing finish [2019-12-31 11:04:11] \n",
      "\tHR@1=0.22542  MRR@1=0.22542  NDCG@1=0.22542\n",
      "\tHR@5=0.41905  MRR@5=0.29627  NDCG@5=0.32681\n",
      "\tHR@10=0.49987  MRR@10=0.30717  NDCG@10=0.35306\n",
      "\tHR@20=0.57474  MRR@20=0.31238  NDCG@20=0.37201\n",
      "[2019-12-31 11:04:11] [17/50] 0 mean_batch_loss : 3.660315\n",
      "Start predicting 2019-12-31 11:04:19\n",
      "testing finish [2019-12-31 11:04:20] \n",
      "\tHR@1=0.22310  MRR@1=0.22310  NDCG@1=0.22310\n",
      "\tHR@5=0.41885  MRR@5=0.29502  NDCG@5=0.32583\n",
      "\tHR@10=0.50126  MRR@10=0.30608  NDCG@10=0.35255\n",
      "\tHR@20=0.57494  MRR@20=0.31121  NDCG@20=0.37119\n",
      "early stopping\n",
      "best model change\n",
      "current model hyper-parameters: session_length=20, hidden_size=100, lr=0.0010,delta=16.0, amsgrad=True, method=general, dropout=0.5, weight_decay=0.000000. \n",
      "\n",
      "current model HR@20=0.57474  MRR@20=0.31259.\n",
      "the best result so far. HR@20=0.57474  MRR@20=0.31259， hyper-parameters: session_length-20, hidden_size-100, lr-0.0010,delta=16.0, amsgrad-True, method-general, dropout-0.5, weight_decay-0.000000. \n",
      "\n",
      "The best result HR@20=0.57474  MRR@20=0.31259, hyper-parameters: session_length-20, hidden_size-100, lr-0.0010,delta=16.0, amsgrad-True, method-general, dropout-0.5, weight_decay-0.000000. \n",
      "over.\n"
     ]
    }
   ],
   "source": [
    "hidden_sizes = [100]\n",
    "dropouts = [0.5]\n",
    "attention_methods = [\"general\"]\n",
    "lrs = [1e-3]\n",
    "session_lengths = [20]\n",
    "weight_decays = [0]\n",
    "patience = 3\n",
    "deltas = [16]\n",
    "amsgrads = [True]\n",
    "best_params = \"\"\n",
    "best_all_model = 0.0\n",
    "best_all_hr = 0.0\n",
    "best_all_mrr = 0.0\n",
    "best_all_r1m = 0.0\n",
    "for session_length in session_lengths:\n",
    "    for hidden_size in hidden_sizes:\n",
    "        for amsgrad in amsgrads:\n",
    "            for attention_method in attention_methods:\n",
    "                for dropout in dropouts:\n",
    "                    for weight_decay in weight_decays:\n",
    "                        for lr in lrs:\n",
    "                            for delta in deltas:\n",
    "                                args = {}\n",
    "                                print(\"current model hyper-parameters: session_length=%d, hidden_size=%d, lr=%.4f,delta=%.1f, amsgrad=%s, method=%s, dropout=%.1f, weight_decay=%.6f. \\n\" % (session_length,hidden_size,lr,delta,str(amsgrad),attention_method,dropout,weight_decay))\n",
    "                                args[\"session_length\"] = session_length\n",
    "                                args[\"hidden_size\"] = hidden_size\n",
    "                                args[\"amsgrad\"] = amsgrad\n",
    "                                args[\"method\"] = attention_method\n",
    "                                args[\"dropout\"] = dropout\n",
    "                                args[\"weight_decay\"] = weight_decay\n",
    "                                args[\"lr\"] = lr\n",
    "                                args[\"delta\"] = delta\n",
    "                                args[\"patience\"] = patience\n",
    "                                best_model,best_model_hr,best_model_mrr = train(args)\n",
    "                                if best_model_hr + best_model_mrr > best_all_r1m:\n",
    "                                    print(\"best model change\")\n",
    "                                    best_all_r1m = best_model_hr + best_model_mrr\n",
    "                                    best_all_hr = best_model_hr\n",
    "                                    best_all_mrr = best_model_mrr\n",
    "                                    best_all_model = best_model\n",
    "                                    best_params = \"session_length-%d, hidden_size-%d, lr-%.4f,delta=%.1f, amsgrad-%s, method-%s, dropout-%.1f, weight_decay-%.6f\"%(session_length,hidden_size,lr,delta,str(amsgrad),attention_method,dropout,weight_decay)\n",
    "                                best_model = None\n",
    "                                print(\"current model hyper-parameters: session_length=%d, hidden_size=%d, lr=%.4f,delta=%.1f, amsgrad=%s, method=%s, dropout=%.1f, weight_decay=%.6f. \\n\" % (session_length,hidden_size,lr,delta,str(amsgrad),attention_method,dropout,weight_decay))\n",
    "                                print(\"current model HR@20=%.5f  MRR@20=%.5f.\"%(best_model_hr,best_model_mrr))\n",
    "                                print(\"the best result so far. HR@20=%.5f  MRR@20=%.5f， hyper-parameters: %s. \\n\"%(best_all_hr,best_all_mrr,best_params))\n",
    "print(\"The best result HR@20=%.5f  MRR@20=%.5f, hyper-parameters: %s. \"%(best_all_hr,best_all_mrr,best_params))\n",
    "print(\"over.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T03:04:20.972125Z",
     "start_time": "2019-12-31T03:04:20.967758Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "O8eiYy5UbyFR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.0000],\n",
       "        [0.8842],\n",
       "        [0.6071],\n",
       "        [1.3033],\n",
       "        [1.3105],\n",
       "        [1.9434],\n",
       "        [1.8757],\n",
       "        [2.0732],\n",
       "        [2.0489],\n",
       "        [1.9761],\n",
       "        [2.0412],\n",
       "        [1.9635],\n",
       "        [1.9627],\n",
       "        [1.9923],\n",
       "        [1.8329],\n",
       "        [1.8374],\n",
       "        [1.8829],\n",
       "        [1.7630],\n",
       "        [1.9085],\n",
       "        [1.8781],\n",
       "        [1.6539]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_all_model.position_weights.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T03:04:20.990202Z",
     "start_time": "2019-12-31T03:04:20.972875Z"
    }
   },
   "outputs": [],
   "source": [
    "# best_all_model.gate_weights.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "WDP-CE",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "smy",
   "language": "python",
   "name": "smy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
